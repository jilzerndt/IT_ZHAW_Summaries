\section{Elementare Wahrscheinlichkeitsrechnung}

\subsection{Diskrete Wahrscheinlichkeitsräume}


\begin{formula}{Symbole}
    \begin{itemize}
        \item $\Omega$: Ergebnisraum (Menge aller möglichen Ergebnisse)
        \item $\omega$: Ergebnis eines Zufallsexperiments
        \item $|\Omega|$: Anzahl der Elemente im Ergebnisraum
        \item $P: \Omega \rightarrow[0,1]$: Wahrscheinlichkeitsmaß (Zähldichte)\\
        ordnet jedem Ergebnis $\omega \in \Omega$ seine Wahrscheinlichkeit zu,\\ wobei $\sum_{\omega \in \Omega} P(\omega) = 1$ gilt
        \item $2^\Omega$: Ereignisraum (Menge aller möglichen Ereignisse)
        \item $P(A)$: Wahrscheinlichkeit des Ereignisses $A$
        \item $|A|$: Anzahl der Elemente im Ereignis $A$
        \item $A,B,C$: Ereignisse (Teilmengen von $\Omega$)
        \item $\bar{A}$: Komplementärereignis von $A$
    \end{itemize}
\end{formula}

\begin{definition}{Zufallsexperiment}
folgende Bedingungen müssen erfüllt sein:
\begin{itemize}
    \item Der Vorgang lässt sich unter den gleichen äußeren Bedingungen beliebig oft wiederholen
    \item Es sind mehrere sich gegenseitig ausschließende Ergebnisse möglich
    \item Das Ergebnis lässt sich nicht mit Sicherheit voraussagen, sondern ist zufallsbedingt
\end{itemize}
\end{definition}

\begin{concept}{Ereignisse und Wahrscheinlichkeitsraum}

Das \textbf{Wahrscheinlichkeitsmaß} $P: 2^\Omega \rightarrow [0,1]$ ist definiert durch:
$$P(A) = \sum_{\omega \in A} \rho(\omega) \text{ für } A \subseteq \Omega$$

Ein \textbf{Laplace-Raum} liegt vor, wenn alle Elementarereignisse gleich wahrscheinlich sind:
$$P(A)=\frac{|A|}{|\Omega|}$$
\end{concept}

\begin{theorem}{Eigenschaften von Wahrscheinlichkeitsräumen}\\
Für einen diskreten Wahrscheinlichkeitsraum $(\Omega,P)$ gelten:
\begin{itemize}
    \item Unmögliches Ereignis: $P(\emptyset) = 0$
    \item Sicheres Ereignis: $P(\Omega) = 1$
    \item Komplementäres Ereignis: $P(\Omega \setminus A) = 1 - P(A)$
    \item Vereinigung: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
    \item Sigma-Additivität: Für paarweise disjunkte Ereignisse gilt:\\
    $P(A_1 \cup A_2 \cup A_3 \cup ...) = P(A_1) + P(A_2) + P(A_3) + ...$
\end{itemize}
\end{theorem}

\begin{corollary}{Wahrscheinlichkeits-Ausdrücke und Regeln}
\begin{itemize}
    \item $P(A)$ = Wahrscheinlichkeit von Ereignis $A$
    \item $P(B)$ = Wahrscheinlichkeit von Ereignis $B$
    \item $P(\bar{A})$ = Wahrscheinlichkeit des Gegenereignisses von $A$
    \item $P(B|A)$ = Wahrscheinlichkeit von $B$ unter der Bedingung dass $A$ eingetreten ist
    \item $P(B|\bar{A})$ = Wahrscheinlichkeit von $B$ unter der Bedingung dass $A$ nicht eingetreten ist
    \item $P(A \cap B)$ = Wahrscheinlichkeit dass beide Ereignisse eintreten
    \item $P(A \cup B)$ = Wahrscheinlichkeit dass mindestens eines der Ereignisse eintritt
    \item \textbf{Additionssatz:} $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
    \item \textbf{Komplementärregel:} $P(\bar{A}) = 1 - P(A)$
    \item \textbf{Multiplikationssatz:} $P(A \cap B) = P(A) \cdot P(B|A) = P(B) \cdot P(A|B)$
\end{itemize}   
\end{corollary}

\subsubsection{Strategien zur Berechnung von Wahrscheinlichkeiten}

\begin{KR}{Grundschritte der Wahrscheinlichkeitsberechnung}\\
1. \textbf{Ergebnisraum identifizieren}
   \begin{itemize}
   \item Alle möglichen Ergebnisse auflisten
   \item Prüfen, ob es sich um einen Laplace-Raum handelt
   \end{itemize}

2. \textbf{Ereignis präzisieren}
   \begin{itemize}
   \item Exakte mathematische Beschreibung des gesuchten Ereignisses
   \item Zerlegung in Teilmengen falls nötig
   \end{itemize}

3. \textbf{Berechnungsstrategie wählen}
   \begin{itemize}
   \item Direkte Berechnung: $P(A) = \frac{|A|}{|\Omega|}$
   \item Über Gegenereignis: $P(A) = 1 - P(\bar{A})$
   \item Über bedingte Wahrscheinlichkeit falls abhängig
   \end{itemize}

4. \textbf{Berechnung durchführen} Kombinatorische Formeln anwenden
   \begin{itemize}
   \item Zwischenergebnisse notieren
   \item Probe durch Plausibilitätskontrolle
   \end{itemize}
\end{KR}

\begin{concept}{Problemlösung mit Gegenereignis} Oft einfacher\\
1. \textbf{Prüfe, ob Gegenereignis einfacher ist}
   \begin{itemize}
   \item Original: "Mindestens eine...'' oder "Mehr als..."
   \item Gegenereignis: "Keine...'' oder "Höchstens..."
   \end{itemize}

2. \textbf{Berechne Wahrscheinlichkeit des Gegenereignis}
   \begin{itemize}
   \item Oft einfacher zu zählen
   \item Weniger Fälle zu berücksichtigen
   \end{itemize}

3. \textbf{Wende Komplementärregel an:} $P(A) = 1 - P(\bar{A})$
   \begin{itemize}
   \item Überprüfe Plausibilität des Ergebnisses
   \end{itemize}
\end{concept}

\subsubsection{Zufallsvariablen}

\begin{formula}{Symbole}
    \begin{itemize}
        \item $X,Y,Z$: Zufallsvariablen (Funktionen von $\Omega$ nach $\mathbb{R}$)
        \item $x,y,z$: Mögliche Werte der Zufallsvariablen
        \item $P(X=x)$: Wahrscheinlichkeit, dass $X$ den Wert $x$ annimmt
        \item $P(X \leq x)$: Wahrscheinlichkeit, dass $X$ kleiner oder gleich $x$ ist
        \item $P(X=x,Y=y)$: Wahrscheinlichkeit, dass $X=x$ und $Y=y$ sind
        \item $f(x)$: Wahrscheinlichkeitsfunktion (PMF) von $X$
        \item $F(x)$: Verteilungsfunktion (CDF) von $X$
        \item $E(X)$: Erwartungswert von $X$
        \item $V(X)$: Varianz von $X$
        \item $S(X)$: Standardabweichung von $X$
        \item $\alpha, \beta, \gamma$: Konstanten
        \item $\sum_{x\in\mathbb{R}}$ = Summe über alle möglichen Werte von $x \in \mathbb{R}$
    \end{itemize}
\end{formula}

\begin{definition}{Zufallsvariablen}
Eine \textbf{Zufallsvariable} $X$ ist eine Funktion $X: \Omega \rightarrow \mathbb{R}$, die jedem Ergebnis eine reelle Zahl zuordnet.

Die \textbf{Wahrscheinlichkeitsfunktion} (PMF) ist definiert durch:
$$f(x) = P(X = x) = P(\{\omega \in \Omega : X(\omega) = x\})$$

Die \textbf{Verteilungsfunktion} (CDF) ist definiert durch:
$$F(x) = P(X \leq x) = \sum_{t \leq x} f(t)$$
\end{definition}

\begin{theorem}{Eigenschaften von PMF und CDF}
\begin{itemize}
    \item $\sum_{x \in \mathbb{R}} f(x) = 1$ und $F(x) = \sum_{t \leq x} f(t)$
    \item $\lim_{x \to \infty} F(x) = 1$ und $\lim_{x \to -\infty} F(x) = 0$
    \item Monotonie: $x \leq y \Rightarrow F(x) \leq F(y)$
    \item $P(a < X \leq b) = F(b) - F(a)$
\end{itemize}
\end{theorem}



\subsection{Kenngrössen}

\begin{definition}{Erwartungswert und Varianz}
Für eine diskrete Zufallsvariable $X$:
\vspace{-2mm}\\
$$\text{\textbf{Erwartungswert: }} E(X) = \sum_{x \in \mathbb{R}} x \cdot f(x)$$
$$\text{\textbf{Varianz: }}V(X) = E((X-E(X))^2) = \sum_{x \in \mathbb{R}} (x-E(X))^2 \cdot f(x)$$
$$\text{\textbf{Standardabweichung: }}S(X) = \sqrt{V(X)}$$
\end{definition}

\begin{theorem}{Rechenregeln} für \textcolor{pink}{\textbf{stochastisch unabhängige}} Zufallsvariablen $X$, $Y$:
    \vspace{1mm}
\begin{itemize}
    \setlength{\itemsep}{2pt}
    \item \textbf{Addition:} $E(X + Y) = E(X) + E(Y)$, $V(X \pm Y) = V(X) + V(Y)$
    \item \textbf{Multiplikation:} $E(X \cdot Y) = E(X) \cdot E(Y)$
    \item \textbf{Linearität:} $E(aX + b) = aE(X) + b$
    \item \textbf{Verschiebungssatz:} $V(X) = E(X^2) - (E(X))^2$\\
    wobei $E(X^2) = \sum_{x\in\mathbb{R}} P(X = x) \cdot x^2$
    \item \textbf{Lineare Transformation:} $V(aX + b) = a^2V(X)$
\end{itemize}
\end{theorem}

\begin{corollary}{Erwartungswert und Varianz}    
Für eine stetige Zufallsvariable $X$:
$$
\begin{gathered}
E(X)=\int_{-\infty}^{\infty} f(x) \cdot x dx \quad
V(X)=\int_{-\infty}^{\infty} f(x) \cdot(x-E(X))^2 dx
\end{gathered}
$$
\end{corollary}

\begin{KR}{Berechnung von Erwartungswert und Varianz}\\
1. \textbf{Erwartungswert bestimmen:} \\Formel je nach Art der Zufallsvariable (diskret/stetig)

2. \textbf{Varianz berechnen:} direkt (Formel) oder über Verschiebungssatz

3. \textbf{Bei Standardabweichung:} Wurzel aus Varianz ziehen
   \begin{itemize}
   \item Einheit beachten (gleich wie Ursprungsdaten)
   \end{itemize}
\end{KR}

\begin{example2}{Erwartungswert bei Würfelspiel}
Bei einem Würfelspiel gewinnt man:
\begin{itemize}
\item Bei 6: 5€, bei 5: 2€, bei 1-4: verliert man 1€
\end{itemize}
\vspace{1mm}
\begin{enumerate}
\item \textbf{Wahrscheinlichkeiten und Werte aufstellen:}
   \begin{itemize}
   \item P(X = 5€) = 1/6, P(X = 2€) = 1/6, P(X = -1€) = 4/6
   \end{itemize}

\item \textbf{Erwartungswert berechnen:}
   $$E(X) = 5 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + (-1) \cdot \frac{4}{6} = \frac{5+2-4}{6} = \frac{3}{6} = 0.5$$

\item \textbf{Varianz berechnen:}
   \begin{align*}
   E(X^2) &= 25 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 1 \cdot \frac{4}{6} = \frac{25+4+4}{6} = \frac{33}{6} \\
   V(X) &= E(X^2) - (E(X))^2 = \frac{33}{6} - (\frac{1}{2})^2 = \frac{33}{6} - \frac{1}{4} \approx 5.25
   \end{align*}
\end{enumerate}

\textbf{Interpretation:}
\begin{itemize}
\item Positiver Erwartungswert: Spiel ist langfristig profitabel
\item Hohe Varianz: Große Schwankungen möglich
\end{itemize}
\end{example2}

\begin{KR}{Interpretation von Erwartungswert und Varianz}\\
\textbf{Erwartungswert:} Nicht unbedingt ein möglicher Wert
   \begin{itemize}
   \item Langfristiger Durchschnitt, Schwerpunkt der Verteilung
   \end{itemize}

\textbf{Varianz:} Mass für die Streuung (Quadratische Einheit beachten)
   \begin{itemize}
   \item Je größer, desto unsicherer die Vorhersage
   \end{itemize}

\textbf{Standardabweichung:} Typische Abweichung vom Mittelwert 
   \begin{itemize}
   \item Oft für Konfidenzintervalle verwendet (Gleiche Einheit wie Daten)
   \end{itemize}
\end{KR}

\subsection{Stochastische Unabhängigkeit}

\begin{theorem}{Stochastische Unabhängigkeit Ereignisse}\\
Zwei Ereignisse $A$ und $B$ heißen \textbf{stochastisch unabhängig}, falls:
$$P(A \cap B) = P(A) \cdot P(B)$$
\end{theorem}

\begin{corollary}{Eigenschaften der stochastischen Unabhängigkeit}\\
Für unabhängige Ereignisse $A$ und $B$ gilt:
\begin{itemize}
    \item $A$ und $\Omega \setminus B$ sind unabhängig
    \item $\Omega \setminus A$ und $\Omega \setminus B$ sind unabhängig
    \item $P(A|B) = P(A)$ falls $P(B) > 0$
\end{itemize}
\end{corollary}

\begin{theorem}{Stochastische Unabhängigkeit Zufallsvariablen}\\
Zwei Zufallsvariablen $X$ und $Y$ heißen \textbf{stochastisch unabhängig}, falls für alle $x,y \in \mathbb{R}$:
$$P(X=x, Y=y) = P(X=x) \cdot P(Y=y)$$
\end{theorem}

\begin{KR}{Prüfung auf stochastische Unabhängigkeit}\\
1. \textbf{Für Ereignisse}
   \begin{itemize}
   \item Berechne $P(A \cap B)$ und $P(A) \cdot P(B)$
   \item Vergleiche die Werte
   \end{itemize}

2. \textbf{Für Zufallsvariablen}
   \begin{itemize}
   \item Stelle Verbundverteilung auf und prüfe für alle Wertepaare:\\ $P(X=x, Y=y) = P(X=x) \cdot P(Y=y)$
   \item Alternative: Prüfe Kovarianz = 0
   \end{itemize}

3. \textbf{Praktische Überlegungen}
   \begin{itemize}
   \item Physikalische/logische Abhängigkeit?
   \item Kausaler Zusammenhang?
   \item Gemeinsame Einflussfaktoren?
   \end{itemize}
\end{KR}

\begin{example2}{Würfelwurf und Münzwurf}\\
\textbf{Aufgabe:} Ein Würfel wird geworfen und eine Münze geworfen.\\
Ereignisse:
\begin{itemize}
\item A: "Würfel zeigt eine 6"
\item B: "Münze zeigt Kopf"
\end{itemize}

\textbf{Lösung:}
\begin{enumerate}
\item \textbf{Einzelwahrscheinlichkeiten:}
   \begin{itemize}
   \item $P(A) = \frac{1}{6}$
   \item $P(B) = \frac{1}{2}$
   \end{itemize}

\item \textbf{Schnittwahrscheinlichkeit:}
   $P(A \cap B) = \frac{1}{12} = \frac{1}{6} \cdot \frac{1}{2} = P(A) \cdot P(B)$

\item \textbf{Schlussfolgerung:} Die Ereignisse sind stochastisch unabhängig
\end{enumerate}
\end{example2}

\begin{example2}{Kartenziehen ohne Zurücklegen}\\
\textbf{Aufgabe:} Aus einem Kartenspiel werden nacheinander zwei Karten gezogen.

Ereignisse:
\begin{itemize}
\item A: "Erste Karte ist Herz"
\item B: "Zweite Karte ist Herz"
\end{itemize}

\textbf{Lösung:}
\begin{enumerate}
\item \textbf{Wahrscheinlichkeiten:}
   \begin{itemize}
   \item $P(A) = \frac{13}{52} = \frac{1}{4}$
   \item $P(B|A) = \frac{12}{51}$
   \item $P(B|\bar{A}) = \frac{13}{51}$
   \end{itemize}

\item \textbf{Prüfung:}
   $$P(B) = \frac{13}{52} \neq P(B|A)$$

\item \textbf{Schlussfolgerung:} Die Ereignisse sind stochastisch abhängig
\end{enumerate}
\end{example2}

\subsection{Bedingte Wahrscheinlichkeit}

\begin{definition}{Bedingte Wahrscheinlichkeit}
von $B$ unter der Bedingung $A$ ist:
$$P(B|A) = \frac{P(A \cap B)}{P(A)} \quad \text{für } P(A) > 0$$
\end{definition}

\begin{theorem}{Multiplikationssatz}
$P(A \cap B)=P(A) \cdot P(B|A)=P(B) \cdot P(A|B)$

\textbf{Anwendung:}
\begin{itemize}
    \item Berechnung von Schnittwahrscheinlichkeiten
    \item Prüfung auf stochastische Unabhängigkeit
    \item Zerlegung von mehrstufigen Experimenten
\end{itemize}
\end{theorem}

\begin{KR}{Erstellen einer Vierfeldertafel}\\
1. \textbf{Aufbau der Tabelle}
   \begin{itemize}
   \item Zeilen: Erstes Merkmal (A und nicht A)
   \item Spalten: Zweites Merkmal (B und nicht B)
   \item Randwahrscheinlichkeiten notieren
   \end{itemize}

2. \textbf{Eintragen der Wahrscheinlichkeiten}
   \begin{itemize}
   \item Schnittwahrscheinlichkeiten in die Felder
   \item Zeilensummen = P(A) bzw. P(nicht A)
   \item Spaltensummen = P(B) bzw. P(nicht B)
   \end{itemize}

3. \textbf{Berechnung bedingter Wahrscheinlichkeiten}
   \begin{itemize}
   \item $P(B|A) = \frac{P(A \cap B)}{P(A)}$ und $P(A|B) = \frac{P(A \cap B)}{P(B)}$
   \end{itemize}
\end{KR}

\begin{example2}{Medizinischer Test}\\
\textbf{Aufgabe:} Ein Test auf eine Krankheit hat folgende Eigenschaften:
\begin{itemize}
\item 1\% der Bevölkerung hat die Krankheit
\item Test ist bei Kranken zu 98\% positiv
\item Test ist bei Gesunden zu 95\% negativ
\end{itemize}

\textbf{Lösung mit Vierfeldertafel:}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
 & Test + & Test - & Summe \\
\hline
Krank & 0.0098 & 0.0002 & 0.01 \\
\hline
Gesund & 0.0495 & 0.9405 & 0.99 \\
\hline
Summe & 0.0593 & 0.9407 & 1 \\
\hline
\end{tabular}
\end{center}

\textbf{Berechnung:} Wahrscheinlichkeit krank bei positivem Test:
$$P(\text{krank}|\text{positiv}) = \frac{0.0098}{0.0593} \approx 0.165 = 16.5\%$$
\end{example2}

\begin{theorem}{Satz der Totalen Wahrscheinlichkeit}
$$P(B)=P(A) \cdot P(B|A)+P(\bar{A}) \cdot P(B|\bar{A})$$
\textbf{Anwendung:}
\begin{itemize}
    \item Berechnung von P(B) durch Fallunterscheidung
    \item Basis für den Satz von Bayes
    \item Wichtig bei Entscheidungsbäumen
\end{itemize}
\end{theorem}

\begin{KR}{Ereignisbäume}\\
1. \textbf{Aufbau}
   \begin{itemize}
   \item Von links nach rechts zeichnen
   \item Alle Verzweigungen vollständig angeben
   \item Übergangswahrscheinlichkeiten an Äste schreiben
   \end{itemize}

2. \textbf{Pfadwahrscheinlichkeiten}
   \begin{itemize}
   \item Multiplikation entlang des Pfades
   \item Für jedes Endereignis alle Pfade addieren
   \item Summe aller Pfadwahrscheinlichkeiten = 1
   \end{itemize}
\end{KR}

%TODO: add example for Ereignisbaum and Total Probability Theorem

\begin{theorem}{Satz von Bayes}
    \vspace{-4mm}\\
$$P(A|B)=\frac{P(A) \cdot P(B|A)}{P(B)}$$
\vspace{-4mm}\\
\textbf{Anwendung:}
\begin{itemize}
    \item Umkehrung bedingter Wahrscheinlichkeiten
    \item Aktualisierung von Wahrscheinlichkeiten
    \item Diagnostische Tests
\end{itemize}
\end{theorem}


\begin{KR}{Anwendung des Satzes von Bayes}\\
1. \textbf{Identifiziere die bekannten Größen}
   \begin{itemize}
   \item A priori Wahrscheinlichkeit P(A)
   \item Bedingte Wahrscheinlichkeit P(B|A)
   \item Totale Wahrscheinlichkeit P(B)
   \end{itemize}

2. \textbf{Berechne P(B) falls nötig}
   \begin{itemize}
   \item Nutze Satz der totalen Wahrscheinlichkeit
   \item P(B) = P(A)·P(B|A) + P(Ā)·P(B|Ā)
   \end{itemize}

3. \textbf{Berechne P(A|B)}
   \begin{itemize}
   \item Setze in Bayes-Formel ein
   \item Interpretiere das Ergebnis
   \end{itemize}
\end{KR}

\begin{example2}{Qualitätskontrolle}
\textbf{Aufgabe:} Eine Maschine produziert Teile. 
\begin{itemize}
\item 95\% der Teile sind fehlerfrei
\item Ein Test erkennt fehlerhafte Teile zu 98\%
\item Der Test klassifiziert 3\% der guten Teile falsch
\end{itemize}

\textbf{Gesucht:} Wahrscheinlichkeit für tatsächlich fehlerhaftes Teil bei positivem Test

\textbf{Lösung:}
\begin{itemize}
\item $P(F)$ = 0.05 (fehlerhaft)
\item $P(T|F)$ = 0.98 (Test positiv wenn fehlerhaft)
\item $P(T|\bar{F})$ = 0.03 (Test positiv wenn gut)
\item $P(T)$ = 0.05·0.98 + 0.95·0.03 = 0.0775
\item $P(F|T)$ = $\frac{0.05 \cdot 0.98}{0.0775} \approx 0.632$ = 63.2\%
\end{itemize}
\end{example2}


\subsubsection{Kovarianz und Korrelation}

\begin{definition}{Kovarianz und Korrelation}
Die \textbf{Kovarianz} zweier Zufallsvariablen ist:
$$Cov(X,Y) = E((X-E(X))(Y-E(Y))) = E(XY) - E(X)E(Y)$$

Der \textbf{Korrelationskoeffizient} ist:
$$\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{V(X)V(Y)}}$$

\textbf{Eigenschaften:}
\begin{itemize}
    \item $-1 \leq \rho_{XY} \leq 1$
    \item $\rho_{XY} = \pm 1$: perfekter linearer Zusammenhang
    \item $\rho_{XY} = 0$: unkorreliert
\end{itemize}
\end{definition}

\begin{KR}{Anwendung von Kovarianz und Korrelation}\\
1. \textbf{Kovarianz berechnen}
   \begin{itemize}
   \item Direkter Weg: $Cov(X,Y) = E(XY) - E(X)E(Y)$
   \item Alternativ: $\frac{1}{n}\sum(x_i-\bar{x})(y_i-\bar{y})$
   \end{itemize}

2. \textbf{Korrelation bestimmen}
   \begin{itemize}
   \item Kovarianz durch Produkt der Standardabweichungen
   \item Normierung auf [-1,1]
   \end{itemize}

3. \textbf{Interpretation}
   \begin{itemize}
   \item Vorzeichen: Richtung des Zusammenhangs
   \item Betrag: Stärke des Zusammenhangs
   \item Unabhängig von Maßeinheiten
   \end{itemize}
\end{KR}



