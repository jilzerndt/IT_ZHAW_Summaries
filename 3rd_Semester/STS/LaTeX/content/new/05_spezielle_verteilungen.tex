\section{Spezielle Verteilungen} 

\subsection{Diskrete und Stetige Zufallsvariablen}

\begin{definition}{Diskrete und Stetige Zufallsvariablen}
Bei einer \textbf{diskreten Zufallsvariable} gibt es immer Lücken zwischen den Werten; sie kann nur bestimmte Werte annehmen.

Eine \textbf{stetige Zufallsvariable} hat ein kontinuierliches Spektrum von möglichen Werten.

\textbf{Berechnung von Wahrscheinlichkeiten:}
\begin{itemize}
    \item Diskret: $P(X=x) = f(x)$ (PMF)
    \item Stetig: $P(X \leq x) = \int_{-\infty}^x f(t)dt$ (CDF)
\end{itemize}
\end{definition}

\begin{concept}{Gegenüberstellung von diskreten und stetigen Zufallsvariablen}

\textcolor{darkturquoise}{\textbf{Diskrete Zufallsvariable:}}
\begin{itemize}
    \item Dichtefunktion: $f(x) = P(X=x)$
    \item Verteilungsfunktion: $F(x) = \sum_{x \leq X} f(x)$
    \item Wahrscheinlichkeiten: $P(a \leq X \leq b) = \sum_{a \leq x \leq b} f(x)$
    \item Erwartungswert: $E(X) = \sum_{x \in \mathbb{R}} x \cdot f(x)$
    \item Varianz: $V(X) = \sum_{x \in \mathbb{R}} (x-E(X))^2 \cdot f(x)$
\end{itemize}

\textcolor{darkturquoise}{\textbf{Stetige Zufallsvariable:}}
\begin{itemize}
    \item Dichtefunktion: $f(x) = F'(x) \neq P(X=x)$
    \item Verteilungsfunktion: $F(x) = \int_{-\infty}^x f(t)dt$
    \item Wahrscheinlichkeiten: $P(a \leq X \leq b) = \int_a^b f(x)dx$
    \item Erwartungswert: $E(X) = \int_{-\infty}^{\infty} x \cdot f(x)dx$
    \item Varianz: $V(X) = \int_{-\infty}^{\infty} (x-E(X))^2 \cdot f(x)dx$
\end{itemize}
\end{concept}

%TODO: add missing information about Intervallwahrscheinlichkeiten

\subsection{Diskrete Verteilungen}

\begin{theorem}{Übersicht der diskreten Verteilungen}
    
1. \textbf{Hypergeometrische Verteilung:} Ziehen ohne Zurücklegen
   \begin{itemize}
   \item Endliche Grundgesamtheit
   \item Veränderliche Wahrscheinlichkeiten
   \end{itemize}
2. \textbf{Bernoulli-Verteilung:} Genau zwei mögliche Ausgänge
   \begin{itemize}
   \item Ein einzelner Versuch
   \item Konstante Erfolgswahrscheinlichkeit
   \end{itemize}
3. \textbf{Binomial-Verteilung:} Mehrere unabhängige Versuche
   \begin{itemize}
   \item Feste Anzahl unabhängiger Versuche
   \item Mit Zurücklegen/große Grundgesamtheit
   \item Konstante Erfolgswahrscheinlichkeit
   \end{itemize}
4. \textbf{Poisson-Verteilung:} Seltene Ereignisse
   \begin{itemize}
   \item Festes Zeitintervall/Raumbereich 
   \item Rate $\lambda$ bekannt
   \end{itemize}
\end{theorem}

\begin{KR}{Wahl der richtigen Verteilung}\\
1. \textbf{Prüfe Ziehungsart}
   \begin{itemize}
   \item Mit Zurücklegen → Binomialverteilung
   \item Ohne Zurücklegen → Hypergeometrische Verteilung
   \item Seltene Ereignisse → Poisson-Verteilung
   \end{itemize}

2. \textbf{Prüfe Grundgesamtheit}
   \begin{itemize}
   \item Endlich, klein → Hypergeometrische Verteilung
   \item Sehr groß/unendlich → Binomialverteilung
   \item Zeitlich/räumlich kontinuierlich → Poisson-Verteilung
   \end{itemize}

3. \textbf{Beachte Approximationen}
   \begin{itemize}
   \item Binomial → Poisson für $n \to \infty$, $p \to 0$, $np = \lambda$
   \item Hypergeometrisch → Binomial für $\frac{n}{N} \leq 0.05$
   \end{itemize}
\end{KR}


\begin{definition}{Hypergeometrische Verteilung}\\
Ziehen \textbf{ohne Zurücklegen} aus einer endlichen Grundgesamtheit.
$$\text{\textbf{Wahrscheinlichkeitsfunktion: }}P(X=k) = \frac{\binom{M}{k} \cdot \binom{N-M}{n-k}}{\binom{N}{n}}$$
\textbf{Notation:} $X \sim H(N,M,n)$

\begin{minipage}[t]{0.5\textwidth}
\textbf{Parameter:}
\begin{itemize}
    \item $N$: Grundgesamtheit
    \item $M$: Anzahl Merkmalsträger
    \item $n$: Stichprobenumfang
\end{itemize}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\textbf{Kenngrößen:}
\begin{itemize}
    \item $E(X) = n \cdot \frac{M}{N}$
    \item $V(X) = n \cdot \frac{M}{N} \cdot (1-\frac{M}{N}) \cdot \frac{N-n}{N-1}$
\end{itemize}
\end{minipage}
\end{definition}

\begin{definition}{Bernoulli-Verteilung}
Experiment mit genau zwei möglichen Ausgängen (Erfolg/Misserfolg bzw 1/0).
$$P(X=1) = p, \quad P(X=0) = 1-p = q$$
\textbf{Notation:} $X \sim B(1,p)$

\begin{minipage}[t]{0.5\textwidth}
\textbf{Parameter:}
\begin{itemize}
    \item $p$ = Erfolgswahrscheinlichkeit
    \item $q = 1-p$ = Gegenwahrscheinlichkeit
\end{itemize}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\textbf{Kenngrößen:}
\begin{itemize}
    \item $E(X)=E(X^2)=p$
    \item $V(X)=p \cdot(1-p)=pq$
\end{itemize}
\end{minipage}
\end{definition}

\begin{remark}
Voraussetzungen für die Bernoulli-Verteilung:
\begin{itemize}
    \item Genau zwei mögliche Ausgänge
    \item Unabhängige Wiederholungen
    \item Konstante Erfolgswahrscheinlichkeit
\end{itemize}
\end{remark}


\begin{definition}{Binomialverteilung}\\
$n$-malige \textbf{unabhängige Wiederholung} eines Bernoulli-Experiments
$$\text{\textbf{Wahrscheinlichkeitsfunktion: }}P(X=k) = \binom{n}{k} \cdot p^k \cdot q^{n-k}$$
\textbf{Notation:} $X \sim B(n,p)$

\begin{minipage}[t]{0.5\textwidth}
\textbf{Parameter:}
\begin{itemize}
    \item $n$: Anzahl Versuche
    \item $p$: Erfolgswahrscheinlichkeit
    \item $q = 1-p$: Gegenwahrscheinlichkeit
\end{itemize}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\textbf{Kenngrößen:}
\begin{itemize}
    \item $E(X) = n \cdot p$
    \item $V(X) = n \cdot p \cdot q$
\end{itemize}
\end{minipage}
\end{definition}

\begin{definition}{Poissonverteilung}\\
Modelliert \textbf{seltene Ereignisse} in einem festen Intervall.
$$\text{\textbf{Wahrscheinlichkeitsfunktion: }}P(X=k) = \frac{\lambda^k}{k!} \cdot e^{-\lambda}, \quad \lambda > 0$$

\begin{minipage}[t]{0.6\textwidth}
\textbf{Notation:} $X \sim Poi(\lambda)$

\textbf{Parameter:}
\begin{itemize}
    \item $\lambda$: Rate/Erwartungswert pro Intervall
\end{itemize}
\end{minipage}
\begin{minipage}[t]{0.38\textwidth}
\textbf{Kenngrößen:}
\begin{itemize}
    \item $E(X) = \lambda$
    \item $V(X) = \lambda$
\end{itemize}
\end{minipage}
\end{definition}







\subsection{Stetige Verteilungen}

\begin{concept}{Erwartungswert und Varianz der Normalverteilung}\\
Für eine Zufallsvariable $X \sim N(\mu;\sigma)$ gilt:
$$
E(X) = \mu, \quad V(X) = \sigma^2
$$
\vspace{-6mm}\\
\textbf{Parameter:}\\
$\mu$ = Erwartungswert (Lage)\\
$\sigma^2$ = Varianz\\
$\sigma$ = Standardabweichung (Streuung)
\end{concept}

\begin{definition}{Gauss-Verteilung/Normalverteilung}
Die stetige Zufallsvariable $X$ folgt der Normalverteilung mit den Parametern $\mu, \sigma \in \mathbb{R}, \sigma>0$:
\vspace{-2mm}\\
$$
\text{Dichtefunktion der Normalverteilung: }\varphi_{\mu,\sigma}(x)=\frac{1}{\sqrt{2\pi} \cdot \sigma} \cdot e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
$$
\vspace{-5mm}\\
\textbf{Notation:} $X \sim N(\mu,\sigma)$
\vspace{2mm}\\
\textcolor{blue}{Standardnormalverteilung} ($\mu=0$ und $\sigma=1$):
\vspace{-3mm}\\
$$
\text{Dichtefunktion der Standardnormalverteilung: }\varphi(x)=\frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{1}{2}x^2}
$$
\end{definition}

\begin{theorem}{Eigenschaften der Normalverteilung}
    \begin{itemize}
        \item Symmetrisch um $\mu$
        \item Wendepunkte bei $\mu \pm \sigma$
        \item Ca. 68\% der Werte in $[\mu-\sigma, \mu+\sigma]$
        \item Ca. 95\% der Werte in $[\mu-2\sigma, \mu+2\sigma]$
        \item Ca. 99,7\% der Werte in $[\mu-3\sigma, \mu+3\sigma]$
    \end{itemize}
\end{theorem}

\begin{definition}{Die Verteilungsfunktion der Normalverteilung}\\
Die kumulative Verteilungsfunktion (CDF) von $\varphi_{\mu,\sigma}(x)$ wird mit $\phi_{\mu,\sigma}(x)$ bezeichnet. Sie ist definiert durch:
$$
\phi_{\mu,\sigma}(x) = P(X \leq x) = \int_{-\infty}^x \varphi_{\mu,\sigma}(t)dt = \frac{1}{\sqrt{2\pi} \cdot \sigma} \cdot \int_{-\infty}^x e^{-\frac{1}{2}(\frac{t-\mu}{\sigma})^2} dt
$$
\\
$\varphi_{\mu,\sigma}(x)$ = Dichtefunktion der Normalverteilung\\
$P(X \leq x)$ = Wahrscheinlichkeit dass $X$ kleiner oder gleich $x$ ist
\end{definition}

\begin{concept}{Standardisierung der Normalverteilung}\\
Bei einer stetigen Zufallsvariable $X$ lässt sich die Verteilungsfunktion als Integral einer Funktion $f$ darstellen:
\vspace{-2mm}\\
$$
\quad \quad F(x) = P(X \leq x) = \int_{-\infty}^x f(u) \cdot du
$$

Liegt eine beliebige Normalverteilung $N(\mu,\sigma)$ vor, muss standardisiert werden. Statt ursprünglichen Zufallsvariablen $X$ betrachtet man die Zufallsvariable:
\vspace{-2mm}\\
$$
U = \frac{X-\mu}{\sigma}
$$
\\
$F(x)$ = Verteilungsfunktion\\
$P(X \leq x)$ = Wahrscheinlichkeit dass $X$ kleiner oder gleich $x$ ist\\
$f(u)$ = Dichtefunktion\\
$U$ = Standardisierte Zufallsvariable\\
$X$ = Ursprüngliche Zufallsvariable\\
\end{concept}

\begin{KR}{Arbeiten mit der Normalverteilung}\\
1. \textbf{Standardisierung}
   \begin{itemize}
   \item $Z = \frac{X-\mu}{\sigma}$ transformiert zu N(0,1)
   \item Benutze Tabelle der Standardnormalverteilung
   \item Beachte: $\phi(z) = 1 - \phi(-z)$
   \end{itemize}

2. \textbf{Stetigkeitskorrektur}
   \begin{itemize}
   \item Bei Approximation diskreter Verteilungen
   \item Untere Grenze: $a - 0.5$
   \item Obere Grenze: $b + 0.5$
   \end{itemize}

3. \textbf{Faustregel für Intervalle}
   \begin{itemize}
   \item $\mu \pm \sigma$: ca. 68\% der Werte
   \item $\mu \pm 2\sigma$: ca. 95\% der Werte
   \item $\mu \pm 3\sigma$: ca. 99.7\% der Werte
   \end{itemize}
\end{KR}

\subsection{Zentraler Grenzwertsatz und Approximationen}



\begin{theorem}{Zentraler Grenzwertsatz}\\
Für eine Folge von Zufallsvariablen $X_1, X_2, \ldots, X_n$ mit gleichem Erwartungswert $\mu$ und gleicher Varianz $\sigma^2$ gilt:
$$
E(S_n)=n \cdot \mu, \quad V(S_n)=n \cdot \sigma^2
$$
$$
E(\bar{X}_n)=\mu, \quad V(\bar{X}_n)=\frac{\sigma^2}{n}=\frac{1}{n^2} \cdot V(S_n)
$$
$S_n$ = Summe der Zufallsvariablen\\
$\bar{X}_n$ = Arithmetisches Mittel der Zufallsvariablen\\
$n$ = Anzahl der Zufallsvariablen\\
$\mu$ = Erwartungswert der einzelnen Zufallsvariablen\\
$\sigma^2$ = Varianz der einzelnen Zufallsvariablen\\

Die standardisierte Zufallsvariable:
$$
U_n=\frac{((X_1+X_2+\cdots+X_n)-n\mu)}{\sqrt{n} \cdot \sigma}=\frac{(\bar{X}-\mu)}{\frac{\sigma}{\sqrt{n}}}
$$

Sind die Zufallsvariablen alle identisch $N(\mu,\sigma)$ verteilt, so sind die Summe $S_n$ und das arithmetische Mittel $\bar{X}_n$ wieder normalverteilt mit:
\begin{itemize}
  \item $S_n: \quad N(n \cdot \mu, \sqrt{n} \cdot \sigma)$
  \item $\bar{X}_n: \quad N(\mu, \frac{\sigma}{\sqrt{n}})$
\end{itemize}
\vspace{3mm}
Verteilungsfunktion $F_n(u)$ konvergiert für $n \to \infty$ gegen die Verteilungsfunktion $\phi(u)$ der Standardnormalverteilung:
$$
\lim_{n\to\infty} F_n(u) = \phi(u) = \frac{1}{\sqrt{2\pi}} \cdot \int_{-\infty}^u e^{-\frac{1}{2}t^2} dt
$$
\end{theorem}

\begin{corollary}{Weitere Eigenschaften}\\
Für die Summe $S_n = X_1 + ... + X_n$ von $n$ unabhängigen, identisch verteilten Zufallsvariablen mit $E(X_i)=\mu$ und $V(X_i)=\sigma^2$ gilt:

\begin{itemize}
    \item $S_n$ ist approximativ normalverteilt
    \item $E(S_n) = n\mu$
    \item $V(S_n) = n\sigma^2$
\end{itemize}

Für das arithmetische Mittel $\bar{X}_n = \frac{S_n}{n}$ gilt:
\begin{itemize}
    \item $\bar{X}_n$ ist approximativ normalverteilt
    \item $E(\bar{X}_n) = \mu$
    \item $V(\bar{X}_n) = \frac{\sigma^2}{n}$
\end{itemize}
\end{corollary}

\begin{KR}{Anwendung des Zentralen Grenzwertsatzes}\\
1. \textbf{Prüfe Voraussetzungen}
   \begin{itemize}
   \item Unabhängige Zufallsvariablen
   \item Identische Verteilung
   \item Endliche Varianz
   \item Genügend große Stichprobe (n $\geq$ 30)
   \end{itemize}

2. \textbf{Berechne Parameter}
   \begin{itemize}
   \item $\mu_{S_n} = n\mu$
   \item $\sigma_{S_n} = \sqrt{n}\sigma$
   \item $\mu_{\bar{X}} = \mu$
   \item $\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}$
   \end{itemize}

3. \textbf{Standardisiere}
   \begin{itemize}
   \item Transformiere zu $Z = \frac{X-\mu}{\sigma}$
   \item Verwende Tabelle der Standardnormalverteilung
   \end{itemize}
\end{KR}

\subsubsection{Approximationen}

\begin{concept}{Approximation durch die Normalverteilung}
\begin{itemize}
  \item Binomialverteilung: $\quad \mu=np, \sigma^2=npq$
  \item Poissonverteilung: $\quad \mu=\lambda, \sigma^2=\lambda$
\end{itemize}
$$
P(a \leq X \leq b)=\sum_{x=a}^{b} P(X=x) \approx \phi_{\mu,\sigma}(b+\frac{1}{2})-\phi_{\mu,\sigma}(a-\frac{1}{2})
$$
$P(a \leq X \leq b)$ = Wahrscheinlichkeit dass $X$ zwischen $a$ und $b$ liegt\\
$\phi_{\mu,\sigma}$ = Verteilungsfunktion der Normalverteilung\\
$a, b$ = Untere und obere Grenze\\
\end{concept}

\begin{theorem}{Approximationsregeln}\\
\textbf{Binomialverteilung $\rightarrow$ Normalverteilung:}
\begin{itemize}
    \item Bedingung: $npq > 9$
    \item Parameter: $\mu = np$, $\sigma^2 = npq$
    \item $B(n,p) \approx N(np, \sqrt{npq})$
    \item Stetigkeitskorrektur beachten!
\end{itemize}

\textbf{Binomialverteilung $\rightarrow$ Poissonverteilung:}
\begin{itemize}
    \item Bedingung: $n \geq 50$ und $p \leq 0.1$
    \item $B(n,p) \approx Poi(np)$
\end{itemize}

\textbf{Hypergeometrisch $\rightarrow$ Binomialverteilung:}
\begin{itemize}
    \item Bedingung: $n \leq \frac{N}{20}$
    \item $H(N,M,n) \approx B(n,\frac{M}{N})$
\end{itemize}
\end{theorem}

\begin{corollary}{Faustregeln für Approximationen}
\begin{itemize}
  \item Die Approximation (Binomialverteilung) kann verwendet werden, wenn $npq > 9$
  \item Für grosses $n(n \geq 50)$ und kleines $p(p \leq 0.1)$ kann die Binomial- durch die Poisson-Verteilung approximiert werden:
  $$
  B(n,p) \approx \text{Poi}(n \cdot p)
  $$
  
  \item Eine Hypergeometrische Verteilung kann durch eine Binomialverteilung angenähert werden, wenn $n \leq \frac{N}{20}$:
  $$
  H(N,M,n) \approx B(n,\frac{M}{N})
  $$
\end{itemize}
\end{corollary}

\begin{remark}
    $H(N,M,n)$ = Hypergeometrische Verteilung\\
    $B(n,p)$ = Binomialverteilung\\
    $\text{Poi}(\lambda)$ = Poissonverteilung mit Parameter $\lambda = n \cdot p$\\
    $N$ = Grundgesamtheit\\
    $M$ = Anzahl der Erfolge in der Grundgesamtheit\\
    $n$ = Stichprobengröße
\end{remark}

\begin{KR}{Wahl der richtigen Verteilung}\\
1. \textbf{Diskrete Verteilungen:}
   \begin{itemize}
   \item Ziehen ohne Zurücklegen: Hypergeometrisch
   \item Unabhängige Versuche: Binomial
   \item Seltene Ereignisse: Poisson
   \end{itemize}

2. \textbf{Approximationen prüfen:}
   \begin{itemize}
   \item $npq > 9$: Normal-Approximation möglich
   \item $n \geq 50, p \leq 0.1$: Poisson-Approximation möglich
   \item $n \leq \frac{N}{20}$: Binomial-Approximation möglich
   \end{itemize}

3. \textbf{Stetigkeitskorrektur:}
   \begin{itemize}
   \item Bei Normal-Approximation: $\pm 0.5$ an den Grenzen
   \item $P(X \leq k) \approx P(X \leq k + 0.5)$
   \item $P(X = k) \approx P(k - 0.5 \leq X \leq k + 0.5)$
   \end{itemize}
\end{KR}

\begin{KR}{Entscheidung über Approximationen}\\
1. \textbf{Prüfe Stichprobenumfang}
   \begin{itemize}
   \item Klein (n < 30): Exakte Verteilung
   \item Mittel (30 $\leq$ n < 50): Je nach p
   \item Groß (n $\geq$ 50): Approximation möglich
   \end{itemize}

2. \textbf{Prüfe Wahrscheinlichkeit}
   \begin{itemize}
   \item p $\leq$ 0.1: Poisson möglich
   \item 0.1 < p < 0.9: Normal möglich
   \item npq > 9: Normal empfohlen
   \end{itemize}

3. \textbf{Wähle Approximation}
   \begin{itemize}
   \item Binomial → Normal: Große Stichproben, mittleres p
   \item Binomial → Poisson: Große n, kleines p
   \item Hypergeometrisch → Binomial: Kleine Stichprobe relativ zur Grundgesamtheit
   \end{itemize}

4. \textbf{Beachte}
   \begin{itemize}
   \item Stetigkeitskorrektur bei Normal
   \item Rundungsregeln bei Grenzen
   \item Vergleich mit exakter Lösung wenn möglich
   \end{itemize}
\end{KR}

