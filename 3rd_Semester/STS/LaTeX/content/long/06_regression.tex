\section{Methode der kleinsten Quadrate}
\begin{definition}{Lineare Regression}\\
Gegeben sind Datenpunkte $(x_i; y_i)$ mit $1 \leq i \leq n$. Die Residuen / Fehler $\epsilon_i=g(x_i)-y_i$ dieser Datenpunkte sind Abstände in $y$-Richtung zwischen $y_i$ und der Geraden $g$. Die Ausgleichs- oder Regressiongerade ist diejenige Gerade, für die die Summe der quadrierten Residuen $\sum_{i=1}^{n} \epsilon_i^2$ am kleinsten ist.\\
$(x_i, y_i)$ = Datenpunkte\\
$\epsilon_i$ = Residuum (Abweichung) des $i$-ten Datenpunkts\\
$g(x_i)$ = Wert der Regressionsgerade an der Stelle $x_i$\\
$n$ = Anzahl der Datenpunkte\\
\end{definition}

\begin{KR}{Lineare Regression berechnen}\\
1. Berechne arithmetische Mittel $\bar{x}$ und $\bar{y}$
2. Berechne Kovarianzen und Varianzen:
   \begin{itemize}
     \item $s_{xy} = \frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})$
     \item $s_x^2 = \frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})^2$
     \item $s_y^2 = \frac{1}{n}\sum_{i=1}^n (y_i-\bar{y})^2$
   \end{itemize}
3. Berechne Steigung $m$ und y-Achsenabschnitt $d$:
   \begin{itemize}
     \item $m = \frac{s_{xy}}{s_x^2}$
     \item $d = \bar{y} - m\bar{x}$
   \end{itemize}
4. Regressionsgerade: $g(x) = mx + d$
\end{KR}

\begin{example2}{Lineare Regression}
Gegeben sind die Datenpunkte:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$x_i$ & 1 & 2 & 3 & 4 & 5 \\
\hline
$y_i$ & 2.1 & 4.0 & 6.3 & 7.8 & 9.9 \\
\hline
\end{tabular}
\end{center}

1. $\bar{x} = 3$, $\bar{y} = 6.02$

2. Kovarianzen und Varianzen:
   \begin{itemize}
     \item $s_{xy} = 3.945$
     \item $s_x^2 = 2$
     \item $s_y^2 = 8.4916$
   \end{itemize}

3. Parameter:
   \begin{itemize}
     \item $m = \frac{3.945}{2} = 1.9725$
     \item $d = 6.02 - 1.9725 \cdot 3 = 0.1025$
   \end{itemize}

4. Regressionsgerade: $g(x) = 1.9725x + 0.1025$
\end{example2}

\begin{theorem}{Regressionsgerade}\\
Die Regressionsgerade $g(x)=mx+d$ mit den Parametern $m$ und $d$ ist die Gerade, für welche die Residualvarianz $s_{\epsilon}^2$ minimal ist.

$$
\text{Steigung: } m=\frac{s_{xy}}{s_x^2}, \quad \text{y-Achsenabschnitt: } d=\bar{y}-m\bar{x}, \quad s_{\epsilon}^2=s_y^2-\frac{s_{xy}^2}{s_x^2}
$$
$m$ = Steigung der Regressionsgerade\\
$d$ = y-Achsenabschnitt\\
$s_{xy}$ = Kovarianz von $x$ und $y$\\
$s_x^2$ = Varianz der $x$-Werte\\
$s_y^2$ = Varianz der $y$-Werte\\
$\bar{x}$ = Arithmetisches Mittel der $x$-Werte\\
$\bar{y}$ = Arithmetisches Mittel der $y$-Werte\\
$s_{\epsilon}^2$ = Residualvarianz\\
\end{theorem}

\begin{KR}{Residuen und Residuenplot analysieren}\\
1. Berechne die Residuen für jeden Datenpunkt:
   \begin{itemize}
     \item $\epsilon_i = y_i - (mx_i + d)$
   \end{itemize}
2. Erstelle Residuenplot:
   \begin{itemize}
     \item x-Achse: Prognostizierte Werte $\hat{y}_i = mx_i + d$
     \item y-Achse: Residuen $\epsilon_i$
   \end{itemize}
3. Prüfe Eigenschaften:
   \begin{itemize}
     \item Residuen sollten zufällig um Null streuen
     \item Keine systematischen Muster erkennbar
     \item Gleiche Streubreite über alle $\hat{y}_i$
   \end{itemize}
\end{KR}

\subsection{Bestimmtheitsmass}
\begin{concept}{Varianzaufspaltung}\\
Die Totale Varianz setzt sich zusammen aus der Residualvarianz und der Varianz der prognostizierten Werte:
\begin{itemize}
  \item $s_y^2$ Totale Varianz
  \item $s_{\hat{y}}^2$ prognostizierte (erklärte) Varianz
  \item $s_{\epsilon}^2$ Residualvarianz
\end{itemize}

$$
s_y^2=s_{\epsilon}^2+s_{\hat{y}}^2
$$
$s_y^2$ = Totale Varianz der beobachteten $y$-Werte\\
$s_{\epsilon}^2$ = Varianz der Residuen\\
$s_{\hat{y}}^2$ = Varianz der durch die Regression geschätzten Werte\\
\end{concept}

\begin{KR}{Bestimmtheitsmass berechnen}\\
1. Berechne die totale Varianz $s_y^2$
2. Berechne die Residualvarianz $s_{\epsilon}^2$
3. Berechne die erklärte Varianz $s_{\hat{y}}^2$
4. Berechne das Bestimmtheitsmass:
   $$R^2 = \frac{s_{\hat{y}}^2}{s_y^2} = 1 - \frac{s_{\epsilon}^2}{s_y^2}$$
5. Interpretation:
   \begin{itemize}
     \item $R^2 \approx 1$: Sehr gute Anpassung
     \item $R^2 \approx 0$: Schlechte Anpassung
   \end{itemize}
\end{KR}

\begin{theorem}{Bestimmtheitsmass}\\
Das Bestimmtheitsmass $R^2$ beurteilt die globale Anpassungsgüte einer Regression über den Anteil der prognostizierten Varianz $s_{\hat{y}}^2$ an der totalen Varianz $s_y^2$:
$$
R^2=\frac{s_{\hat{y}}^2}{s_y^2}
$$
$R^2$ = Bestimmtheitsmass (zwischen 0 und 1)\\
$s_{\hat{y}}^2$ = Varianz der prognostizierten Werte\\
$s_y^2$ = Totale Varianz\\

Das Bestimmtheitsmass $R^2$ entspricht dem Quadrat des Korrelationskoeffizienten:
$$
R^2=\frac{s_{xy}^2}{s_x^2 \cdot s_y^2}=(r_{xy})^2
$$
$s_{xy}$ = Kovarianz von $x$ und $y$\\
$s_x^2$ = Varianz der $x$-Werte\\
$s_y^2$ = Varianz der $y$-Werte\\
$r_{xy}$ = Korrelationskoeffizient\\
\end{theorem}

\subsection{Linearisierungsfunktionen}
\begin{concept}{Transformationen}\\
\begin{center}
\begin{tabular}{|c|c|}
\hline
Ausgangsfunktion & Transformation \\
\hline
$y=q \cdot x^m$ & $\log(y)=\log(q)+m \cdot \log(x)$ \\
\hline
$y=q \cdot m^x$ & $\log(y)=\log(q)+\log(m) \cdot x$ \\
\hline
$y=q \cdot e^{m \cdot x}$ & $\ln(y)=\ln(q)+m \cdot x$ \\
\hline
$y=\frac{1}{q+m \cdot x}$ & $V=q+m \cdot x; V=\frac{1}{y}$ \\
\hline
$y=q+m \cdot \ln(x)$ & $y=q+m \cdot U; u=\ln(x)$ \\
\hline
$y=\frac{1}{q \cdot m^x}$ & $\log(\frac{1}{y})=\log(q)+\log(m) \cdot x$ \\
\hline
\end{tabular}
\end{center}
$y$ = Abhängige Variable\\
$x$ = Unabhängige Variable\\
$q, m$ = Parameter der Funktion\\
$e$ = Eulersche Zahl\\
$\ln$ = Natürlicher Logarithmus\\
$\log$ = Logarithmus zur Basis 10\\
\end{concept}

\begin{KR}{Nichtlineare Regression durch Linearisierung}\\
1. Bestimme passende Transformation aus Tabelle
2. Führe Transformation durch
3. Wende lineare Regression auf transformierte Daten an
4. Transformiere Parameter zurück

Beispiel für exponentielles Wachstum $y=q \cdot e^{mx}$:
\begin{enumerate}
  \item Transformation: $\ln(y)=\ln(q)+mx$
  \item Setze $Y=\ln(y)$, $b=\ln(q)$
  \item Lineare Regression für $Y=mx+b$
  \item Rücktransformation: $q=e^b$
\end{enumerate}
\end{KR}

\begin{example2}{Exponentielles Wachstum}
Gegeben sind die Messwerte:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$x$ & 1 & 2 & 3 & 4 \\
\hline
$y$ & 2.1 & 4.2 & 8.1 & 15.9 \\
\hline
\end{tabular}
\end{center}

1. Transformation $Y=\ln(y)$:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$x$ & 1 & 2 & 3 & 4 \\
\hline
$Y$ & 0.742 & 1.435 & 2.092 & 2.766 \\
\hline
\end{tabular}
\end{center}

2. Lineare Regression ergibt: $Y = 0.674x + 0.071$

3. Rücktransformation:
   \begin{itemize}
     \item $m = 0.674$
     \item $q = e^{0.071} = 1.074$
   \end{itemize}

4. Ergebnis: $y = 1.074 \cdot e^{0.674x}$
\end{example2}

\subsection{Methode der kleinsten Quadrate}

\begin{definition}{Matrix-Darstellung}\\
Die Parameter $m$ und $q$ der Regressionsgeraden werden mit der Matrix $A$ berechnet:
$$A = \begin{pmatrix} x_1 & 1 \\ \vdots & \vdots \\ x_n & 1 \end{pmatrix}, \quad A^T \cdot A \cdot \begin{pmatrix} m \\ q \end{pmatrix} = A^T \cdot \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}$$
\end{definition}

\begin{KR}{Matrix-Methode für lineare Regression}\\
1. Erstelle Design-Matrix $A$:
   $$A = \begin{pmatrix} x_1 & 1 \\ \vdots & \vdots \\ x_n & 1 \end{pmatrix}$$
2. Berechne $A^T \cdot A$
3. Berechne $(A^T \cdot A)^{-1}$
4. Berechne Parameter:
   $$\begin{pmatrix} m \\ q \end{pmatrix} = (A^T \cdot A)^{-1} \cdot A^T \cdot \vec{y}$$
\end{KR}

\begin{formula}{Residuenberechnung}\\
Die Residuen $\epsilon_i$ ergeben sich als:
$$\epsilon_i = y_i - \hat{y}_i = y_i - (mx_i + q)$$

Die Summe der quadrierten Residuen wird minimiert:
$$\sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - (mx_i + q))^2 \rightarrow \text{min}$$
\end{formula}

\begin{KR}{Mehrfachregression}\\
1. Aufstellen der Designmatrix:
   $$A = \begin{pmatrix} 
   x_{11} & x_{12} & \cdots & x_{1(k-1)} & 1 \\
   x_{21} & x_{22} & \cdots & x_{2(k-1)} & 1 \\
   \vdots & \vdots & \ddots & \vdots & \vdots \\
   x_{n1} & x_{n2} & \cdots & x_{n(k-1)} & 1
   \end{pmatrix}$$

2. Berechnung der Parameter:
   $$\vec{p} = (A^T A)^{-1} A^T \vec{y}$$

3. Residuen berechnen:
   $$\vec{\epsilon} = \vec{y} - A\vec{p}$$

4. Bestimmtheitsmass:
   $$R^2 = 1 - \frac{\sum \epsilon_i^2}{\sum(y_i - \bar{y})^2}$$
\end{KR}

\begin{example2}{Mehrfachregression}
Ein Gebrauchtwagenhändler möchte den Preis (P) seiner Autos basierend auf Alter (A) und Kilometerstand (K) berechnen.
Gegeben sind folgende Daten:

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Auto & Alter (Jahre) & km (10000) & Preis (1000 CHF) \\
\hline
1 & 2 & 3 & 25 \\
2 & 3 & 4 & 20 \\
3 & 4 & 6 & 15 \\
4 & 5 & 7 & 12 \\
\hline
\end{tabular}
\end{center}

1. Designmatrix aufstellen:
   $$A = \begin{pmatrix}
   2 & 3 & 1 \\
   3 & 4 & 1 \\
   4 & 6 & 1 \\
   5 & 7 & 1
   \end{pmatrix}$$

2. Parameter berechnen:
   $$\vec{p} = \begin{pmatrix} -3 \\ -1.5 \\ 35 \end{pmatrix}$$

3. Resultierende Funktion:
   $$P = -3A - 1.5K + 35$$
\end{example2}

\begin{KR}{Polynomiale Regression}\\
Für Regression mit Polynomen höheren Grades:

1. Erweitere Designmatrix:
   $$A = \begin{pmatrix}
   x_1^n & x_1^{n-1} & \cdots & x_1 & 1 \\
   x_2^n & x_2^{n-1} & \cdots & x_2 & 1 \\
   \vdots & \vdots & \ddots & \vdots & \vdots \\
   x_m^n & x_m^{n-1} & \cdots & x_m & 1
   \end{pmatrix}$$

2. Löse wie bei linearer Regression:
   $$\vec{p} = (A^T A)^{-1} A^T \vec{y}$$

3. Polynom aufstellen:
   $$y = p_1x^n + p_2x^{n-1} + ... + p_nx + p_{n+1}$$
\end{KR}

\begin{example2}{Quadratische Regression}
Gegeben sind Messwerte:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$x$ & 0 & 1 & 2 & 3 & 4 \\
\hline
$y$ & 1 & 2.1 & 5.2 & 10.1 & 17.2 \\
\hline
\end{tabular}
\end{center}

1. Designmatrix für quadratisches Polynom:
   $$A = \begin{pmatrix}
   0 & 0 & 1 \\
   1 & 1 & 1 \\
   4 & 2 & 1 \\
   9 & 3 & 1 \\
   16 & 4 & 1
   \end{pmatrix}$$

2. Parameter berechnen:
   $$\vec{p} = \begin{pmatrix} 1 \\ 0.1 \\ 1 \end{pmatrix}$$

3. Quadratische Funktion:
   $$y = x^2 + 0.1x + 1$$
\end{example2}

\begin{concept}{Gütekriterien für Regression}\\
1. Bestimmtheitsmass $R^2$:
   \begin{itemize}
     \item $R^2 > 0.9$: Sehr gute Anpassung
     \item $0.7 < R^2 < 0.9$: Gute Anpassung
     \item $0.5 < R^2 < 0.7$: Mittelmässige Anpassung
     \item $R^2 < 0.5$: Schlechte Anpassung
   \end{itemize}

2. Residuenanalyse:
   \begin{itemize}
     \item Residuen sollten zufällig um 0 schwanken
     \item Keine systematischen Muster erkennbar
     \item Residuen sollten normalverteilt sein
   \end{itemize}

3. Prognosegüte:
   \begin{itemize}
     \item Mittlerer quadratischer Fehler (MSE)
     \item Wurzel des mittleren quadratischen Fehlers (RMSE)
     \item Mittlerer absoluter Fehler (MAE)
   \end{itemize}
\end{concept}

\begin{example2}{Modellwahl durch Residuenanalyse}
Für einen Datensatz wurden drei Modelle getestet:
\begin{itemize}
  \item Linear: $y = 2x + 1$
  \item Quadratisch: $y = x^2 + x + 1$
  \item Exponentiell: $y = 2e^{0.5x}$
\end{itemize}

Bestimmtheitsmasse:
\begin{itemize}
  \item Linear: $R^2 = 0.85$
  \item Quadratisch: $R^2 = 0.98$
  \item Exponentiell: $R^2 = 0.92$
\end{itemize}

Residuenanalyse zeigt:
\begin{itemize}
  \item Linear: Systematische Krümmung in Residuen
  \item Quadratisch: Zufällige Verteilung der Residuen
  \item Exponentiell: Leichte Systematik in Residuen
\end{itemize}

Schlussfolgerung: Das quadratische Modell ist am besten geeignet.
\end{example2}

\begin{KR}{Prüfungsaufgaben lösen}\\
1. Aufgabentyp identifizieren:
   \begin{itemize}
     \item Einfache lineare Regression
     \item Mehrfachregression
     \item Nichtlineare Regression mit Transformation
     \item Polynomiale Regression
   \end{itemize}

2. Vorgehen wählen:
   \begin{itemize}
     \item Linear: Direkte Berechnung mit Formeln
     \item Nichtlinear: Transformation und lineare Regression
     \item Polynomial: Erweiterte Designmatrix
     \item Mehrfach: Matrix-Methode
   \end{itemize}

3. Berechnungen durchführen:
   \begin{itemize}
     \item Parameter bestimmen
     \item Bestimmtheitsmass berechnen
     \item Residuen analysieren
   \end{itemize}

4. Ergebnisse interpretieren:
   \begin{itemize}
     \item Modellgüte bewerten
     \item Residuen beurteilen
     \item Prognosen erstellen
   \end{itemize}
\end{KR}

\begin{example2}{Klausuraufgabe - Linearisierung}
Gegeben sind Messwerte für ein exponentielles Wachstum:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$t$ (h) & 0 & 1 & 2 & 3 \\
\hline
$N$ & 100 & 150 & 225 & 340 \\
\hline
\end{tabular}
\end{center}

Finden Sie eine Funktion der Form $N(t) = N_0 e^{kt}$

1. Transformation:
   $$\ln(N) = \ln(N_0) + kt$$

2. Neue Wertetabelle:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$t$ & 0 & 1 & 2 & 3 \\
\hline
$\ln(N)$ & 4.61 & 5.01 & 5.42 & 5.83 \\
\hline
\end{tabular}
\end{center}

3. Lineare Regression:
   $$\ln(N) = 0.405t + 4.61$$

4. Rücktransformation:
   $$N(t) = 100.4 e^{0.405t}$$

5. Bestimmtheitsmass: $R^2 = 0.999$
\end{example2}