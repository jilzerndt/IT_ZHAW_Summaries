\section{Spezielle Verteilungen}

\subsection{Diskrete und Stetige Zufallsvariablen}
%TODO: remove redundant content
\begin{definition}{Verteilungen und Erwartungswerte}
\textbf{Für diskrete Verteilungen:}
$$
\begin{gathered}
E(X)=\sum_{x \in \mathbb{R}} f(x) \cdot x \\
V(X)=\sum_{x \in \mathbb{R}} f(x) \cdot(x-E(X))^2
\end{gathered}
$$

\textbf{Für stetige Verteilungen:}
$$
\begin{gathered}
E(X)=\int_{-\infty}^{\infty} f(x) \cdot x dx \\
V(X)=\int_{-\infty}^{\infty} f(x) \cdot(x-E(X))^2 dx
\end{gathered}
$$

\textbf{Parameter:}
\begin{itemize}
    \item $E(X)$ = Erwartungswert der Zufallsvariable $X$
    \item $V(X)$ = Varianz der Zufallsvariable $X$
    \item $f(x)$ = Wahrscheinlichkeitsfunktion (diskret) oder Dichtefunktion (stetig)
    \item $x$ = Mögliche Werte der Zufallsvariable
\end{itemize}
\end{definition}

\begin{KR}{Berechnung von Erwartungswert und Varianz}
1. \textbf{Diskrete Verteilung}
   \begin{itemize}
   \item Liste alle möglichen Werte $x_i$ auf
   \item Bestimme zugehörige Wahrscheinlichkeiten $P(X=x_i)$
   \item $E(X) = \sum x_i \cdot P(X=x_i)$
   \item $V(X) = \sum (x_i-E(X))^2 \cdot P(X=x_i)$
   \end{itemize}

2. \textbf{Stetige Verteilung}
   \begin{itemize}
   \item Identifiziere Dichtefunktion $f(x)$
   \item Berechne $E(X) = \int x \cdot f(x) dx$
   \item Berechne $V(X) = \int (x-E(X))^2 \cdot f(x) dx$
   \end{itemize}
\end{KR}

\subsection{Diskrete Verteilungen}
%TODO: remove redundant content

\begin{definition}{Bernoulli-Verteilung}
Experiment mit genau zwei möglichen Ausgängen (Erfolg/Misserfolg).
$$P(X=1) = p, \quad P(X=0) = 1-p = q$$

\textbf{Kenngrößen:}
\begin{itemize}
    \item $E(X) = p$
    \item $V(X) = p(1-p)$
\end{itemize}
\end{definition}

\begin{KR}{Anwendung der Bernoulli-Verteilung}
1. \textbf{Prüfe Voraussetzungen}
   \begin{itemize}
   \item Genau zwei mögliche Ausgänge
   \item Unabhängige Wiederholungen
   \item Konstante Erfolgswahrscheinlichkeit
   \end{itemize}

2. \textbf{Parameter identifizieren}
   \begin{itemize}
   \item p = Erfolgswahrscheinlichkeit
   \item q = 1-p = Misserfolgswahrscheinlichkeit
   \end{itemize}

3. \textbf{Berechnung}
   \begin{itemize}
   \item E(X) = p
   \item V(X) = pq
   \end{itemize}
\end{KR}

\begin{definition}{Bernoulliverteilung}
Bernoulli-Experimente sind Zufallsexperimente mit nur zwei möglichen Ergebnissen (1 und 0):
$$
P(X=1)=p, \quad P(X=0)=1-p=q
$$
Es gilt:
\begin{enumerate}
	\item $E(X)=E(X^2)=p$
	\item $V(X)=p \cdot(1-p)$
\end{enumerate}
\hfill \break
$E(X)$ = Erwartungswert\\
$V(X)$ = Varianz\\
$P(X=1)$ = Wahrscheinlichkeit für Erfolg\\
$p$ = Erfolgswahrscheinlichkeit\\
$q$ = Gegenwahrscheinlichkeit $(1-p)$\\
\end{definition}

\begin{definition}{Bernoulliverteilung}
Experiment mit genau zwei möglichen Ausgängen:
$$
P(X=1)=p, \quad P(X=0)=1-p=q
$$

\textbf{Parameter:}
\begin{itemize}
    \item $p$ = Erfolgswahrscheinlichkeit
    \item $q = 1-p$ = Gegenwahrscheinlichkeit
\end{itemize}

\textbf{Kenngrößen:}
\begin{enumerate}
    \item $E(X)=E(X^2)=p$
    \item $V(X)=p \cdot(1-p)=pq$
\end{enumerate}
\end{definition}

\begin{example}{Münzwurf}
\textbf{Aufgabe:} Faire Münze wird geworfen. X = 1 bei Kopf, X = 0 bei Zahl.

\textbf{Lösung:}
\begin{itemize}
\item p = 0.5 (faire Münze)
\item $E(X) = 0.5$
\item $V(X) = 0.5 \cdot 0.5 = 0.25$
\item $P(X=1) = 0.5$
\item $P(X=0) = 0.5$
\end{itemize}
\end{example}

\begin{definition}{Binomialverteilung}
n-malige unabhängige Wiederholung eines Bernoulli-Experiments.
$$P(X=k) = \binom{n}{k}p^k(1-p)^{n-k}$$

\textbf{Kenngrößen:}
\begin{itemize}
    \item $E(X) = np$
    \item $V(X) = np(1-p)$
\end{itemize}

\textbf{Notation:} $X \sim B(n,p)$
\end{definition}

\begin{example}{Qualitätskontrolle mit Binomialverteilung}
\textbf{Aufgabe:} Eine Maschine produziert Teile mit Ausschussquote 5\%. In einer Stichprobe von 100 Teilen:
\begin{itemize}
\item a) Wie viele defekte Teile sind zu erwarten?
\item b) Wie groß ist die Wahrscheinlichkeit für genau 3 defekte Teile?
\item c) Wie groß ist die Wahrscheinlichkeit für höchstens 2 defekte Teile?
\end{itemize}

\textbf{Lösung:}
\begin{enumerate}
\item \textbf{Parameter:}
   \begin{itemize}
   \item n = 100 (Stichprobenumfang)
   \item p = 0.05 (Ausschusswahrscheinlichkeit)
   \item X $\thicksim$ B(100, 0.05)
   \end{itemize}

\item \textbf{Erwartungswert:}
   $$E(X) = np = 100 \cdot 0.05 = 5$$

\item \textbf{Genau 3 defekte:}
   $$P(X=3) = \binom{100}{3}(0.05)^3(0.95)^{97} \approx 0.1404$$

\item \textbf{Höchstens 2 defekte:}
   $$P(X \leq 2) = \sum_{k=0}^2 \binom{100}{k}(0.05)^k(0.95)^{100-k} \approx 0.0861$$
\end{enumerate}
\end{example}

\begin{definition}{Binomialverteilung (Mit zurücklegen)}
\begin{itemize}
  \item $n$ = Anzahl Wiederholungen
  \item $p$ = Wahrscheinlichkeit für ein Ergebnis 1
  \item $q = 1-p$
\end{itemize}

$$P(X=x) = \binom{n}{x} \cdot p^x \cdot q^{n-x}$$

Schreibweise: $X \sim B(n;p)$

1. $\mu = E(X) = np$
2. $\sigma^2 = V(X) = npq$
3. $\sigma = S(X) = \sqrt{npq}$
\end{definition}

\begin{definition}{Binomialverteilung}
n-malige unabhängige Wiederholung eines Bernoulli-Experiments:
$$P(X=k) = \binom{n}{k} \cdot p^k \cdot q^{n-k}$$

\textbf{Parameter:}
\begin{itemize}
    \item $n$ = Anzahl Wiederholungen
    \item $p$ = Wahrscheinlichkeit für Erfolg
    \item $q = 1-p$ = Gegenwahrscheinlichkeit
\end{itemize}

\textbf{Kenngrößen:}
\begin{enumerate}
    \item $E(X) = np$
    \item $V(X) = npq$
    \item $\sigma = \sqrt{npq}$
\end{enumerate}

\textbf{Notation:} $X \sim B(n;p)$
\end{definition}

\begin{example}{Binomialverteilung in der Qualitätskontrolle}
\textbf{Aufgabe:} Ein Produktionsprozess hat eine Fehlerquote von 5\%. In einer Stichprobe von 100 Teilen:

\textbf{Parameter:}
\begin{itemize}
\item n = 100 (Stichprobenumfang)
\item p = 0.05 (Fehlerwahrscheinlichkeit)
\item X $\sim$  B(100, 0.05)
\end{itemize}

\textbf{Berechnung:}
\begin{itemize}
\item $E(X) = 100 \cdot 0.05 = 5$ defekte Teile erwartet
\item $V(X) = 100 \cdot 0.05 \cdot 0.95 = 4.75$
\item $P(X=3) = \binom{100}{3} \cdot 0.05^3 \cdot 0.95^{97} \approx 0.1754$
\item $P(X \leq 2) = \sum_{k=0}^2 \binom{100}{k} \cdot 0.05^k \cdot 0.95^{100-k} \approx 0.1247$
\end{itemize}
\end{example}

\begin{definition}{Hypergeometrische Verteilung (Ohne zurücklegen)}
\begin{itemize}
  \item $N$ = Objekte gesamthaft
  \item $M$ = Objekte einer bestimmten Sorte
  \item $n$ = Stichprobengrösse
  \item $x$ = Merkmalsträger
\end{itemize}

$$P(X=x)=\frac{\binom{M}{x} \cdot \binom{N-M}{n-x}}{\binom{N}{n}}$$

Schreibweise: $X \sim H(N,M,n)$

1. $\mu = E(X) = n \cdot \frac{M}{N}$
2. $\sigma^2 = V(X) = n \cdot \frac{M}{N} \cdot (1-\frac{M}{N}) \cdot \frac{N-n}{N-1}$
3. $\sigma = S(X) = \sqrt{V(X)}$
\end{definition}

\begin{definition}{Hypergeometrische Verteilung}
Ziehen ohne Zurücklegen aus einer endlichen Grundgesamtheit.

\textbf{Parameter:}
\begin{itemize}
    \item N: Grundgesamtheit
    \item M: Anzahl der Merkmalsträger
    \item n: Stichprobenumfang
\end{itemize}

$$P(X=k) = \frac{\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{n}}$$

\textbf{Kenngrößen:}
\begin{itemize}
    \item $E(X) = n\frac{M}{N}$
    \item $V(X) = n\frac{M}{N}(1-\frac{M}{N})\frac{N-n}{N-1}$
\end{itemize}

\textbf{Notation:} $X \sim H(N,M,n)$
\end{definition}

\begin{example}{Ziehung ohne Zurücklegen}
\textbf{Aufgabe:} In einer Urne sind 20 Kugeln, davon 8 rot. Es werden 5 Kugeln ohne Zurücklegen gezogen. 

\textbf{Lösung:}
\begin{enumerate}
\item \textbf{Parameter:}
   \begin{itemize}
   \item N = 20 (Gesamtanzahl)
   \item M = 8 (rote Kugeln)
   \item n = 5 (Ziehungen)
   \end{itemize}

\item \textbf{Erwartungswert:}
   $$E(X) = 5 \cdot \frac{8}{20} = 2$$

\item \textbf{Varianz:}
   $$V(X) = 5 \cdot \frac{8}{20} \cdot \frac{12}{20} \cdot \frac{15}{19} \approx 1.184$$

\item \textbf{P(genau 2 rote):}
   $$P(X=2) = \frac{\binom{8}{2}\binom{12}{3}}{\binom{20}{5}} \approx 0.3682$$
\end{enumerate}
\end{example}

\begin{definition}{Hypergeometrische Verteilung}
Ziehen ohne Zurücklegen:
$$P(X=k)=\frac{\binom{M}{k} \cdot \binom{N-M}{n-k}}{\binom{N}{n}}$$

\textbf{Parameter:}
\begin{itemize}
    \item $N$ = Grundgesamtheit
    \item $M$ = Anzahl Merkmalsträger
    \item $n$ = Stichprobengröße
    \item $k$ = Erfolge in Stichprobe
\end{itemize}

\textbf{Kenngrößen:}
\begin{enumerate}
    \item $E(X) = n \cdot \frac{M}{N}$
    \item $V(X) = n \cdot \frac{M}{N} \cdot (1-\frac{M}{N}) \cdot \frac{N-n}{N-1}$
    \item $\sigma = \sqrt{V(X)}$
\end{enumerate}

\textbf{Notation:} $X \sim H(N,M,n)$
\end{definition}

\begin{example}{Lotterie}
\textbf{Aufgabe:} In einer Urne sind 100 Lose, davon 10 Gewinnerlose. Ein Spieler zieht 5 Lose.

\textbf{Parameter:}
\begin{itemize}
\item N = 100 (Gesamtlose)
\item M = 10 (Gewinnerlose)
\item n = 5 (gezogene Lose)
\end{itemize}

\textbf{Berechnung:}
\begin{itemize}
\item $E(X) = 5 \cdot \frac{10}{100} = 0.5$ Gewinne erwartet
\item $V(X) = 5 \cdot \frac{10}{100} \cdot \frac{90}{100} \cdot \frac{95}{99} \approx 0.432$
\item $P(X=1) = \frac{\binom{10}{1} \cdot \binom{90}{4}}{\binom{100}{5}} \approx 0.3726$
\end{itemize}
\end{example}

\begin{definition}{Poisson Verteilung}
\begin{itemize}
  \item $\lambda$ = Rate
\end{itemize}

$$P(X=x) = \frac{\lambda^x}{x!} \cdot e^{-\lambda}, \quad \lambda > 0$$

Schreibweise: $X \sim Poi(\lambda)$

1. $\mu = E(X) = \lambda$
2. $\sigma^2 = V(X) = \lambda$
3. $\sigma = S(X) = \sqrt{\lambda}$
\end{definition}

\begin{definition}{Poisson-Verteilung}
Modelliert seltene Ereignisse in festem Zeit- oder Raumintervall.

$$P(X=k) = \frac{\lambda^k}{k!}e^{-\lambda}$$

\textbf{Parameter:}
\begin{itemize}
    \item $\lambda$: Erwartungswert pro Intervall
\end{itemize}

\textbf{Kenngrößen:}
\begin{itemize}
    \item $E(X) = \lambda$
    \item $V(X) = \lambda$
\end{itemize}

\textbf{Notation:} $X \sim Poi(\lambda)$
\end{definition}

\begin{definition}{Poisson-Verteilung}
Modelliert seltene Ereignisse:
$$P(X=k) = \frac{\lambda^k}{k!} \cdot e^{-\lambda}, \quad \lambda > 0$$

\textbf{Parameter:}
\begin{itemize}
    \item $\lambda$ = Erwartungswert/Rate
\end{itemize}

\textbf{Kenngrößen:}
\begin{enumerate}
    \item $E(X) = \lambda$
    \item $V(X) = \lambda$
    \item $\sigma = \sqrt{\lambda}$
\end{enumerate}

\textbf{Notation:} $X \sim Poi(\lambda)$
\end{definition}

\begin{example}{Anrufe in Call-Center}
\textbf{Aufgabe:} Ein Call-Center erhält durchschnittlich 4 Anrufe pro Stunde.

\textbf{Parameter:}
\begin{itemize}
\item $\lambda = 4$ (Anrufe pro Stunde)
\item X $\sim$ Poi(4)
\end{itemize}

\textbf{Berechnung:}
\begin{itemize}
\item $E(X) = 4$ Anrufe erwartet
\item $V(X) = 4$
\item $P(X=3) = \frac{4^3}{3!} \cdot e^{-4} \approx 0.1954$
\item $P(X \leq 2) = e^{-4} \cdot (1 + 4 + \frac{16}{2}) \approx 0.2381$
\end{itemize}
\end{example}

\begin{example}{Poisson-Verteilung in der Praxis}
\textbf{Aufgabe:} Ein Callcenter erhält durchschnittlich 3 Anrufe pro 10 Minuten.
\begin{itemize}
\item a) Wahrscheinlichkeit für genau 2 Anrufe in 10 Minuten?
\item b) Wahrscheinlichkeit für mehr als 4 Anrufe?
\end{itemize}

\textbf{Lösung:}
\begin{enumerate}
\item \textbf{Parameter:}
   \begin{itemize}
   \item $\lambda = 3$ (Erwartungswert)
   \item X $\thicksim$ Poi(3)
   \end{itemize}

\item \textbf{Genau 2 Anrufe:}
   $$P(X=2) = \frac{3^2}{2!}e^{-3} \approx 0.2240$$

\item \textbf{Mehr als 4 Anrufe:}
   $$P(X>4) = 1 - \sum_{k=0}^4 \frac{3^k}{k!}e^{-3} \approx 0.1847$$
\end{enumerate}
\end{example}

\begin{KR}{Wahl der richtigen Verteilung}
1. \textbf{Prüfe Ziehungsart}
   \begin{itemize}
   \item Mit Zurücklegen → Binomialverteilung
   \item Ohne Zurücklegen → Hypergeometrische Verteilung
   \item Seltene Ereignisse → Poisson-Verteilung
   \end{itemize}

2. \textbf{Prüfe Grundgesamtheit}
   \begin{itemize}
   \item Endlich, klein → Hypergeometrische Verteilung
   \item Sehr groß/unendlich → Binomialverteilung
   \item Zeitlich/räumlich kontinuierlich → Poisson-Verteilung
   \end{itemize}

3. \textbf{Beachte Approximationen}
   \begin{itemize}
   \item Binomial → Poisson für $n \to \infty$, $p \to 0$, $np = \lambda$
   \item Hypergeometrisch → Binomial für $\frac{n}{N} \leq 0.05$
   \end{itemize}
\end{KR}




\begin{KR}{Wahl der richtigen diskreten Verteilung}
1. \textbf{Bernoulli-Verteilung}
   \begin{itemize}
   \item Genau zwei mögliche Ausgänge
   \item Ein einzelner Versuch
   \item Konstante Erfolgswahrscheinlichkeit
   \end{itemize}

2. \textbf{Binomial-Verteilung}
   \begin{itemize}
   \item Feste Anzahl unabhängiger Versuche
   \item Mit Zurücklegen/große Grundgesamtheit
   \item Konstante Erfolgswahrscheinlichkeit
   \end{itemize}

3. \textbf{Hypergeometrische Verteilung}
   \begin{itemize}
   \item Ziehen ohne Zurücklegen
   \item Endliche Grundgesamtheit
   \item Veränderliche Wahrscheinlichkeiten
   \end{itemize}

4. \textbf{Poisson-Verteilung}
   \begin{itemize}
   \item Seltene Ereignisse
   \item Festes Zeitintervall/Raumbereich
   \item Rate $\lambda$ bekannt
   \end{itemize}
\end{KR}

\subsection{Normalverteilung}

\begin{definition}{Gauss-Verteilung}
Die Dichtefunktion der Normalverteilung ist:
$$\varphi_{\mu,\sigma}(x)=\frac{1}{\sqrt{2\pi} \cdot \sigma} \cdot e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$

\textbf{Standardnormalverteilung} ($\mu=0$, $\sigma=1$):
$$\varphi(x)=\frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{1}{2}x^2}$$

\textbf{Parameter:}
\begin{itemize}
    \item $\mu$ = Erwartungswert
    \item $\sigma$ = Standardabweichung
    \item $\varphi_{\mu,\sigma}(x)$ = Dichtefunktion
    \item $\phi_{\mu,\sigma}(x)$ = Verteilungsfunktion
\end{itemize}

\textbf{Notation:} $X \sim N(\mu;\sigma)$
\end{definition}

\begin{definition}{Gauss-Verteilung}
Die stetige Zufallsvariable $X$ folgt der Normalverteilung mit den Parametern $\mu, \sigma \in \mathbb{R}, \sigma>0$:
$$
\varphi_{\mu,\sigma}(x)=\frac{1}{\sqrt{2\pi} \cdot \sigma} \cdot e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
$$

Standardnormalverteilung ($\mu=0$ und $\sigma=1$):
$$
\varphi(x)=\frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{1}{2}x^2}
$$

$\varphi_{\mu,\sigma}(x)$ = Dichtefunktion der Normalverteilung\\
$\varphi(x)$ = Dichtefunktion der Standardnormalverteilung\\

$\mu$ = Erwartungswert\\
$\sigma$ = Standardabweichung\\
$e$ = Eulersche Zahl\\
$\pi$ = Kreiszahl Pi\\
\end{definition}
\begin{definition}{Die Verteilungsfunktion der Normalverteilung}\\
Die kumulative Verteilungsfunktion (CDF) von $\varphi_{\mu,\sigma}(x)$ wird mit $\phi_{\mu,\sigma}(x)$ bezeichnet. Sie ist definiert durch:
$$
\phi_{\mu,\sigma}(x) = P(X \leq x) = \int_{-\infty}^x \varphi_{\mu,\sigma}(t)dt = \frac{1}{\sqrt{2\pi} \cdot \sigma} \cdot \int_{-\infty}^x e^{-\frac{1}{2}(\frac{t-\mu}{\sigma})^2} dt
$$
\\
$\phi_{\mu,\sigma}(x)$ = Verteilungsfunktion der Normalverteilung\\
$\varphi_{\mu,\sigma}(x)$ = Dichtefunktion der Normalverteilung\\
$P(X \leq x)$ = Wahrscheinlichkeit dass $X$ kleiner oder gleich $x$ ist\\
$\mu$ = Erwartungswert\\
$\sigma$ = Standardabweichung\\
$\pi$ = Kreiszahl Pi\\
$e$ = Eulersche Zahl\\
\end{definition}

\begin{theorem}{Approximation durch die Normalverteilung}
\begin{itemize}
  \item Binomialverteilung: $\quad \mu=np, \sigma^2=npq$
  \item Poissonverteilung: $\quad \mu=\lambda, \sigma^2=\lambda$
\end{itemize}

$$
P(a \leq X \leq b)=\sum_{x=a}^{b} P(X=x) \approx \phi_{\mu,\sigma}(b+\frac{1}{2})-\phi_{\mu,\sigma}(a-\frac{1}{2})
$$
$P(a \leq X \leq b)$ = Wahrscheinlichkeit dass $X$ zwischen $a$ und $b$ liegt\\
$\phi_{\mu,\sigma}$ = Verteilungsfunktion der Normalverteilung\\
$a, b$ = Untere und obere Grenze\\
\end{theorem}
\begin{definition}{Standardisierung der Normalverteilung}\\
Bei einer stetigen Zufallsvariable $X$ lässt sich die Verteilungsfunktion als Integral einer Funktion $f$ darstellen:
$$
F(x) = P(X \leq x) = \int_{-\infty}^x f(u) \cdot du
$$

Liegt eine beliebige Normalverteilung $N(\mu,\sigma)$ vor, muss standardisiert werden. Statt ursprünglichen Zufallsvariablen $X$ betrachtet man die Zufallsvariable:
$$
U = \frac{X-\mu}{\sigma}
$$
\\
$F(x)$ = Verteilungsfunktion\\
$P(X \leq x)$ = Wahrscheinlichkeit dass $X$ kleiner oder gleich $x$ ist\\
$f(u)$ = Dichtefunktion\\
$U$ = Standardisierte Zufallsvariable\\
$X$ = Ursprüngliche Zufallsvariable\\
$\mu$ = Erwartungswert\\
$\sigma$ = Standardabweichung\\
\end{definition}
\begin{definition}{Erwartungswert und Varianz der Normalverteilung}\\
Für eine Zufallsvariable $X \sim N(\mu;\sigma)$ gilt:
$$
E(X) = \mu, \quad V(X) = \sigma^2
$$
\\
$E(X)$ = Erwartungswert der Zufallsvariable $X$\\
$V(X)$ = Varianz der Zufallsvariable $X$\\
$\mu$ = Erwartungsparameter\\
$\sigma^2$ = Varianzparameter\\
\end{definition}

\begin{KR}{Arbeiten mit der Normalverteilung}
1. \textbf{Standardisierung}
   \begin{itemize}
   \item $Z = \frac{X-\mu}{\sigma}$ transformiert zu N(0,1)
   \item Benutze Tabelle der Standardnormalverteilung
   \item Beachte: $\phi(z) = 1 - \phi(-z)$
   \end{itemize}

2. \textbf{Stetigkeitskorrektur}
   \begin{itemize}
   \item Bei Approximation diskreter Verteilungen
   \item Untere Grenze: $a - 0.5$
   \item Obere Grenze: $b + 0.5$
   \end{itemize}

3. \textbf{Faustregel für Intervalle}
   \begin{itemize}
   \item $\mu \pm \sigma$: ca. 68% der Werte
   \item $\mu \pm 2\sigma$: ca. 95% der Werte
   \item $\mu \pm 3\sigma$: ca. 99.7% der Werte
   \end{itemize}
\end{KR}

\begin{example}{Körpergrößen}
\textbf{Aufgabe:} Körpergrößen in einer Population sind normalverteilt mit $\mu = 175$ cm und $\sigma = 10$ cm.

\textbf{Berechnung:}
\begin{itemize}
\item $P(X \leq 185) = \phi(\frac{185-175}{10}) = \phi(1) \approx 0.8413$
\item $P(165 \leq X \leq 185) = \phi(1) - \phi(-1) \approx 0.6826$
\item $P(X > 195) = 1 - \phi(2) \approx 0.0228$
\end{itemize}
\end{example}

\subsection{Zentraler Grenzwertsatz und Approximationen}

\begin{theorem}{Zentraler Grenzwertsatz}
Für eine Folge von Zufallsvariablen $X_1, X_2, \ldots, X_n$ mit gleichem Erwartungswert $\mu$ und gleicher Varianz $\sigma^2$ gilt für die Summe $S_n$ und das arithmetische Mittel $\bar{X}_n$:

$$E(S_n)=n \cdot \mu, \quad V(S_n)=n \cdot \sigma^2$$
$$E(\bar{X}_n)=\mu, \quad V(\bar{X}_n)=\frac{\sigma^2}{n}=\frac{1}{n^2} \cdot V(S_n)$$

Die standardisierte Zufallsvariable ist:
$$U_n=\frac{(X_1+X_2+\cdots+X_n)-n\mu}{\sqrt{n} \cdot \sigma}=\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}$$

Für $n \to \infty$ konvergiert die Verteilungsfunktion $F_n(u)$ gegen die Standardnormalverteilung:
$$\lim_{n\to\infty} F_n(u) = \phi(u) = \frac{1}{\sqrt{2\pi}} \cdot \int_{-\infty}^u e^{-\frac{1}{2}t^2} dt$$
\end{theorem}

\begin{KR}{Anwendung des Zentralen Grenzwertsatzes}
1. \textbf{Prüfe Voraussetzungen}
   \begin{itemize}
   \item Unabhängige Zufallsvariablen
   \item Identische Verteilung
   \item Endliche Varianz
   \item Genügend große Stichprobe (n $\geq$ 30)
   \end{itemize}

2. \textbf{Berechne Parameter}
   \begin{itemize}
   \item $\mu_{S_n} = n\mu$
   \item $\sigma_{S_n} = \sqrt{n}\sigma$
   \item $\mu_{\bar{X}} = \mu$
   \item $\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}$
   \end{itemize}

3. \textbf{Standardisiere}
   \begin{itemize}
   \item Transformiere zu $Z = \frac{X-\mu}{\sigma}$
   \item Verwende Tabelle der Standardnormalverteilung
   \end{itemize}
\end{KR}

\begin{theorem}{Zentraler Grenzwertsatz}\\
Für eine Folge von Zufallsvariablen $X_1, X_2, \ldots, X_n$ mit gleichem Erwartungswert $\mu$ und gleicher Varianz $\sigma^2$ gilt:
$$
E(S_n)=n \cdot \mu, \quad V(S_n)=n \cdot \sigma^2, \quad E(\bar{X}_n)=\mu, \quad V(\bar{X}_n)=\frac{\sigma^2}{n}=\frac{1}{n^2} \cdot V(S_n)
$$
$S_n$ = Summe der Zufallsvariablen\\
$\bar{X}_n$ = Arithmetisches Mittel der Zufallsvariablen\\
$n$ = Anzahl der Zufallsvariablen\\
$\mu$ = Erwartungswert der einzelnen Zufallsvariablen\\
$\sigma^2$ = Varianz der einzelnen Zufallsvariablen\\

Die standardisierte Zufallsvariable:
$$
U_n=\frac{((X_1+X_2+\cdots+X_n)-n\mu)}{\sqrt{n} \cdot \sigma}=\frac{(\bar{X}-\mu)}{\frac{\sigma}{\sqrt{n}}}
$$

Sind die Zufallsvariablen alle identisch $N(\mu,\sigma)$ verteilt, so sind die Summe $S_n$ und das arithmetische Mittel $\bar{X}_n$ wieder normalverteilt mit:
\begin{itemize}
  \item $S_n: \quad N(n \cdot \mu, \sqrt{n} \cdot \sigma)$
  \item $\bar{X}_n: \quad N(\mu, \frac{\sigma}{\sqrt{n}})$
\end{itemize}

Verteilungsfunktion $F_n(u)$ konvergiert für $n \to \infty$ gegen die Verteilungsfunktion $\phi(u)$ der Standardnormalverteilung:
$$
\lim_{n\to\infty} F_n(u) = \phi(u) = \frac{1}{\sqrt{2\pi}} \cdot \int_{-\infty}^u e^{-\frac{1}{2}t^2} dt
$$
\end{theorem}

\begin{concept}{Faustregeln für Approximationen}
\begin{itemize}
  \item Die Approximation (Binomialverteilung) kann verwendet werden, wenn $npq > 9$
  \item Für grosses $n(n \geq 50)$ und kleines $p(p \leq 0.1)$ kann die Binomial- durch die Poisson-Verteilung approximiert werden:
  $$
  B(n,p) \approx \text{Poi}(n \cdot p)
  $$
  $B(n,p)$ = Binomialverteilung\\
  $\text{Poi}(\lambda)$ = Poissonverteilung mit Parameter $\lambda = n \cdot p$\\
  
  \item Eine Hypergeometrische Verteilung kann durch eine Binomialverteilung angenähert werden, wenn $n \leq \frac{N}{20}$:
  $$
  H(N,M,n) \approx B(n,\frac{M}{N})
  $$
  $H(N,M,n)$ = Hypergeometrische Verteilung\\
  $B(n,p)$ = Binomialverteilung\\
  $N$ = Grundgesamtheit\\
  $M$ = Anzahl der Erfolge in der Grundgesamtheit\\
  $n$ = Stichprobengröße\\
\end{itemize}
\end{concept}

\begin{concept}{Faustregeln für Approximationen}
\textbf{Binomialverteilung durch Normalverteilung:}
\begin{itemize}
  \item Bedingung: $npq > 9$
  \item Parameter: $\mu = np$, $\sigma^2 = npq$
  \item Stetigkeitskorrektur beachten!
\end{itemize}

\textbf{Binomialverteilung durch Poissonverteilung:}
\begin{itemize}
  \item Bedingung: $n \geq 50$ und $p \leq 0.1$
  \item $B(n,p) \approx Poi(n \cdot p)$
\end{itemize}

\textbf{Hypergeometrische durch Binomialverteilung:}
\begin{itemize}
  \item Bedingung: $n \leq \frac{N}{20}$
  \item $H(N,M,n) \approx B(n,\frac{M}{N})$
\end{itemize}
\end{concept}

\begin{example}{Approximation der Binomialverteilung}
\textbf{Aufgabe:} Eine Produktionsanlage produziert mit Ausschusswahrscheinlichkeit 5\%. In einer Charge von 200 Teilen:
\begin{itemize}
\item Wie groß ist die Wahrscheinlichkeit für 15 oder mehr defekte Teile?
\end{itemize}

\textbf{Lösung:}
\begin{enumerate}
\item \textbf{Prüfung Approximationsbedingung:}
   \begin{itemize}
   \item $npq = 200 \cdot 0.05 \cdot 0.95 = 9.5 > 9$
   \item Normalapproximation ist zulässig
   \end{itemize}

\item \textbf{Parameter der Normalverteilung:}
   \begin{itemize}
   \item $\mu = np = 200 \cdot 0.05 = 10$
   \item $\sigma = \sqrt{npq} = \sqrt{9.5} \approx 3.08$
   \end{itemize}

\item \textbf{Berechnung mit Stetigkeitskorrektur:}
   \begin{align*}
   P(X \geq 15) &= 1 - P(X \leq 14) \\
   &= 1 - P(X \leq 14.5) \\
   &= 1 - \phi(\frac{14.5-10}{3.08}) \\
   &= 1 - \phi(1.46) \\
   &\approx 0.0721
   \end{align*}
\end{enumerate}
\end{example}

\begin{example}{Approximation durch Poissonverteilung}
\textbf{Aufgabe:} Ein seltener Gendefekt tritt mit Wahrscheinlichkeit p = 0.001 auf. In einer Gruppe von 2000 Menschen:
\begin{itemize}
\item Wie groß ist die Wahrscheinlichkeit für genau 3 Betroffene?
\end{itemize}

\textbf{Lösung:}
\begin{enumerate}
\item \textbf{Prüfung Approximationsbedingung:}
   \begin{itemize}
   \item $n = 2000 \geq 50$ und $p = 0.001 \leq 0.1$
   \item Poissonapproximation ist zulässig
   \end{itemize}

\item \textbf{Parameter:}
   \begin{itemize}
   \item $\lambda = np = 2000 \cdot 0.001 = 2$
   \end{itemize}

\item \textbf{Berechnung:}
   $$P(X = 3) = \frac{2^3}{3!} \cdot e^{-2} \approx 0.180$$

\item \textbf{Vergleich mit Binomialverteilung:}
   $$P_{Bin}(X = 3) = \binom{2000}{3} \cdot 0.001^3 \cdot 0.999^{1997} \approx 0.180$$
\end{enumerate}
\end{example}

\begin{KR}{Entscheidung über Approximationen}
1. \textbf{Prüfe Stichprobenumfang}
   \begin{itemize}
   \item Klein (n < 30): Exakte Verteilung
   \item Mittel (30 $\leq$ n < 50): Je nach p
   \item Groß (n $\geq$ 50): Approximation möglich
   \end{itemize}

2. \textbf{Prüfe Wahrscheinlichkeit}
   \begin{itemize}
   \item p $\leq$ 0.1: Poisson möglich
   \item 0.1 < p < 0.9: Normal möglich
   \item npq > 9: Normal empfohlen
   \end{itemize}

3. \textbf{Wähle Approximation}
   \begin{itemize}
   \item Binomial → Normal: Große Stichproben, mittleres p
   \item Binomial → Poisson: Große n, kleines p
   \item Hypergeometrisch → Binomial: Kleine Stichprobe relativ zur Grundgesamtheit
   \end{itemize}

4. \textbf{Beachte}
   \begin{itemize}
   \item Stetigkeitskorrektur bei Normal
   \item Rundungsregeln bei Grenzen
   \item Vergleich mit exakter Lösung wenn möglich
   \end{itemize}
\end{KR}

