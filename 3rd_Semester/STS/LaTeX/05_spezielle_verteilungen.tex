\section{Spezielle Verteilungen}
\begin{definition}{Verteilungen und Erwartungswerte}
Für diskrete Verteilungen:
$$
\begin{gathered}
E(X)=\sum_{x \in \mathbb{R}} f(x) \cdot x \\
V(X)=\sum_{x \in \mathbb{R}} f(x) \cdot(x-E(X))^2
\end{gathered}
$$
$E(X)$ = Erwartungswert der Zufallsvariable $X$\\
$V(X)$ = Varianz der Zufallsvariable $X$\\
$f(x)$ = Wahrscheinlichkeitsfunktion\\
$x$ = Mögliche Werte der Zufallsvariable\\

Für stetige Verteilungen:
$$
\begin{gathered}
E(X)=\int_{-\infty}^{\infty} f(x) \cdot x dx \\
V(X)=\int_{-\infty}^{\infty} f(x) \cdot(x-E(X))^2 dx
\end{gathered}
$$
$E(X)$ = Erwartungswert der Zufallsvariable $X$\\
$V(X)$ = Varianz der Zufallsvariable $X$\\
$f(x)$ = Dichtefunktion\\
$x$ = Mögliche Werte der Zufallsvariable\\
\end{definition}

\begin{definition}{Bernoulliverteilung}
Bernoulli-Experimente sind Zufallsexperimente mit nur zwei möglichen Ergebnissen (1 und 0):
$$
P(X=1)=p, \quad P(X=0)=1-p=q
$$
Es gilt:
\begin{enumerate}
	\item $E(X)=E(X^2)=p$
	\item $V(X)=p \cdot(1-p)$
\end{enumerate}
\hfill \break
$E(X)$ = Erwartungswert\\
$V(X)$ = Varianz\\
$P(X=1)$ = Wahrscheinlichkeit für Erfolg\\
$p$ = Erfolgswahrscheinlichkeit\\
$q$ = Gegenwahrscheinlichkeit $(1-p)$\\
\end{definition}


\subsection{Normalverteilung}
\begin{definition}{Gauss-Verteilung}
Die stetige Zufallsvariable $X$ folgt der Normalverteilung mit den Parametern $\mu, \sigma \in \mathbb{R}, \sigma>0$:
$$
\varphi_{\mu,\sigma}(x)=\frac{1}{\sqrt{2\pi} \cdot \sigma} \cdot e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
$$

Standardnormalverteilung ($\mu=0$ und $\sigma=1$):
$$
\varphi(x)=\frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{1}{2}x^2}
$$

$\varphi_{\mu,\sigma}(x)$ = Dichtefunktion der Normalverteilung\\
$\varphi(x)$ = Dichtefunktion der Standardnormalverteilung\\

$\mu$ = Erwartungswert\\
$\sigma$ = Standardabweichung\\
$e$ = Eulersche Zahl\\
$\pi$ = Kreiszahl Pi\\
\end{definition}

\begin{theorem}{Approximation durch die Normalverteilung}
\begin{itemize}
  \item Binomialverteilung: $\quad \mu=np, \sigma^2=npq$
  \item Poissonverteilung: $\quad \mu=\lambda, \sigma^2=\lambda$
\end{itemize}

$$
P(a \leq X \leq b)=\sum_{x=a}^{b} P(X=x) \approx \phi_{\mu,\sigma}(b+\frac{1}{2})-\phi_{\mu,\sigma}(a-\frac{1}{2})
$$
$P(a \leq X \leq b)$ = Wahrscheinlichkeit dass $X$ zwischen $a$ und $b$ liegt\\
$\phi_{\mu,\sigma}$ = Verteilungsfunktion der Normalverteilung\\
$a, b$ = Untere und obere Grenze\\
\end{theorem}

\begin{theorem}{Zentraler Grenzwertsatz}
Für eine Folge von Zufallsvariablen $X_1, X_2, \ldots, X_n$ mit gleichem Erwartungswert $\mu$ und gleicher Varianz $\sigma^2$ gilt:
$$
E(S_n)=n \cdot \mu, \quad V(S_n)=n \cdot \sigma^2, \quad E(\bar{X}_n)=\mu, \quad V(\bar{X}_n)=\frac{\sigma^2}{n}
$$
$S_n$ = Summe der Zufallsvariablen\\
$\bar{X}_n$ = Arithmetisches Mittel der Zufallsvariablen\\
$n$ = Anzahl der Zufallsvariablen\\
$\mu$ = Erwartungswert der einzelnen Zufallsvariablen\\
$\sigma^2$ = Varianz der einzelnen Zufallsvariablen\\

Die standardisierte Zufallsvariable:
$$
U_n=\frac{((X_1+X_2+\cdots+X_n)-n\mu)}{\sqrt{n} \cdot \sigma}=\frac{(\bar{X}-\mu)}{\frac{\sigma}{\sqrt{n}}}
$$
$U_n$ = Standardisierte Zufallsvariable\\
$X_1, X_2, \ldots, X_n$ = Einzelne Zufallsvariablen\\
$\bar{X}$ = Arithmetisches Mittel\\
\end{theorem}

\begin{concept}{Faustregeln für Approximationen}
\begin{itemize}
  \item Die Approximation (Binomialverteilung) kann verwendet werden, wenn $npq > 9$
  \item Für grosses $n(n \geq 50)$ und kleines $p(p \leq 0.1)$ kann die Binomial- durch die Poisson-Verteilung approximiert werden:
  $$
  B(n,p) \approx \text{Poi}(n \cdot p)
  $$
  $B(n,p)$ = Binomialverteilung\\
  $\text{Poi}(\lambda)$ = Poissonverteilung mit Parameter $\lambda = n \cdot p$\\
  
  \item Eine Hypergeometrische Verteilung kann durch eine Binomialverteilung angenähert werden, wenn $n \leq \frac{N}{20}$:
  $$
  H(N,M,n) \approx B(n,\frac{M}{N})
  $$
  $H(N,M,n)$ = Hypergeometrische Verteilung\\
  $B(n,p)$ = Binomialverteilung\\
  $N$ = Grundgesamtheit\\
  $M$ = Anzahl der Erfolge in der Grundgesamtheit\\
  $n$ = Stichprobengröße\\
\end{itemize}
\end{concept}
