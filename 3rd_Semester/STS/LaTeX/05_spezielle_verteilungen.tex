\section{Spezielle Verteilungen}
\begin{definition}{Verteilungen und Erwartungswerte}
Für diskrete Verteilungen:
$$
\begin{gathered}
E(X)=\sum_{x \in \mathbb{R}} f(x) \cdot x \\
V(X)=\sum_{x \in \mathbb{R}} f(x) \cdot(x-E(X))^2
\end{gathered}
$$
$E(X)$ = Erwartungswert der Zufallsvariable $X$\\
$V(X)$ = Varianz der Zufallsvariable $X$\\
$f(x)$ = Wahrscheinlichkeitsfunktion\\
$x$ = Mögliche Werte der Zufallsvariable\\

Für stetige Verteilungen:
$$
\begin{gathered}
E(X)=\int_{-\infty}^{\infty} f(x) \cdot x dx \\
V(X)=\int_{-\infty}^{\infty} f(x) \cdot(x-E(X))^2 dx
\end{gathered}
$$
$E(X)$ = Erwartungswert der Zufallsvariable $X$\\
$V(X)$ = Varianz der Zufallsvariable $X$\\
$f(x)$ = Dichtefunktion\\
$x$ = Mögliche Werte der Zufallsvariable\\
\end{definition}

\begin{definition}{Bernoulliverteilung}
Bernoulli-Experimente sind Zufallsexperimente mit nur zwei möglichen Ergebnissen (1 und 0):
$$
P(X=1)=p, \quad P(X=0)=1-p=q
$$
Es gilt:
\begin{enumerate}
	\item $E(X)=E(X^2)=p$
	\item $V(X)=p \cdot(1-p)$
\end{enumerate}
\hfill \break
$E(X)$ = Erwartungswert\\
$V(X)$ = Varianz\\
$P(X=1)$ = Wahrscheinlichkeit für Erfolg\\
$p$ = Erfolgswahrscheinlichkeit\\
$q$ = Gegenwahrscheinlichkeit $(1-p)$\\
\end{definition}


\subsection{Normalverteilung}
\begin{definition}{Gauss-Verteilung}
Die stetige Zufallsvariable $X$ folgt der Normalverteilung mit den Parametern $\mu, \sigma \in \mathbb{R}, \sigma>0$:
$$
\varphi_{\mu,\sigma}(x)=\frac{1}{\sqrt{2\pi} \cdot \sigma} \cdot e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
$$

Standardnormalverteilung ($\mu=0$ und $\sigma=1$):
$$
\varphi(x)=\frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{1}{2}x^2}
$$

$\varphi_{\mu,\sigma}(x)$ = Dichtefunktion der Normalverteilung\\
$\varphi(x)$ = Dichtefunktion der Standardnormalverteilung\\

$\mu$ = Erwartungswert\\
$\sigma$ = Standardabweichung\\
$e$ = Eulersche Zahl\\
$\pi$ = Kreiszahl Pi\\
\end{definition}
\begin{definition}{Die Verteilungsfunktion der Normalverteilung}\\
Die kumulative Verteilungsfunktion (CDF) von $\varphi_{\mu,\sigma}(x)$ wird mit $\phi_{\mu,\sigma}(x)$ bezeichnet. Sie ist definiert durch:
$$
\phi_{\mu,\sigma}(x) = P(X \leq x) = \int_{-\infty}^x \varphi_{\mu,\sigma}(t)dt = \frac{1}{\sqrt{2\pi} \cdot \sigma} \cdot \int_{-\infty}^x e^{-\frac{1}{2}(\frac{t-\mu}{\sigma})^2} dt
$$
\\
$\phi_{\mu,\sigma}(x)$ = Verteilungsfunktion der Normalverteilung\\
$\varphi_{\mu,\sigma}(x)$ = Dichtefunktion der Normalverteilung\\
$P(X \leq x)$ = Wahrscheinlichkeit dass $X$ kleiner oder gleich $x$ ist\\
$\mu$ = Erwartungswert\\
$\sigma$ = Standardabweichung\\
$\pi$ = Kreiszahl Pi\\
$e$ = Eulersche Zahl\\
\end{definition}

\begin{theorem}{Approximation durch die Normalverteilung}
\begin{itemize}
  \item Binomialverteilung: $\quad \mu=np, \sigma^2=npq$
  \item Poissonverteilung: $\quad \mu=\lambda, \sigma^2=\lambda$
\end{itemize}

$$
P(a \leq X \leq b)=\sum_{x=a}^{b} P(X=x) \approx \phi_{\mu,\sigma}(b+\frac{1}{2})-\phi_{\mu,\sigma}(a-\frac{1}{2})
$$
$P(a \leq X \leq b)$ = Wahrscheinlichkeit dass $X$ zwischen $a$ und $b$ liegt\\
$\phi_{\mu,\sigma}$ = Verteilungsfunktion der Normalverteilung\\
$a, b$ = Untere und obere Grenze\\
\end{theorem}
\begin{definition}{Standardisierung der Normalverteilung}\\
Bei einer stetigen Zufallsvariable $X$ lässt sich die Verteilungsfunktion als Integral einer Funktion $f$ darstellen:
$$
F(x) = P(X \leq x) = \int_{-\infty}^x f(u) \cdot du
$$

Liegt eine beliebige Normalverteilung $N(\mu,\sigma)$ vor, muss standardisiert werden. Statt ursprünglichen Zufallsvariablen $X$ betrachtet man die Zufallsvariable:
$$
U = \frac{X-\mu}{\sigma}
$$
\\
$F(x)$ = Verteilungsfunktion\\
$P(X \leq x)$ = Wahrscheinlichkeit dass $X$ kleiner oder gleich $x$ ist\\
$f(u)$ = Dichtefunktion\\
$U$ = Standardisierte Zufallsvariable\\
$X$ = Ursprüngliche Zufallsvariable\\
$\mu$ = Erwartungswert\\
$\sigma$ = Standardabweichung\\
\end{definition}
\begin{definition}{Erwartungswert und Varianz der Normalverteilung}\\
Für eine Zufallsvariable $X \sim N(\mu;\sigma)$ gilt:
$$
E(X) = \mu, \quad V(X) = \sigma^2
$$
\\
$E(X)$ = Erwartungswert der Zufallsvariable $X$\\
$V(X)$ = Varianz der Zufallsvariable $X$\\
$\mu$ = Erwartungsparameter\\
$\sigma^2$ = Varianzparameter\\
\end{definition}

\begin{theorem}{Zentraler Grenzwertsatz}\\
Für eine Folge von Zufallsvariablen $X_1, X_2, \ldots, X_n$ mit gleichem Erwartungswert $\mu$ und gleicher Varianz $\sigma^2$ gilt:
$$
E(S_n)=n \cdot \mu, \quad V(S_n)=n \cdot \sigma^2, \quad E(\bar{X}_n)=\mu, \quad V(\bar{X}_n)=\frac{\sigma^2}{n}=\frac{1}{n^2} \cdot V(S_n)
$$
$S_n$ = Summe der Zufallsvariablen\\
$\bar{X}_n$ = Arithmetisches Mittel der Zufallsvariablen\\
$n$ = Anzahl der Zufallsvariablen\\
$\mu$ = Erwartungswert der einzelnen Zufallsvariablen\\
$\sigma^2$ = Varianz der einzelnen Zufallsvariablen\\

Die standardisierte Zufallsvariable:
$$
U_n=\frac{((X_1+X_2+\cdots+X_n)-n\mu)}{\sqrt{n} \cdot \sigma}=\frac{(\bar{X}-\mu)}{\frac{\sigma}{\sqrt{n}}}
$$

Sind die Zufallsvariablen alle identisch $N(\mu,\sigma)$ verteilt, so sind die Summe $S_n$ und das arithmetische Mittel $\bar{X}_n$ wieder normalverteilt mit:
\begin{itemize}
  \item $S_n: \quad N(n \cdot \mu, \sqrt{n} \cdot \sigma)$
  \item $\bar{X}_n: \quad N(\mu, \frac{\sigma}{\sqrt{n}})$
\end{itemize}

Verteilungsfunktion $F_n(u)$ konvergiert für $n \to \infty$ gegen die Verteilungsfunktion $\phi(u)$ der Standardnormalverteilung:
$$
\lim_{n\to\infty} F_n(u) = \phi(u) = \frac{1}{\sqrt{2\pi}} \cdot \int_{-\infty}^u e^{-\frac{1}{2}t^2} dt
$$
\end{theorem}

\begin{concept}{Faustregeln für Approximationen}
\begin{itemize}
  \item Die Approximation (Binomialverteilung) kann verwendet werden, wenn $npq > 9$
  \item Für grosses $n(n \geq 50)$ und kleines $p(p \leq 0.1)$ kann die Binomial- durch die Poisson-Verteilung approximiert werden:
  $$
  B(n,p) \approx \text{Poi}(n \cdot p)
  $$
  $B(n,p)$ = Binomialverteilung\\
  $\text{Poi}(\lambda)$ = Poissonverteilung mit Parameter $\lambda = n \cdot p$\\
  
  \item Eine Hypergeometrische Verteilung kann durch eine Binomialverteilung angenähert werden, wenn $n \leq \frac{N}{20}$:
  $$
  H(N,M,n) \approx B(n,\frac{M}{N})
  $$
  $H(N,M,n)$ = Hypergeometrische Verteilung\\
  $B(n,p)$ = Binomialverteilung\\
  $N$ = Grundgesamtheit\\
  $M$ = Anzahl der Erfolge in der Grundgesamtheit\\
  $n$ = Stichprobengröße\\
\end{itemize}
\end{concept}
\begin{definition}{Hypergeometrische Verteilung (Ohne zurücklegen)}
\begin{itemize}
  \item $N$ = Objekte gesamthaft
  \item $M$ = Objekte einer bestimmten Sorte
  \item $n$ = Stichprobengrösse
  \item $x$ = Merkmalsträger
\end{itemize}

$$P(X=x)=\frac{\binom{M}{x} \cdot \binom{N-M}{n-x}}{\binom{N}{n}}$$

Schreibweise: $X \sim H(N,M,n)$

1. $\mu = E(X) = n \cdot \frac{M}{N}$
2. $\sigma^2 = V(X) = n \cdot \frac{M}{N} \cdot (1-\frac{M}{N}) \cdot \frac{N-n}{N-1}$
3. $\sigma = S(X) = \sqrt{V(X)}$
\end{definition}

\begin{definition}{Binomialverteilung (Mit zurücklegen)}
\begin{itemize}
  \item $n$ = Anzahl Wiederholungen
  \item $p$ = Wahrscheinlichkeit für ein Ergebnis 1
  \item $q = 1-p$
\end{itemize}

$$P(X=x) = \binom{n}{x} \cdot p^x \cdot q^{n-x}$$

Schreibweise: $X \sim B(n;p)$

1. $\mu = E(X) = np$
2. $\sigma^2 = V(X) = npq$
3. $\sigma = S(X) = \sqrt{npq}$
\end{definition}

\begin{definition}{Poisson Verteilung}
\begin{itemize}
  \item $\lambda$ = Rate
\end{itemize}

$$P(X=x) = \frac{\lambda^x}{x!} \cdot e^{-\lambda}, \quad \lambda > 0$$

Schreibweise: $X \sim Poi(\lambda)$

1. $\mu = E(X) = \lambda$
2. $\sigma^2 = V(X) = \lambda$
3. $\sigma = S(X) = \sqrt{\lambda}$
\end{definition}
