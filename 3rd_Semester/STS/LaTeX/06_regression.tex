\section{Methode der kleinsten Quadrate}
\begin{definition}{Lineare Regression}\\
Gegeben sind Datenpunkte $(x_i; y_i)$ mit $1 \leq i \leq n$. Die Residuen / Fehler $\epsilon_i=g(x_i)-y_i$ dieser Datenpunkte sind Abstände in $y$-Richtung zwischen $y_i$ und der Geraden $g$. Die Ausgleichs- oder Regressiongerade ist diejenige Gerade, für die die Summe der quadrierten Residuen $\sum_{i=1}^{n} \epsilon_i^2$ am kleinsten ist.\\
$(x_i, y_i)$ = Datenpunkte\\
$\epsilon_i$ = Residuum (Abweichung) des $i$-ten Datenpunkts\\
$g(x_i)$ = Wert der Regressionsgerade an der Stelle $x_i$\\
$n$ = Anzahl der Datenpunkte\\
\end{definition}

\begin{theorem}{Regressionsgerade}\\
Die Regressionsgerade $g(x)=mx+d$ mit den Parametern $m$ und $d$ ist die Gerade, für welche die Residualvarianz $s_{\epsilon}^2$ minimal ist.

$$
\text{Steigung: } m=\frac{s_{xy}}{s_x^2}, \quad \text{y-Achsenabschnitt: } d=\bar{y}-m\bar{x}, \quad s_{\epsilon}^2=s_y^2-\frac{s_{xy}^2}{s_x^2}
$$
$m$ = Steigung der Regressionsgerade\\
$d$ = y-Achsenabschnitt\\
$s_{xy}$ = Kovarianz von $x$ und $y$\\
$s_x^2$ = Varianz der $x$-Werte\\
$s_y^2$ = Varianz der $y$-Werte\\
$\bar{x}$ = Arithmetisches Mittel der $x$-Werte\\
$\bar{y}$ = Arithmetisches Mittel der $y$-Werte\\
$s_{\epsilon}^2$ = Residualvarianz\\
\end{theorem}

\subsection{Bestimmtheitsmass}
\begin{concept}{Varianzaufspaltung}\\
Die Totale Varianz setzt sich zusammen aus der Residualvarianz und der Varianz der prognostizierten Werte:
\begin{itemize}
  \item $s_y^2$ Totale Varianz
  \item $s_{\hat{y}}^2$ prognostizierte (erklärte) Varianz
  \item $s_{\epsilon}^2$ Residualvarianz
\end{itemize}

$$
s_y^2=s_{\epsilon}^2+s_{\hat{y}}^2
$$
$s_y^2$ = Totale Varianz der beobachteten $y$-Werte\\
$s_{\epsilon}^2$ = Varianz der Residuen\\
$s_{\hat{y}}^2$ = Varianz der durch die Regression geschätzten Werte\\
\end{concept}

\begin{theorem}{Bestimmtheitsmass}\\
Das Bestimmtheitsmass $R^2$ beurteilt die globale Anpassungsgüte einer Regression über den Anteil der prognostizierten Varianz $s_{\hat{y}}^2$ an der totalen Varianz $s_y^2$:
$$
R^2=\frac{s_{\hat{y}}^2}{s_y^2}
$$
$R^2$ = Bestimmtheitsmass (zwischen 0 und 1)\\
$s_{\hat{y}}^2$ = Varianz der prognostizierten Werte\\
$s_y^2$ = Totale Varianz\\

Das Bestimmtheitsmass $R^2$ entspricht dem Quadrat des Korrelationskoeffizienten:
$$
R^2=\frac{s_{xy}^2}{s_x^2 \cdot s_y^2}=(r_{xy})^2
$$
$s_{xy}$ = Kovarianz von $x$ und $y$\\
$s_x^2$ = Varianz der $x$-Werte\\
$s_y^2$ = Varianz der $y$-Werte\\
$r_{xy}$ = Korrelationskoeffizient\\
\end{theorem}

\subsection{Linearisierungsfunktionen}
\begin{concept}{Transformationen}\\
\begin{center}
\begin{tabular}{|c|c|}
\hline
Ausgangsfunktion & Transformation \\
\hline
$y=q \cdot x^m$ & $\log(y)=\log(q)+m \cdot \log(x)$ \\
\hline
$y=q \cdot m^x$ & $\log(y)=\log(q)+\log(m) \cdot x$ \\
\hline
$y=q \cdot e^{m \cdot x}$ & $\ln(y)=\ln(q)+m \cdot x$ \\
\hline
$y=\frac{1}{q+m \cdot x}$ & $V=q+m \cdot x; V=\frac{1}{y}$ \\
\hline
$y=q+m \cdot \ln(x)$ & $y=q+m \cdot U; u=\ln(x)$ \\
\hline
$y=\frac{1}{q \cdot m^x}$ & $\log(\frac{1}{y})=\log(q)+\log(m) \cdot x$ \\
\hline
\end{tabular}
\end{center}
$y$ = Abhängige Variable\\
$x$ = Unabhängige Variable\\
$q, m$ = Parameter der Funktion\\
$e$ = Eulersche Zahl\\
$\ln$ = Natürlicher Logarithmus\\
$\log$ = Logarithmus zur Basis 10\\
\end{concept}

\subsection{Methode der kleinsten Quadrate}

\begin{definition}{Matrix-Darstellung}\\
Die Parameter $m$ und $q$ der Regressionsgeraden werden mit der Matrix $A$ berechnet:
$$A = \begin{pmatrix} x_1 & 1 \\ \vdots & \vdots \\ x_n & 1 \end{pmatrix}, \quad A^T \cdot A \cdot \begin{pmatrix} m \\ q \end{pmatrix} = A^T \cdot \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix}$$
\end{definition}

\begin{formula}{Residuenberechnung}\\
Die Residuen $\epsilon_i$ ergeben sich als:
$$\epsilon_i = y_i - \hat{y}_i = y_i - (mx_i + q)$$

Die Summe der quadrierten Residuen wird minimiert:
$$\sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - (mx_i + q))^2 \rightarrow \text{min}$$
\end{formula}
