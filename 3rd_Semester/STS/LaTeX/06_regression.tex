\section{Methode der kleinsten Quadrate}
\begin{definition}{Lineare Regression}
Gegeben sind Datenpunkte $(x_i; y_i)$ mit $1 \leq i \leq n$. Die Residuen / Fehler $\epsilon_i=g(x_i)-y_i$ dieser Datenpunkte sind Abstände in $y$-Richtung zwischen $y_i$ und der Geraden $g$. Die Ausgleichs- oder Regressiongerade ist diejenige Gerade, für die die Summe der quadrierten Residuen $\sum_{i=1}^{n} \epsilon_i^2$ am kleinsten ist.
\end{definition}

\begin{theorem}{Regressionsgerade}
Die Regressionsgerade $g(x)=mx+d$ mit den Parametern $m$ und $d$ ist die Gerade, für welche die Residualvarianz $s_{\epsilon}^2$ minimal ist.

$$
\text{Steigung: } m=\frac{s_{xy}}{s_x^2}, \quad \text{y-Achsenabschnitt: } d=\bar{y}-m\bar{x}, \quad s_{\epsilon}^2=s_y^2-\frac{s_{xy}^2}{s_x^2}
$$
\end{theorem}

\section{Bestimmtheitsmass}
\begin{concept}{Varianzaufspaltung}
Die Totale Varianz setzt sich zusammen aus der Residualvarianz und der Varianz der prognostizierten Werte:
\begin{itemize}
  \item $s_y^2$ Totale Varianz
  \item $s_{\hat{y}}^2$ prognostizierte (erklärte) Varianz
  \item $s_{\epsilon}^2$ Residualvarianz
\end{itemize}

$$
s_y^2=s_{\epsilon}^2+s_{\hat{y}}^2
$$
\end{concept}

\begin{theorem}{Bestimmtheitsmass}
Das Bestimmtheitsmass $R^2$ beurteilt die globale Anpassungsgüte einer Regression über den Anteil der prognostizierten Varianz $s_{\hat{y}}^2$ an der totalen Varianz $s_y^2$:
$$
R^2=\frac{s_{\hat{y}}^2}{s_y^2}
$$

Das Bestimmtheitsmass $R^2$ entspricht dem Quadrat des Korrelationskoeffizienten:
$$
R^2=\frac{s_{xy}^2}{s_x^2 \cdot s_y^2}=(r_{xy})^2
$$
\end{theorem}

\section{Linearisierungsfunktionen}
\begin{concept}{Transformationen}
\begin{center}
\begin{tabular}{|c|c|}
\hline
Ausgangsfunktion & Transformation \\
\hline
$y=q \cdot x^m$ & $\log(y)=\log(q)+m \cdot \log(x)$ \\
\hline
$y=q \cdot m^x$ & $\log(y)=\log(q)+\log(m) \cdot x$ \\
\hline
$y=q \cdot e^{m \cdot x}$ & $\ln(y)=\ln(q)+m \cdot x$ \\
\hline
$y=\frac{1}{q+m \cdot x}$ & $V=q+m \cdot x; V=\frac{1}{y}$ \\
\hline
$y=q+m \cdot \ln(x)$ & $y=q+m \cdot U; u=\ln(x)$ \\
\hline
$y=\frac{1}{q \cdot m^x}$ & $\log(\frac{1}{y})=\log(q)+\log(m) \cdot x$ \\
\hline
\end{tabular}
\end{center}
\end{concept}
