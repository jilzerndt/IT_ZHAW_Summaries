\section{Die Methode der kleinsten Quadrate}

\subsection{Einführung}
\begin{concept}{Einführung}\\
Weit verbreitete Optimierungsmethode zur Modellierung mathematischer Zusammenhänge in großen Datenmengen. 
Ziel: optimale Parameter zu finden, die funktionalen Zusammenhang zwischen Messdaten am besten beschreiben. 

Lineare Regression: linearer Zusammenhang zwischen Daten vermutet und versucht, optimale Gerade in Datenmenge einzupassen.
\end{concept}

\subsection{Lineare Regression}

\begin{definition}{Lineare Regression}\\
Gegeben sind Datenpunkte $(x_i; y_i)$ mit $1 \leq i \leq n$, die näherungsweise auf einer Geraden liegen. 

Die Residuen oder Fehler $\epsilon_i = y_i - g(x_i)$ dieser Datenpunkte sind die Abstände in $y$-Richtung zwischen $y_i$ und der Geraden $g$.\\

Die ''bestmögliche'' Gerade, die Ausgleichs- oder Regressionsgerade, ist diejenige Gerade, für die die Summe der quadrierten Residuen $\sum_{i=1}^n \epsilon_i^2$ am kleinsten ist:
\vspace{-4mm}\\
\[\sum_{i=1}^n (y_i - g(x_i))^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2\]
Die Residuen $\epsilon_i$ ergeben sich als:
$\epsilon_i = y_i - \hat{y}_i = y_i - (mx_i + q)$
\end{definition}

\begin{remark}
$y_i$: beobachtete $y$-Werte\\
$\hat{y}_i$: prognostizierte bzw. erklärte $y$-Werte\\
$\epsilon_i$: Residuum (oder auch Fehler/Abweichung) des $i$-ten Datenpunktes\\
$g(x_i)$ = Wert der Regressionsgerade an der Stelle $x_i$\\
$n$ = Anzahl der Datenpunkte\\
$(x_i, y_i)$ = Datenpunkte
\end{remark}


\begin{theorem}{Parameter der Regressionsgerade}\\
Die Regressionsgerade $g(x) = mx + d$ mit den Parametern $m$ und $d$ ist die Gerade, für die die Residualvarianz $\tilde{s}_\epsilon^2$ minimal ist.

\textbf{Parameter:}\\
Steigung: $m = \frac{\tilde{s}_{xy}}{\tilde{s}_x^2}$,
y-Achsenabschnitt: $d = \bar{y} - m\bar{x}$\\

\textbf{Wichtige Kenngrößen:}
\vspace{2mm}\\
Arithmetische Mittel: $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ und $\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i$\\

Varianz der $x_i$-Werte:
$\tilde{s}_x^2 = \frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})^2 = (\frac{1}{n}\sum_{i=1}^n x_i^2) - \bar{x}^2$\\

Varianz der $y_i$-Werte:
$\tilde{s}_y^2 = \frac{1}{n}\sum_{i=1}^n (y_i-\bar{y})^2 = (\frac{1}{n}\sum_{i=1}^n y_i^2) - \bar{y}^2$\\

Kovarianz:
$\tilde{s}_{xy} = \frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}) = (\frac{1}{n}\sum_{i=1}^n x_iy_i) - \bar{x}\bar{y}$\\

Residualvarianz:
$\tilde{s}_\epsilon^2 = \tilde{s}_y^2 - \frac{\tilde{s}_{xy}^2}{\tilde{s}_x^2}$\\
\end{theorem}

\begin{KR}{Lineare Regression berechnen}
\begin{enumerate}
  \setlength{\itemsep}{1pt}
\item Berechne arithmetische Mittel $\bar{x}$ und $\bar{y}$
\item Berechne Kovarianzen und Varianzen
\item Berechne Steigung $m$ und y-Achsenabschnitt $d$:
   \begin{itemize}
     \item $m = \frac{s_{xy}}{s_x^2}$, $d = \bar{y} - m\bar{x}$
   \end{itemize}
\item Regressionsgerade: $g(x) = mx + d$
\end{enumerate}
\end{KR}

\begin{example2}{Lineare Regression}
Gegeben sind die Datenpunkte:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$x_i$ & 1 & 2 & 3 & 4 & 5 \\
\hline
$y_i$ & 2.1 & 4.0 & 6.3 & 7.8 & 9.9 \\
\hline
\end{tabular}
\end{center}

1. $\bar{x} = 3$, $\bar{y} = 6.02$

2. Kovarianzen und Varianzen:
   \begin{itemize}
     \item $s_{xy} = 3.945$, $s_x^2 = 2$, $s_y^2 = 8.4916$
   \end{itemize}

3. Parameter:
   \begin{itemize}
     \item $m = \frac{3.945}{2} = 1.9725$, $d = 6.02 - 1.9725 \cdot 3 = 0.1025$
   \end{itemize}

4. Regressionsgerade: $g(x) = 1.9725x + 0.1025$
\end{example2}

\subsubsection{Varianzzerlegung und Bestimmtheitsmass}

\begin{concept}{Varianzzerlegung}\\
Die Totale Varianz setzt sich zusammen aus der Residualvarianz und der Varianz der prognostizierten Werte:
$$\tilde{s}_y^2 = \tilde{s}_\epsilon^2 + \tilde{s}_{\hat{y}}^2$$
$\tilde{s}_{\hat{y}}^2$: prognostizierte (erklärte) Varianz,
$\tilde{s}_\epsilon^2$: Residualvarianz
\end{concept}

\begin{theorem}{Bestimmtheitsmass} $R^2$ (zwischen 0 und 1)\\
Das Bestimmtheitsmass $R^2$ beurteilt die globale Anpassungsgüte einer Regression über den Anteil der prognostizierten Varianz $s_{\hat{y}}^2$ an der totalen Varianz $s_y^2$:
$$
R^2=\frac{s_{\hat{y}}^2}{s_y^2}
$$
$s_{\hat{y}}^2$ = Varianz der prognostizierten Werte,
$s_y^2$ = Totale Varianz\\

Das Bestimmtheitsmass $R^2$ entspricht dem Quadrat des Korrelationskoeffizienten (nach Bravais-Pearson):
$$
R^2=\frac{s_{xy}^2}{s_x^2 \cdot s_y^2}=(r_{xy})^2
$$
$s_x^2$ = Varianz der $x$-Werte,
$s_y^2$ = Varianz der $y$-Werte\\
$s_{xy}$ = Kovarianz von $x$ und $y$\\ $r_{xy}$ = Korrelationskoeffizient
\end{theorem}

\begin{corollary}{Interpretation des Bestimmtheitsmasses}
\begin{itemize}
  \item $R^2 = 0.75$ bedeutet, dass 75\% der gesamten Varianz durch die Regression erklärt sind
  \item Die restlichen 25\% sind Zufallsstreuung
\end{itemize}
\end{corollary}

\begin{KR}{Bestimmtheitsmass berechnen}\\
1. Berechne die totale Varianz $s_y^2$
2. Berechne die Residualvarianz $s_{\epsilon}^2$
3. Berechne die erklärte Varianz $s_{\hat{y}}^2$
4. Berechne das Bestimmtheitsmass:
   $$R^2 = \frac{s_{\hat{y}}^2}{s_y^2} = 1 - \frac{s_{\epsilon}^2}{s_y^2}$$
5. Interpretation:
   \begin{itemize}
     \item $R^2 \approx 1$: Sehr gute Anpassung, $R^2 \approx 0$: Schlechte Anpassung
   \end{itemize}
\end{KR}

\subsubsection{Residuenbetrachtung}

\begin{concept}{Residuenplot}\\
Die Residuen werden bezogen auf die prognostizierten y-Werte $\hat{y}$ dargestellt. Auf der horizontalen Achse werden die prognostizierten y-Werte $\hat{y}$ und auf der vertikalen Achse die Residuen angetragen.

Beurteilungskriterien:\\
- Residuen sollten unsystematisch (d.h. zufällig) streuen\\
- Überall etwa gleich um die horizontale Achse streuen\\
- Betragsmäßig kleine Residuen sollten häufiger sein als große
\end{concept}

\begin{KR}{Residuen und Residuenplot analysieren}\\
1. Berechne die Residuen für jeden Datenpunkt:
   \begin{itemize}
     \item $\epsilon_i = y_i - (mx_i + d)$
   \end{itemize}
2. Erstelle Residuenplot:
   \begin{itemize}
     \item x-Achse: Prognostizierte Werte $\hat{y}_i = mx_i + d$
     \item y-Achse: Residuen $\epsilon_i$
   \end{itemize}
3. Prüfe Eigenschaften:
   \begin{itemize}
     \item Residuen sollten zufällig um Null streuen
     \item Keine systematischen Muster erkennbar
     \item Gleiche Streubreite über alle $\hat{y}_i$
   \end{itemize}
\end{KR}

\begin{formula}{Gütekriterien für Regression}\\
1. Bestimmtheitsmass $R^2$:
   \begin{itemize}
     \item $R^2 > 0.9$: Sehr gute Anpassung
     \item $0.7 < R^2 < 0.9$: Gute Anpassung
     \item $0.5 < R^2 < 0.7$: Mittelmässige Anpassung
     \item $R^2 < 0.5$: Schlechte Anpassung
   \end{itemize}

2. Residuenanalyse:
   \begin{itemize}
     \item Residuen sollten zufällig um 0 schwanken
     \item Keine systematischen Muster erkennbar
     \item Residuen sollten normalverteilt sein
   \end{itemize}

3. Prognosegüte:
   \begin{itemize}
     \item Mittlerer quadratischer Fehler (MSE)
     \item Wurzel des mittleren quadratischen Fehlers (RMSE)
     \item Mittlerer absoluter Fehler (MAE)
   \end{itemize}
\end{formula}

\begin{example2}{Modellwahl durch Residuenanalyse}\\
Für einen Datensatz wurden drei Modelle getestet:
\begin{itemize}
  \item Linear: $y = 2x + 1$
  \item Quadratisch: $y = x^2 + x + 1$
  \item Exponentiell: $y = 2e^{0.5x}$
\end{itemize}

Bestimmtheitsmasse:
\begin{itemize}
  \item Linear: $R^2 = 0.85$
  \item Quadratisch: $R^2 = 0.98$
  \item Exponentiell: $R^2 = 0.92$
\end{itemize}

Residuenanalyse zeigt:
\begin{itemize}
  \item Linear: Systematische Krümmung in Residuen
  \item Quadratisch: Zufällige Verteilung der Residuen
  \item Exponentiell: Leichte Systematik in Residuen
\end{itemize}

Schlussfolgerung: Das quadratische Modell ist am besten geeignet.
\end{example2}

\subsection{Nichtlineares Verhalten}

\begin{concept}{Linearisierung} \textbf{Wichtige Transformationen:}\\
Oft können nichtlineare Regressionsmodelle durch geeignete Transformation auf ein lineares Modell zurückgeführt werden.
\begin{center}
\begin{tabular}{|c|c|}
\hline
Ausgangsfunktion & Transformation \\
\hline
$y = q \cdot x^m$ & $\log(y) = \log(q) + m \cdot \log(x)$ \\
\hline
$y = q \cdot m^x$ & $\log(y) = \log(q) + \log(m) \cdot x$ \\
\hline
$y = q \cdot e^{m \cdot x}$ & $\ln(y) = \ln(q) + m \cdot x$ \\
\hline
$y = \frac{1}{q+m \cdot x}$ & $V = q + m \cdot x; V = \frac{1}{y}$ \\
\hline
$y = q + m \cdot \ln(x)$ & $y = q + m \cdot U; U = \ln(x)$ \\
\hline
$y = \frac{1}{q \cdot m^x}$ & $\log(\frac{1}{y}) = \log(q) + \log(m) \cdot x$ \\
\hline
\end{tabular}
\end{center}
$y$ = Abhängige Variable\\
$x$ = Unabhängige Variable\\
$q, m$ = Parameter der Funktion
\end{concept}

\begin{KR}{Nichtlineare Regression durch Linearisierung}
\begin{enumerate}
  \item Bestimme passende Transformation aus Tabelle
  \item Führe Transformation durch
  \item Wende lineare Regression auf transformierte Daten an
  \item Transformiere Parameter zurück
\end{enumerate}
\end{KR}

\begin{example2}{Exponentielles Wachstum} $y=q \cdot e^{mx}$
mit gegebenen Messwerten:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$x$ & 1 & 2 & 3 & 4 \\
\hline
$y$ & 2.1 & 4.2 & 8.1 & 15.9 \\
\hline
\end{tabular}
\end{center}

1. Transformation $\ln(y)=\ln(q)+mx$ $\rightarrow$ $Y=\ln(y)$, $b=\ln(q)$:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$x$ & 1 & 2 & 3 & 4 \\
\hline
$Y$ & 0.742 & 1.435 & 2.092 & 2.766 \\
\hline
\end{tabular}
\end{center}

2. Lineare Regression: $Y=mx+b$ $rightarrow$ $Y = 0.674x + 0.071$

3. Rücktransformation: $q=e^b$
   \begin{itemize}
     \item $m = 0.674$
     \item $q = e^{0.071} = 1.074$
   \end{itemize}

4. Ergebnis: $y = 1.074 \cdot e^{0.674x}$
\end{example2}

\subsection{Allgemeines Vorgehen bei der Regression}

\begin{definition}{Matrix-Darstellung}
$m$, $q$ der Regressionsgeraden mit $A$ berechnen:

$A = \begin{psmallmatrix} x_1 & 1 \\ .. & .. \\ x_n & 1 \end{psmallmatrix}, \quad A^T \cdot A \cdot \begin{psmallmatrix} m \\ q \end{psmallmatrix} = A^T \cdot \begin{psmallmatrix} y_1 \\ .. \\ y_n \end{psmallmatrix}$
\end{definition}

\begin{concept}{Matrix-Darstellung}\\
Für die Methode der kleinsten Quadrate mit mehreren Variablen wird ein lineares Gleichungssystem aufgestellt:
$y = Xp + \epsilon$

mit:
$p$: Vektor der Parameter,
$y$: Vektor der Messwerte,
$\epsilon$: Vektor der Residuen,
$X$: Matrix der Eingangswerte
\vspace{1mm}\\
Die Lösung ist:
$p = (X^TX)^{-1}X^Ty$
falls $(X^TX)$ invertierbar
\end{concept}



\begin{KR}{Matrix-Methode für lineare Regression}\\
1. Erstelle Design-Matrix $A$:
   $A = \begin{psmallmatrix} x_1 & 1 \\ .. & .. \\ x_n & 1 \end{psmallmatrix}$

2. Berechne $A^T \cdot A$

3. Berechne $(A^T \cdot A)^{-1}$

4. Berechne Parameter:
   $\begin{psmallmatrix} m \\ q \end{psmallmatrix} = (A^T \cdot A)^{-1} \cdot A^T \cdot \vec{y}$
\end{KR}



\begin{KR}{Vorgehen bei Mehrfachregression}\\
1. Aufstellen der Matrix $X$ mit den Eingangswerten

2. Berechnung der Parameter $p = (X^TX)^{-1}X^Ty$

3. Berechnung der Residuen $\epsilon = y - Xp$

4. Überprüfung der Modellgüte durch:
   \begin{itemize}
     \item Bestimmtheitsmass $R^2$
     \item Residuenanalyse
     \item Plausibilität der Parameter
   \end{itemize}
\end{KR}

\begin{concept}{Mehrfachregression}
  \vspace{-4mm}\\
1. Aufstellen der Designmatrix:
   $A = \begin{psmallmatrix} 
   x_{11} & x_{12} & \cdots & x_{1(k-1)} & 1 \\
   x_{21} & x_{22} & \cdots & x_{2(k-1)} & 1 \\
   \vdots & \vdots & \ddots & \vdots & \vdots \\
   x_{n1} & x_{n2} & \cdots & x_{n(k-1)} & 1
   \end{psmallmatrix}$

2. Berechnung der Parameter:
   $\vec{p} = (A^T A)^{-1} A^T \vec{y}$

3. Residuen berechnen:
   $\vec{\epsilon} = \vec{y} - A\vec{p}$

4. Bestimmtheitsmass:
   $R^2 = 1 - \frac{\sum \epsilon_i^2}{\sum(y_i - \bar{y})^2}$
\end{concept}

\begin{example2}{Mehrfachregression}
Ein Gebrauchtwagenhändler möchte den Preis (P) seiner Autos basierend auf Alter (A) und Kilometerstand (K) berechnen.
Gegeben sind folgende Daten:

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Auto & Alter (Jahre) & km (10000) & Preis (1000 CHF) \\
\hline
1 & 2 & 3 & 25 \\
2 & 3 & 4 & 20 \\
3 & 4 & 6 & 15 \\
4 & 5 & 7 & 12 \\
\hline
\end{tabular}
\end{center}

1. Designmatrix aufstellen:
   $A = \begin{psmallmatrix}
   2 & 3 & 1 \\
   3 & 4 & 1 \\
   4 & 6 & 1 \\
   5 & 7 & 1
   \end{psmallmatrix}$

2. Parameter berechnen:
   $\vec{p} = \begin{psmallmatrix} -3 \\ -1.5 \\ 35 \end{psmallmatrix}$

3. Resultierende Funktion:
   $P = -3A - 1.5K + 35$
\end{example2}

\begin{KR}{Polynomiale Regression}
Regression mit Polynomen höheren Grades:

1. Erweitere Designmatrix:
   $A = \begin{psmallmatrix}
   x_1^n & x_1^{n-1} & \cdots & x_1 & 1 \\
   x_2^n & x_2^{n-1} & \cdots & x_2 & 1 \\
   \vdots & \vdots & \ddots & \vdots & \vdots \\
   x_m^n & x_m^{n-1} & \cdots & x_m & 1
   \end{psmallmatrix}$

2. Löse wie bei linearer Regression:
   $\vec{p} = (A^T A)^{-1} A^T \vec{y}$

3. Polynom aufstellen:
   $y = p_1x^n + p_2x^{n-1} + ... + p_nx + p_{n+1}$
\end{KR}

\begin{example2}{Quadratische Regression}
Gegeben sind Messwerte:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$x$ & 0 & 1 & 2 & 3 & 4 \\
\hline
$y$ & 1 & 2.1 & 5.2 & 10.1 & 17.2 \\
\hline
\end{tabular}
\end{center}

1. Designmatrix für quadratisches Polynom:
  $A = \begin{psmallmatrix}
   0 & 0 & 1 \\
   1 & 1 & 1 \\
   4 & 2 & 1 \\
   9 & 3 & 1 \\
   16 & 4 & 1
   \end{psmallmatrix}$

2. Parameter berechnen:
   $\vec{p} = \begin{psmallmatrix} 1 \\ 0.1 \\ 1 \end{psmallmatrix}$

3. Quadratische Funktion:
   $y = x^2 + 0.1x + 1$
\end{example2}



\begin{example2}{Klausuraufgabe - Linearisierung}\\
Gegeben sind Messwerte für ein exponentielles Wachstum:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$t$ (h) & 0 & 1 & 2 & 3 \\
\hline
$N$ & 100 & 150 & 225 & 340 \\
\hline
\end{tabular}
\end{center}

Finden Sie eine Funktion der Form $N(t) = N_0 e^{kt}$

1. Transformation:
   $\ln(N) = \ln(N_0) + kt$

2. Neue Wertetabelle:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$t$ & 0 & 1 & 2 & 3 \\
\hline
$\ln(N)$ & 4.61 & 5.01 & 5.42 & 5.83 \\
\hline
\end{tabular}
\end{center}

3. Lineare Regression:
   $\ln(N) = 0.405t + 4.61$

4. Rücktransformation:
   $N(t) = 100.4 e^{0.405t}$

5. Bestimmtheitsmass: $R^2 = 0.999$
\end{example2}