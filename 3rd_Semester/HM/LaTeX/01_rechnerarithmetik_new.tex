\section{Rechnerarithmetik}

\subsection{Zahlendarstellung}

\begin{definition}{Maschinenzahlen}
Eine maschinendarstellbare Zahl zur Basis $B$ ist ein Element der Menge:
$$M = \{x \in \mathbb{R} \mid x = \pm 0.m_1m_2m_3\ldots m_n \cdot B^{\pm e_1e_2\ldots e_l}\} \cup \{0\}$$
\begin{itemize}
    \item $m_1 \neq 0$ (Normalisierungsbedingung) 
    \item $m_i, e_i \in \{0,1,\ldots,B-1\}$ für $i \neq 0$
    \item $B \in \mathbb{N}, B > 1$ (Basis)
\end{itemize}
\end{definition}

\begin{formula}{Zahlenwert}
Der Wert $\hat{\omega}$ einer Maschinenzahl berechnet sich durch: 
\vspace{-5mm}\\
$$\hat{\omega} = \sum_{i=1}^n m_i B^{\hat{e}-i}, \quad \text{mit} \quad \hat{e} = \sum_{i=1}^l e_i B^{l-i}$$
\end{formula}

\begin{KR}{Werteberechnung einer Maschinenzahl} 
\begin{enumerate}
    \item Normalisierung überprüfen:
    \begin{itemize}
        \item Erste Mantissenstelle $m_1 \neq 0$ (für $x \neq 0$)
        \item Wenn nicht normalisiert: Mantisse verschieben und Exponent anpassen
    \end{itemize}
    
    \item Exponent berechnen:
    \begin{itemize}
        \item $\hat{e} = \sum_{i=1}^l e_i B^{l-i}$ 
        \item Von links nach rechts: Stelle $\cdot$ Basis hochgestellt zur Position
        \item Summieren
    \end{itemize}
    
    \item Wert berechnen:
    \begin{itemize}
        \item $\hat{\omega} = \sum_{i=1}^n m_i B^{\hat{e}-i}$
        \item Mantissenstellen $\cdot$ Basis hochgestellt zu (Exponent - Position)
        \item Summieren
    \end{itemize}
    
    \item Vorzeichen berücksichtigen
\end{enumerate}
\end{KR}

\begin{example2}{Werteberechnung ausführlich} 
Gegeben sei die Maschinenzahl zur Basis $B=2$:
$$x = \underbrace{0.1101}_{\text{n=4}} \cdot \underbrace{2^{101}_2}_{\text{l=3}}$$

\textbf{1. Normalisierung prüfen:}
\begin{itemize}
    \item $m_1 = 1 \neq 0$ $\rightarrow$ normalisiert
\end{itemize}

\textbf{2. Exponent berechnen:}
\begin{align*}
\hat{e} &= 1 \cdot 2^2 + 0 \cdot 2^1 + 1 \cdot 2^0 \\
&= 4 + 0 + 1 = 5
\end{align*}

\textbf{3. Wert berechnen:}
\begin{align*}
\hat{\omega} &= 1 \cdot 2^{5-1} + 1 \cdot 2^{5-2} + 0 \cdot 2^{5-3} + 1 \cdot 2^{5-4} \\
&= 1 \cdot 2^4 + 1 \cdot 2^3 + 0 \cdot 2^2 + 1 \cdot 2^1 \\
&= 16 + 8 + 0 + 2 \\
&= 26
\end{align*}

Also ist $x = 26$
\end{example2}

\begin{example2}{Weitere Beispiele}
\begin{enumerate}
    \item Basis 10: $0.3141 \cdot 10^2$
    \begin{itemize}
        \item Normalisiert, da $m_1 = 3 \neq 0$
        \item $\hat{e} = 2$
        \item $\hat{\omega} = 3 \cdot 10^1 + 1 \cdot 10^0 + 4 \cdot 10^{-1} + 1 \cdot 10^{-2} = 31.41$
    \end{itemize}
    
    \item Basis 16 (hex): $0.A5F \cdot 16^3$
    \begin{itemize}
        \item Normalisiert, da $m_1 = A = 10 \neq 0$
        \item $\hat{e} = 3$
        \item $\hat{\omega} = 10 \cdot 16^2 + 5 \cdot 16^1 + 15 \cdot 16^0 = 2655$
    \end{itemize}
\end{enumerate}
\end{example2}

\begin{example2}{Werteberechnung} Berechnung einer vierstelligen Zahl zur Basis 4:
    %TODO: check if this is correct and/or relevant - either correct or replace with better example
    \vspace{-2mm}\\
    \begin{minipage}{0.3\textwidth}
        $$\underbrace{0.3211}_{n=4} \cdot \underbrace{4^{12}}_{l=2}$$
    \end{minipage}
    \begin{minipage}[t]{0.65\textwidth}
        Exponent: $\hat{e} = 1 \cdot 4^1 + 2 \cdot 4^0 = 6$ \vspace{1mm}\\
        Wert: $\hat{\omega} = 3 \cdot 4^3 + 2 \cdot 4^2 + 1 \cdot 4^1 + 1 \cdot 4^0 = 57$
    \end{minipage}
\end{example2}

\begin{example2}{Werteberechnung} Berechnung einer Zahl zur Basis B=2:
\begin{minipage}{0.45\textwidth}
    $$\underbrace{0.1011}_{\text{n=4}} \cdot \underbrace{2^{3}}_{\text{l=1}}$$
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
    1. Exponent: $\hat{e} = 3$ \\ 
    2. Wert: $\hat{\omega} = 1\cdot2^2 + 0\cdot2^1 + 1\cdot2^0 + 1\cdot2^{-1}$ \\
    $= 4 + 0 + 1 + 0.5 = 5.5$
\end{minipage}
\end{example2}

\begin{concept}{IEEE-754 Standard} definiert zwei wichtige Gleitpunktformate:
\vspace{1mm}\\
\begin{minipage}[t]{0.5\textwidth}
    \textbf{Single Precision} (32 Bit)\\
    Vorzeichen(V): 1 Bit\\
    Exponent(E): 8 Bit (Bias 127)\\
    Mantisse(M): \\ 23 Bit + 1 hidden bit
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
    \textbf{Double Precision} (64 Bit)\\
    Vorzeichen(V): 1 Bit\\
    Exponent(E): 11 Bit (Bias 1023)\\
    Mantisse(M): \\ 52 Bit + 1 hidden bit
\end{minipage}
\end{concept}

\begin{theorem}{Darstellungsbereich}
Für jedes Gleitpunktsystem existieren:
\begin{itemize}
    \item Grösste darstellbare Zahl: \large{$x_{\text{max}} = (1-B^{-n}) \cdot B^{e_{\text{max}}}$}
    \item \normalsize{Kleinste darstellbare positive Zahl:} \large{$x_{\text{min}} = B^{e_{\text{min}}-1}$}
\end{itemize}
\end{theorem}

\subsection{Approximations- und Rundungsfehler}

\begin{definition}{Fehlerarten}
Sei $\tilde{x}$ eine Näherung des exakten Wertes $x$:
\vspace{1mm}\\
\begin{minipage}[t]{0.35\textwidth}
    \textbf{Absoluter Fehler:}  $$\left|\tilde{x}-x\right|$$
\end{minipage}
\hspace{3mm}
\begin{minipage}[t]{0.5\textwidth}
    \textbf{Relativer Fehler:}  $$\left|\frac{\tilde{x}-x}{x}\right| \text{ bzw. } \frac{|\tilde{x}-x|}{|x|} \text{ für } x \neq 0$$
\end{minipage}
\end{definition}

\begin{lemma}{Maschinengenauigkeit} 
    eps ist die kleinste positive Zahl, für die gilt:
    \vspace{-3mm}\\
\begin{minipage}[t]{0.35\textwidth}
    \textbf{Allgemein:}  $$\text{eps} := \frac{B}{2} \cdot B^{-n}$$
\end{minipage}
\hspace{6mm}
\begin{minipage}[t]{0.35\textwidth}
    \textbf{Dezimal:}  $$\text{eps}_{10} := 5 \cdot 10^{-n}$$
\end{minipage}

\begin{minipage}[t]{0.45\textwidth}
    Sie begrenzt den maximalen relativen Rundungsfehler:
\end{minipage}
\begin{minipage}{0.5\textwidth}
    $$\left|\frac{rd(x)-x}{x}\right| \leq \text{eps}$$
\end{minipage}
\end{lemma}

\begin{corollary}{Rundungseigenschaften}
Für alle $x \in \mathbb{R}$ mit $|x| \geq x_{\text{min}}$ gilt:
\vspace{1mm}\\
\begin{minipage}[t]{0.45\textwidth}
    \textbf{Absoluter Fehler:}  $$|rd(x) - x| \leq \frac{B}{2} \cdot B^{e-n-1}$$
\end{minipage}
\hspace{3mm}
\begin{minipage}[t]{0.35\textwidth}
    \textbf{Relativer Fehler:}  $$\left|\frac{rd(x)-x}{x}\right| \leq \text{eps}$$
\end{minipage}
\end{corollary}

\subsubsection{Fehlerfortpflanzung}

\begin{concept}{Konditionierung}
    Die Konditionszahl $K$ beschreibt die relative Fehlervergrösserung bei Funktionsauswertungen:
    \vspace{1mm}\\
\begin{minipage}{0.3\textwidth}
    \vspace{-2mm}
    $$K := \frac{|f'(x)| \cdot |x|}{|f(x)|}$$
\end{minipage}
\hspace{2mm}
\begin{minipage}{0.6\textwidth}
\begin{itemize}
    \item $K \leq 1$: gut konditioniert
    \item $K > 1$: schlecht konditioniert
    \item $K \gg 1$: sehr schlecht konditioniert
\end{itemize}
\end{minipage}
\end{concept}

\begin{theorem}{Fehlerfortpflanzung}
Für $f$ (differenzierbar) gilt näherungsweise:
\vspace{1mm}\\
\begin{minipage}[t]{0.47\textwidth}
    \textbf{Absoluter Fehler:}  
    \vspace{-2mm}\\
    $$|f(\tilde{x})-f(x)| \approx |f'(x)| \cdot |\tilde{x}-x|$$
\end{minipage}
\hspace{3mm}
\begin{minipage}[t]{0.43\textwidth}
    \textbf{Relativer Fehler:}  
    \vspace{-2mm}\\
    $$\frac{|f(\tilde{x})-f(x)|}{|f(x)|} \approx K \cdot \frac{|\tilde{x}-x|}{|x|}$$
\end{minipage}
\end{theorem}

\begin{KR}{Analyse der Fehlerfortpflanzung einer Funktion}
\begin{enumerate}
    \item Berechnen Sie $f'(x)$
    \item Bestimmen Sie die Konditionszahl $K$
    \item Schätzen Sie den absoluten Fehler ab
    \item Schätzen Sie den relativen Fehler ab
    \item Beurteilen Sie die Konditionierung anhand von $K$
\end{enumerate}
\vspace{1mm}
$$
\begin{aligned}
\underbrace{|f(\tilde{x})-f(x)|}_{\text {absoluter Fehler von } f(x)} & \approx\left|f^{\prime}(x)\right| \cdot \underbrace{|\tilde{x}-x|}_{\text {absoluter Fehler von } x} \\
\underbrace{\frac{|f(\tilde{x})-f(x)|}{|f(x)|}}_{\text {relativer Fehler von } f(x)} & \approx \underbrace{\frac{\left|f^{\prime}(x)\right| \cdot|x|}{|f(x)|}}_{\text {Konditionszahl } K} . \quad \underbrace{\frac{|\tilde{x}-x|}{|x|}}_{\text { relativer Fehler von } x }
\end{aligned}
$$
\end{KR}

\raggedcolumns

\begin{example2}{Fehleranalyse}Beispiel: Fehleranalyse von $f(x)=\sin(x)$
    %TODO: check if this is correct and/or relevant - either correct or replace with better example
\begin{enumerate}
    \item $f'(x) = \cos(x)$
    \item $K = \frac{|x\cos(x)|}{|\sin(x)|}$
    \item Für $x \to 0$: $K \to 1$ (gut konditioniert)
    \item Für $x \to \pi$: $K \to \infty$ (schlecht konditioniert)
    \item Der absolute Fehler wird nicht vergrössert, da $|\cos(x)| \leq 1$
\end{enumerate}
\end{example2}

\begin{example2}{Fehleranalyse} Analyse von $f(x)=\sin(x)$:
\begin{enumerate}
    \item $f'(x) = \cos(x)$
    \item $K = \frac{|x\cos(x)|}{|\sin(x)|}$
    \item Für $x = \pi/4$: $K \approx 1$ (gut konditioniert)
    \item Für $x = \pi$: $K \to \infty$ (schlecht konditioniert)
    \item Für $x = 0$: $\lim_{x \to 0} K = 1$ (gut konditioniert)
    \item Absoluter Fehler wird durch $|\cos(x)| \leq 1$ begrenzt
\end{enumerate}
\end{example2}

\subsubsection{Praktische Fehlerquellen der Numerik}

\begin{concept}{Kritische Operationen} häufigste Fehlerquellen:
\begin{itemize}
    \item Auslöschung bei Subtraktion ähnlich großer Zahlen
    \item Überlauf (overflow) bei zu großen Zahlen
    \item Unterlauf (underflow) bei zu kleinen Zahlen
    \item Verlust signifikanter Stellen durch Rundung
\end{itemize}
\end{concept}



\begin{KR}{Vermeidung von Auslöschung}
\begin{enumerate}
    \item Identifizieren Sie Subtraktionen ähnlich großer Zahlen
    \item Suchen Sie nach algebraischen Umformungen
    \item Prüfen Sie alternative Berechnungswege
    \item Verwenden Sie Taylorentwicklungen für kleine Werte
\end{enumerate}
\end{KR}

\begin{example2}{Auslöschung} bei der Berechnung von $\sqrt{x^2 + 1} - 1$:
    %TODO: check if this is correct and/or relevant - either correct or replace with better example
    \vspace{1mm}\\
    Für kleine $x$ führt die direkte Berechnung zu Auslöschung:
    \vspace{1mm}\\
    Für $x = 10^{-8}$: $\sqrt{10^{-16} + 1} - 1 \approx 1.000000000 - 1 = 0$
    \vspace{1mm}\\
    Korrekte Lösung durch Umformung: $\sqrt{x^2 + 1} - 1 = \frac{x^2}{\sqrt{x^2 + 1} + 1}$
\end{example2}

\begin{example2}{Auslöschung} Kritische Berechnungen:
\begin{enumerate}
    \item $\sqrt{1 + x^2} - 1$ für kleine $x$:
    \begin{itemize}
        \item Direkt: Auslöschung für $x = 10^{-8}$
        \item Besser: $\frac{x^2}{\sqrt{1 + x^2} + 1}$
    \end{itemize}
    \item $1 - \cos(x)$ für kleine $x$:
    \begin{itemize}
        \item Direkt: Auslöschung
        \item Besser: $2\sin^2(x/2)$
    \end{itemize}
\end{enumerate}
\end{example2}

\begin{remark2}{Auslöschung}
    Bei der Subtraktion fast gleich großer Zahlen können signifikante Stellen verloren gehen. Beispiel:
    \begin{itemize}
        \item $1.234567 - 1.234566 = 0.000001$
        \item Aus 7 signifikanten Stellen wird 1 signifikante Stelle
    \end{itemize}
\end{remark2}

\subsubsection{Analyse von Algorithmen}

\begin{formula}{Fehlerakkumulation}
Bei $n$ aufeinanderfolgenden Operationen mit relativen Fehlern $\leq \varepsilon$ gilt für den Gesamtfehler:
\begin{itemize}
    \item Best case: $\mathcal{O}(n\varepsilon)$ bei gleichverteilten Fehlern
    \item Worst case: $\mathcal{O}(2^n\varepsilon)$ bei systematischen Fehlern
\end{itemize}
\end{formula}

\begin{concept}{Numerische Stabilität eines Algorithmus}
\begin{itemize}
    \item Kleine Eingabefehler führen zu kleinen Ausgabefehlern
    \item Rundungsfehler akkumulieren sich nicht übermäßig 
    \item Konditionszahl des Problems wird nicht künstlich verschlechtert
\end{itemize}
\end{concept}

\begin{examplecode}{Instabilität} bei rekursiver Berechnung: (Fibonacci-Zahlen)
    %TODO: check if this is correct and/or relevant - either correct or replace with better example
\begin{lstlisting}[language=Python, style=basesmol]
def fib(n):
    if n <= 1:
        return n
    return fib(n-1) + fib(n-2)
\end{lstlisting}
Exponentielles Wachstum der Operationen $\rightarrow$ Fehlerfortpflanzung
\end{examplecode}

\begin{examplecode}{Numerische Stabilität} Fibonacci-Zahlen:
\begin{lstlisting}[language=Python, style=basesmol]
# Instabile rekursive Version
def fib_unstable(n):
    if n <= 1:
        return n
    return fib_unstable(n-1) + fib_unstable(n-2)

# Stabile iterative Version
def fib_stable(n):
    if n <= 1:
        return n
    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b
    return b
\end{lstlisting}
\end{examplecode}

\begin{KR}{Stabilitätsanalyse}
Schritte zur Analyse der numerischen Stabilität:
\begin{enumerate}
    \item Bestimmen Sie kritische Operationen
    \item Schätzen Sie Rundungsfehler pro Operation ab
    \item Analysieren Sie die Fehlerfortpflanzung
    \item Berechnen Sie die worst-case Fehlerschranke
    \item Vergleichen Sie alternative Implementierungen
\end{enumerate}
\end{KR}

\subsubsection{Praktische Implementierungen}

\begin{definition}{Implementierungsgenauigkeit eines Algorithmus}
\begin{itemize}
    \item Relative Genauigkeit der Ausgabe
    \item Maximale Anzahl korrekter Dezimalstellen
    \item Stabilität gegenüber Eingabefehlern
\end{itemize}
\end{definition}

\begin{KR}{Robuste Implementierung von Algorithmen}
\begin{enumerate}
    \item Verwenden Sie stabile Grundoperationen
    \item Vermeiden Sie Differenzen ähnlich großer Zahlen
    \item Prüfen Sie auf Über- und Unterlauf
    \item Implementieren Sie Fehlerkontrollen
    \item Dokumentieren Sie numerische Einschränkungen
\end{enumerate}
\end{KR}

\begin{examplecode}{Robuste Implementation} Beispiel: Quadratische Gleichung
    %TODO: check if this is correct and/or relevant - either correct or replace with better example
\begin{lstlisting}[language=Python, style=basesmol]
def quadratic_stable(a, b, c):
    # ax^2 + bx + c = 0
    if a == 0:
        return [-c/b] if b != 0 else []
        
    # Calculate discriminant
    disc = b*b - 4*a*c
    if disc < 0:
        return []

    # Choose numerically stable formula
    if b >= 0:
        q = -0.5*(b + sqrt(disc))
    else:
        q = -0.5*(b - sqrt(disc))
    x1 = q/a
    x2 = c/(q)

    return sorted([x1, x2])
\end{lstlisting}
\end{examplecode}

\begin{examplecode}{Robuste Implementation} Quadratische Gleichung:
\begin{lstlisting}[language=Python, style=basesmol]
# Einfache Version (numerisch instabil)
def solve_quadratic_simple(a, b, c):
    if a == 0:
        return [-c/b] if b != 0 else []
    d = b**2 - 4*a*c
    if d < 0:
        return []
    x1 = (-b + d**0.5)/(2*a)
    x2 = (-b - d**0.5)/(2*a)
    return [x1, x2]

# Numerisch stabile Version
def solve_quadratic_stable(a, b, c):
    if a == 0:
        return [-c/b] if b != 0 else []
    d = b**2 - 4*a*c
    if d < 0:
        return []
    if b >= 0:
        q = -0.5*(b + d**0.5)
    else:
        q = -0.5*(b - d**0.5)
    x1 = q/a
    x2 = c/q
    return sorted([x1, x2])
\end{lstlisting}
\end{examplecode}
