\section{Näherungsverfahren}

\subsubsection{Approximations- und Rundungsfehler}

\begin{definition}{Fehlerarten}
Sei $\tilde{x}$ eine Näherung des exakten Wertes $x$:
\vspace{1mm}\\
\begin{minipage}[t]{0.45\textwidth}
    \textbf{Absoluter Fehler:} 
    \begin{center} $\left|\tilde{x}-x\right|$ \end{center}
\end{minipage}
\hspace{3mm}
\begin{minipage}[t]{0.5\textwidth}
    \textbf{Relativer Fehler:} 
    \begin{center} $\left|\frac{\tilde{x}-x}{x}\right| \text{ bzw. } \frac{|\tilde{x}-x|}{|x|} \text{ für } x \neq 0$ \end{center}
\end{minipage}
\end{definition}

\begin{concept}{Konditionierung}
    Die Konditionszahl $K$ beschreibt die relative Fehlervergrösserung bei Funktionsauswertungen:
    \vspace{1mm}\\
\begin{minipage}{0.3\textwidth}
    \vspace{-2mm}
    $$K := \frac{|f'(x)| \cdot |x|}{|f(x)|}$$
\end{minipage}
\hspace{2mm}
\begin{minipage}{0.6\textwidth}
\begin{itemize}
    \item $K \leq 1$: gut konditioniert
    \item $K > 1$: schlecht konditioniert
    \item $K \gg 1$: sehr schlecht konditioniert
\end{itemize}
\end{minipage}
\end{concept}


\begin{theorem}{Fehlerfortpflanzung}
Für $f$ (differenzierbar) gilt näherungsweise:
\vspace{1mm}\\
\begin{minipage}[t]{0.47\textwidth}
    \textbf{Absoluter Fehler:}  
    \vspace{-2mm}\\
    $$|f(\tilde{x})-f(x)| \approx |f'(x)| \cdot |\tilde{x}-x|$$
\end{minipage}
\hspace{3mm}
\begin{minipage}[t]{0.43\textwidth}
    \textbf{Relativer Fehler:}  
    \vspace{-2mm}\\
    $$\frac{|f(\tilde{x})-f(x)|}{|f(x)|} \approx K \cdot \frac{|\tilde{x}-x|}{|x|}$$
\end{minipage}
\end{theorem}

\begin{KR}{Analyse der Fehlerfortpflanzung einer Funktion}
\begin{enumerate}
    \item Berechnen Sie $f'(x)$ und die Konditionszahl $K$
    \item Schätzen Sie den absoluten und den relativen Fehler ab
    \item Beurteilen Sie die Konditionierung anhand von $K$
\end{enumerate}
\end{KR}

\begin{example2}{Konditionierung berechnen}
Für $f(x) = \sqrt{1+x^2}$ und $x_0 = 10^{-8}$:
\begin{enumerate}
    \item $f'(x) = \frac{x}{\sqrt{1+x^2}}$, $K = \frac{|x \cdot x|}{|\sqrt{1+x^2} \cdot (1+x^2)|} = \frac{x^2}{(1+x^2)^{3/2}}$
    \item Für $x_0 = 10^{-8}$:
    \begin{itemize}
        \item $K(10^{-8}) \approx 10^{-16}$ (gut konditioniert)
        \item Relativer Fehler wird um Faktor $10^{-16}$ verkleinert
    \end{itemize}
\end{enumerate}
\end{example2}

\raggedcolumns

\begin{example2}{Fehleranalyse}Beispiel: Fehleranalyse von $f(x)=\sin(x)$
\begin{enumerate}
    \item $f'(x) = \cos(x)$, $K = \frac{|x\cos(x)|}{|\sin(x)|}$
    \item Konditionierung:
    \begin{itemize}
    \item Für $x \to 0$: $K \to 1$ (gut konditioniert)
    \item Für $x \to \pi$: $K \to \infty$ (schlecht konditioniert)
    \item Für $x = 0$: $\lim_{x \to 0} K = 1$ (gut konditioniert)
    \end{itemize}
    \item Der absolute Fehler wird nicht vergrössert, da $|\cos(x)| \leq 1$
\end{enumerate}
\end{example2}

\subsubsection{Praktische Fehlerquellen der Numerik}

\begin{concept}{Kritische Operationen} häufigste Fehlerquellen:
\begin{itemize}
    \item Auslöschung bei Subtraktion ähnlich großer Zahlen
    \item Überlauf (overflow) bei zu großen Zahlen
    \item Unterlauf (underflow) bei zu kleinen Zahlen
    \item Verlust signifikanter Stellen durch Rundung
\end{itemize}
\end{concept}

\begin{KR}{Vermeidung von Auslöschung}
\begin{enumerate}
    \item Identifizieren Sie Subtraktionen ähnlich großer Zahlen
    \item Suchen Sie nach algebraischen Umformungen
    \item Prüfen Sie alternative Berechnungswege
    \item Verwenden Sie Taylorentwicklungen für kleine Werte
\end{enumerate}

Beispiele für bessere Formeln:
    \begin{itemize}
        \item $\sqrt{1+x^2}-1 \rightarrow \frac{x^2}{\sqrt{1+x^2}+1}$
        \item $1-\cos(x) \rightarrow 2\sin^2(x/2)$
        \item $\ln(1+x) \rightarrow x-\frac{x^2}{2}$ für kleine $x$
    \end{itemize}
\end{KR}


\section{Numerische Lösung von Nullstellenproblemen}

\begin{lemma}{Nullstellensatz von Bolzano}
Sei $f:[a,b] \rightarrow \mathbb{R}$ stetig. Falls 
\vspace{-1mm}\\
$$f(a) \cdot f(b) < 0$$ 
\vspace{-3mm}\\
dann existiert mindestens eine Nullstelle $\xi \in (a,b)$.
\end{lemma}



\begin{KR}{Nullstellenproblem systematisch lösen}
\begin{enumerate}
    \item Existenz prüfen:
    \begin{itemize}
        \item Intervall $[a,b]$ identifizieren
        \item Vorzeichenwechsel prüfen: $f(a) \cdot f(b) < 0$ 
        \item Stetigkeit von $f$ sicherstellen
    \end{itemize}
    
    \item Verfahren auswählen:
    \begin{itemize}
        \item Fixpunktiteration: einfache Umformung $x = F(x)$ möglich
        \item Newton: $f'(x)$ leicht berechenbar
        \item Sekantenverfahren: $f'(x)$ schwer berechenbar
    \end{itemize}
    
    \item Konvergenz sicherstellen:
    \begin{itemize}
        \item Fixpunktiteration: $|F'(x)| < 1$ im relevanten Bereich
        \item Newton: $|\frac{f(x)f''(x)}{[f'(x)]^2}| < 1$ im relevanten Bereich
        \item Sekanten: Konvergenzgeschwindigkeit beachten
    \end{itemize}

    \item Geeigneten Startwert wählen:
    \begin{itemize}
        \item Fixpunkt: $x_0$ im Intervall und nahe Fixpunkt ($|f'(x)| < 1$)
        \item Newton: $f'(x_0) \neq 0$
        \begin{itemize}
            \item Bei mehrfachen Nullstellen: Start zwischen Wendepunkt und Nullstelle
            \item Für Polynome: Startwerte zwischen -1 und 1 oft geeignet
        \end{itemize}
        \item Sekanten: Zwei Startwerte $x_0$ und $x_1$: $f(x_0) \neq f(x_1)$
        \begin{itemize}
            \item Idealerweise auf verschiedenen Seiten der Nullstelle
        \end{itemize}
    \end{itemize}
    
    \item Abbruchkriterien festlegen:
    \begin{itemize}
        \item Funktionswert: $|f(x_n)| < \epsilon_1$
        \item Iterationsschritte: $|x_{n+1}-x_n| < \epsilon_2$
        \item Maximale Iterationszahl
    \end{itemize}
\end{enumerate}
\end{KR}



\begin{example2}{Verfahrensauswahl}\\
Finden Sie die positive Nullstelle von $f(x) = x^3 - 2x - 5$

\paragraph{Vorgehen:}
\begin{enumerate}
    \item Existenz:
    \begin{itemize}
        \item $f(2) = -1 < 0$ und $f(3) = 16 > 0$
        \item $\Rightarrow$ Nullstelle in $[2,3]$
    \end{itemize}
    
    \item Verfahrenswahl:
    \begin{itemize}
        \item $f'(x) = 3x^2 - 2$ leicht berechenbar
        \item $\Rightarrow$ Newton-Verfahren geeignet
    \end{itemize}
    
    \item Konvergenzcheck:
    \begin{itemize}
        \item $f'(x) > 0$ für $x > 0.82$
        \item $f''(x) = 6x$ monoton
        \item $\Rightarrow$ Newton-Verfahren konvergiert
    \end{itemize}
\end{enumerate}
\end{example2}



\raggedcolumns
\columnbreak


\subsubsection{Fixpunktiteration}

\begin{definition}{Fixpunktgleichung}
ist eine Gleichung der Form: $F(x)=x$\\
Die Lösungen $\bar{x}$, für die $F(\bar{x})=\bar{x}$ erfüllt ist, heissen Fixpunkte.
\end{definition}

\begin{concept}{Grundprinzip der Fixpunktiteration}
sei $F:[a,b] \rightarrow \mathbb{R}$ mit $x_0 \in [a,b]$ 
$$\text{Die rekursive Folge }x_{n+1} \equiv F(x_n), \quad n=0,1,2,\ldots$$
\vspace{-4mm}\\
heisst Fixpunktiteration von $F$ zum Startwert $x_0$.
\end{concept}



\begin{theorem}{Konvergenzverhalten}\\
Sei $F:[a,b] \rightarrow \mathbb{R}$ mit stetiger Ableitung $F'$ und $\bar{x} \in [a,b]$ ein Fixpunkt von $F$. Dann gilt für die Fixpunktiteration $x_{n+1}=F(x_n)$:
\vspace{1mm}\\
\begin{minipage}[t]{0.45\textwidth}
    \textbf{Anziehender Fixpunkt:}
    \vspace{-3mm}\\
    $$|F'(\bar{x})| < 1$$
    \vspace{-4mm}\\
    $x_n$ konvergiert gegen $\bar{x}$,\\
    falls $x_0$ nahe genug bei $\bar{x}$
\end{minipage}
\hspace{3mm}
\begin{minipage}[t]{0.45\textwidth}
    \textbf{Abstossender Fixpunkt:}
    \vspace{-3mm}\\
    $$|F'(\bar{x})| > 1$$
    \vspace{-4mm}\\
    $x_n$ konvergiert für keinen\\
    Startwert $x_0 \neq \bar{x}$
\end{minipage}
\end{theorem}

\begin{lemma}{Banachscher Fixpunktsatz}
$F:[a,b] \rightarrow [a,b]$ und $\exists$ Konstante $\alpha$:
\begin{itemize}
    \item $0 < \alpha < 1$ (Lipschitz-Konstante)
    \item $|F(x)-F(y)| \leq \alpha|x-y|$ für alle $x,y \in [a,b]$
\end{itemize}
\vspace{2mm}


\begin{minipage}[t]{0.35\textwidth}
    Dann gilt:
\begin{itemize}
    \item $F$ hat genau einen Fixpunkt $\bar{x}$ in $[a,b]$
    \item Die Fixpunktiteration konvergiert gegen $\bar{x}$ für alle $x_0 \in [a,b]$
\end{itemize}
\end{minipage}
\hspace{2mm}
\begin{minipage}[t]{0.55\textwidth}
    \textbf{Fehlerabschätzungen}:
    \vspace{-2mm}\\
    $$\textbf{a-priori: } |x_n-\bar{x}| \leq \frac{\alpha^n}{1-\alpha} \cdot |x_1-x_0|$$
    $$\textbf{a-posteriori: } |x_n-\bar{x}| \leq \frac{\alpha}{1-\alpha} \cdot |x_n-x_{n-1}|$$
\end{minipage}
\end{lemma}

\begin{KR}{Konvergenznachweis für Fixpunktiteration}
\begin{enumerate}
    \item Bringe die Gleichung in Fixpunktform: $f(x)=0 \Rightarrow x = F(x)$
    \begin{itemize}
        \item Form mit kleinstem $|F'(x)|$ wählen
    \end{itemize}
    \item Prüfe, ob $F$ das Intervall $[a,b]$ in sich abbildet:
    \begin{itemize}
        \item Wähle geeignetes Intervall ($[a,b]$ $F(a) \geq a$ und $F(b) \leq b$)
    \end{itemize}
    \item Bestimme die Lipschitz-Konstante $\alpha$: $\rightarrow$ Berechne $F'(x)$
    \begin{itemize}
        \item Finde $\alpha = \max_{x \in [a,b]} |F'(x)|$ und prüfe $\alpha < 1$
    \end{itemize}
    \item Fehlerabschätzung:
    \begin{itemize}
        \item A-priori: $|x_n-\bar{x}| \leq \frac{\alpha^n}{1-\alpha}|x_1-x_0|$
        \item A-posteriori: $|x_n-\bar{x}| \leq \frac{\alpha}{1-\alpha}|x_n-x_{n-1}|$
    \end{itemize}
\end{enumerate}

\begin{minipage}[t]{0.5\textwidth}
    4. Berechnen Sie die nötigen \\Iterationen für Genauigkeit tol:
\end{minipage}
\begin{minipage}[t]{0.4\textwidth}
    \vspace{-3mm}
$$n \geq \frac{\ln(\frac{tol \cdot (1-\alpha)}{|x_1-x_0|})}{\ln \alpha}$$
\end{minipage}
\end{KR}

\begin{example2}{Fixpunktiteration} Nullstellen von $f(x)=e^x - x - 2$

Umformung in Fixpunktform: $x = \ln(x+2)$, also $F(x)=\ln(x+2)$
\begin{enumerate}
    \item $F'(x) = \frac{1}{x+2}$ monoton fallend
    \item Für $I=[1,2]$: $F(1)=1.099 > 1$, $F(2)=1.386 < 2$
    \item $\alpha = \max_{x \in [1,2]} |\frac{1}{x+2}| = \frac{1}{3} < 1$
    \item Konvergenz für Startwerte in $[1,2]$ gesichert
    \item Für Genauigkeit $10^{-6}$ benötigt: $n \geq 12$ Iterationen
\end{enumerate}
\end{example2}


\begin{example2}{Fixpunktiteration}
Bestimmen Sie $\sqrt{3}$ mittels Fixpunktiteration.
\begin{enumerate}
    \item Umformung: $x^2 = 3 \Rightarrow x = \frac{x^2+3}{2x} =: F(x)$
    
    \item Konvergenznachweis für $[1,2]$: $F'(x) = \frac{x^2-3}{2x^2}$

    \item $F([1,2]) \subseteq [1,2]$ und $|F'(x)| \leq \alpha = 0.25 < 1$ für $x \in [1,2]$
    
    \item Für Genauigkeit $10^{-6}$:
    \begin{itemize}
        \item $|x_1-x_0| = |1.5-2| = 0.5$
        \item $n \geq \frac{\ln(10^{-6} \cdot 0.75/0.5)}{\ln 0.25} \approx 12$
    \end{itemize}
\end{enumerate}
\end{example2}



\subsubsection{Newton-Verfahren}

\begin{concept}{Grundprinzip Newton-Verfahren}
    \vspace{-2mm}\\
\begin{minipage}[t]{0.6\textwidth}
    Approximation der NS durch \\ sukzessive Tangentenberechnung:
\end{minipage}
\begin{minipage}{0.3\textwidth}
    \vspace{-3mm}
    $$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$$
\end{minipage}
\vspace{-2mm}\\
\begin{minipage}[t]{0.6\textwidth}
    Konvergiert, wenn für alle $x$ im \\ relevanten Intervall gilt:
\end{minipage}
\begin{minipage}{0.3\textwidth}
    \vspace{-3mm}
    $$\left|\frac{f(x) \cdot f''(x)}{[f'(x)]^2}\right| < 1$$
\end{minipage}
\end{concept}

\begin{KR}{Newton-Verfahren anwenden}
\begin{enumerate}
    \item Funktion $f(x)$ und Ableitung $f'(x)$ aufstellen
    \item Geeigneten Startwert $x_0$ nahe der Nullstelle wählen
    \begin{itemize}
        \item Prüfen, ob $f'(x_0) \neq 0$
    \end{itemize}
    \item Iterieren bis zur gewünschten Genauigkeit:
    $x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$
    \item Abbruchkriterien prüfen:
    \begin{itemize}
        \item Funktionswert: $|f(x_n)| < \epsilon_1$
        \item Änderung aufeinanderfolgenden Werte: $|x_{n+1}-x_n| < \epsilon_2$
        \item Maximale Iterationszahl nicht überschritten
    \end{itemize}
    \item Fehlerabschätzung:
    \begin{itemize}
        \item $|x_n-\bar{x}| < \epsilon$ falls
        \item $f(x_n-\epsilon) \cdot f(x_n+\epsilon) < 0$
    \end{itemize}
\end{enumerate}
\end{KR}

\begin{example2}{Newton-Verfahren} Nullstellen von $f(x)=x^2-2$\\
Ableitung: $f'(x) = 2x$, Startwert $x_0 = 1$
\vspace{1mm}\\
\begin{minipage}[t]{0.65\textwidth}
    \vspace{-3mm}
    \begin{enumerate}
        \item $x_1 = 1 - \frac{1^2-2}{2 \cdot 1} = 1.5$
        \item $x_2 = 1.5 - \frac{1.5^2-2}{2 \cdot 1.5} = 1.4167$
        \item $x_3 = 1.4167 - \frac{1.4167^2-2}{2 \cdot 1.4167} = 1.4142$
    \end{enumerate}
\end{minipage}
\begin{minipage}[t]{0.3\textwidth}
    $\rightarrow$ Konvergenz \\ gegen $\sqrt{2}$ nach \\ wenigen Schritten
\end{minipage}
\end{example2}





\begin{theorem}{Vereinfachtes Newton-Verfahren}\\
    \begin{minipage}{0.5\textwidth}
        Alternative Variante mit \\ konstanter Ableitung:
    \end{minipage}
    \begin{minipage}{0.25\textwidth}
        \vspace{-5mm}
        $$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_0)}$$
    \end{minipage}
    \vspace{1mm}\\
    Konvergiert langsamer, aber benötigt weniger Rechenaufwand.
\end{theorem}

\begin{concept}{Sekantenverfahren}\\
    Alternative zum Newton-Verfahren ohne Ableitungsberechnung. Verwendet zwei Punkte $(x_{n-1}, f(x_{n-1}))$ und $(x_n, f(x_n))$:
    \vspace{-2mm}\\
    $$x_{n+1} = x_n - \frac{x_n-x_{n-1}}{f(x_n)-f(x_{n-1})} \cdot f(x_n)$$
    \vspace{-3mm}\\
    Benötigt zwei Startwerte $x_0$ und $x_1$.
\end{concept}


\begin{example2}{Sekantenverfahren} Nullstellen von $f(x)=x^2-2$\\
    %TODO: check if this is correct and/or relevant - either correct or replace with better example
Startwerte $x_0 = 1$ und $x_1 = 2$
\vspace{1mm}\\
\begin{minipage}[t]{0.65\textwidth}
    \vspace{-3mm}
    \begin{enumerate}
        \item $x_2 = 1 - \frac{1-2}{1^2-2} \cdot 1 = 1.5$
        \item $x_3 = 1.5 - \frac{1.5-1}{1.5^2-2} \cdot 1.5 = 1.4545$
        \item $x_4 = 1.4545 - \frac{1.4545-1.5}{1.4545^2-2} \cdot 1.4545 = 1.4143$
    \end{enumerate}
\end{minipage}
\hspace{2mm}
\begin{minipage}[t]{0.28\textwidth}
    $\rightarrow$ Konvergenz\\ gegen $\sqrt{2}$ nach \\wenigen Schritten
\end{minipage}
\end{example2}

\begin{example2}{Newton für Nichtlineare Systeme}
Bestimmen Sie die Nullstelle des Systems:
$f_1(x,y) = x^2 + y^2 - 1 = 0$
$f_2(x,y) = y - x^2 = 0$

\paragraph{Lösung:}
\begin{enumerate}
    \item Jacobi-Matrix aufstellen:
    $J = \begin{psmallmatrix} 
    2x & 2y \\
    -2x & 1
    \end{psmallmatrix}$
    
    \item Newton-Iteration:\\
    $\begin{pmatrix} x_{k+1} \\ y_{k+1} \end{pmatrix} = 
    \begin{pmatrix} x_k \\ y_k \end{pmatrix} - 
    J^{-1}(x_k,y_k) \begin{pmatrix} f_1(x_k,y_k) \\ f_2(x_k,y_k) \end{pmatrix}$
    
    \item Mit Startwert $(0.5, 0.25)$ erste Iteration durchführen
\end{enumerate}
\end{example2}






\subsubsection{Fehlerabschätzung}

\begin{KR}{Fehlerabschätzung für Nullstellen}\\
So schätzen Sie den Fehler einer Näherungslösung ab:
\begin{enumerate}
    \item Sei $x_n$ der aktuelle Näherungswert
    \item Wähle Toleranz $\epsilon > 0$
    \item Prüfe Vorzeichenwechsel: $f(x_n-\epsilon) \cdot f(x_n+\epsilon) < 0$
    \item Falls ja: Nullstelle liegt in $(x_n-\epsilon, x_n+\epsilon)$
    \item Damit gilt: $|x_n-\xi| < \epsilon$
\end{enumerate}
\end{KR}


\begin{example2}{Praktische Fehlerabschätzung} Fehlerbestimmung bei $f(x)=x^2-2$

    \begin{minipage}[t]{0.6\textwidth}
        
        \begin{enumerate}
            \item Näherungswert: $x_3 = 1.4142157$
            \item Mit $\epsilon = 10^{-5}$:
            \item $f(x_3-\epsilon) = 1.4142057^2 - 2 < 0$
            \item $f(x_3+\epsilon) = 1.4142257^2 - 2 > 0$
        \end{enumerate}
    \end{minipage}
    \begin{minipage}[t]{0.35\textwidth}
        \textbf{Also}: $|x_3-\sqrt{2}| < 10^{-5}$
        
        $\rightarrow$ Nullstelle liegt in $(1.4142057, 1.4142257)$
    \end{minipage}
\end{example2}

\subsubsection{Konvergenzverhalten}

\begin{definition}{Konvergenzordnung}
    Sei $(x_n)$ eine gegen $\bar{x}$ konvergierende Folge. \\
    Die Konvergenzordnung $q \geq 1$ ist definiert durch:
    \vspace{-2mm}\\
    $$|x_{n+1}-\bar{x}| \leq c \cdot |x_n-\bar{x}|^q$$
    wo $c > 0$ eine Konstante. Für $q = 1$ muss zusätzl. $c < 1$ gelten.
\end{definition}

\begin{theorem}{Konvergenzordnungen der Verfahren} Konvergenzgeschwindigkeiten
    \vspace{1mm}\\
    \textbf{Newton-Verfahren:} Quadratische Konvergenz: $q = 2$
    \vspace{1mm}\\
    \textbf{Vereinfachtes Newton:} Lineare Konvergenz: $q = 1$
    \vspace{1mm}\\
    \textbf{Sekantenverfahren:} Superlineare Konvergenz: $q = \frac{1+\sqrt{5}}{2} \approx 1.618$
\end{theorem}

\begin{example2}{Konvergenzgeschwindigkeit} Vergleich der Verfahren:
    \vspace{1mm}\\
    Startwert $x_0 = 1$, Funktion $f(x) = x^2 - 2$, Ziel: $\sqrt{2}$
    \begin{center}
    \begin{tabular}{l|c|c|c}
    n & Newton & Vereinfacht & Sekanten \\\hline
    1 & 1.5000000 & 1.5000000 & 1.5000000\\
    2 & 1.4166667 & 1.4500000 & 1.4545455\\
    3 & 1.4142157 & 1.4250000 & 1.4142857\\
    4 & 1.4142136 & 1.4125000 & 1.4142136
    \end{tabular}
    \end{center}
\end{example2}



\begin{example2}{Fehleranalyse der Verfahren}
Vergleich der Fehlerkonvergenz für $f(x) = e^x - x - 2$:

\paragraph{Theoretisch:}
\begin{itemize}
    \item Newton: $|e_{n+1}| \leq C|e_n|^2$ mit $e_n = x_n - \xi$
    \item Sekanten: $|e_{n+1}| \leq C|e_n|^{1.618}$
    \item Fixpunkt: $|e_{n+1}| \leq \alpha|e_n|$ mit $\alpha < 1$
\end{itemize}

\paragraph{Praktisch:} Mit $x_0 = 1$:
\begin{center}
\begin{tabular}{l|c|c|c}
n & $|x_n-\xi|_{Newton}$ & $|x_n-\xi|_{Sekanten}$ & $|x_n-\xi|_{Fixpunkt}$ \\\hline
1 & 1.0e-1 & 2.3e-1 & 3.1e-1 \\
2 & 5.2e-3 & 4.5e-2 & 9.6e-2 \\
3 & 1.4e-5 & 3.8e-3 & 3.0e-2
\end{tabular}
\end{center}
\end{example2}



\section{Numerische Lösung linearer Gleichungssysteme}

\subsection{Matrix-Zerlegungen}

\begin{definition}{Dreieckszerlegung}
Eine Matrix $A \in \mathbb{R}^{n\times n}$ kann zerlegt werden in:
\vspace{1mm}\\
\begin{minipage}[t]{0.5\textwidth}
    \textbf{Untere Dreiecksmatrix L:}\\
    $l_{ij} = 0$ für $j > i$\\
    Diagonale normiert ($l_{ii}=1$)
\end{minipage}
\hspace{3mm}
\begin{minipage}[t]{0.45\textwidth}
    \textbf{Obere Dreiecksmatrix R:}\\
    $r_{ij} = 0$ für $i > j$\\
    Diagonalelemente $\neq 0$
\end{minipage}
\end{definition}


\subsubsection{LR-Zerlegung}

\begin{theorem}{LR-Zerlegung}\\
Jede reguläre Matrix $A$, für die der Gauss-Algorithmus ohne Zeilenvertauschungen durchführbar ist, lässt sich zerlegen in:
$A = LR$
wobei $L$ eine normierte untere und $R$ eine obere Dreiecksmatrix ist.
\end{theorem}

\begin{KR}{LR-Zerlegung durchführen}
    $(E | A | E) \underbrace{\leadsto}_{Gauss} (P | R | L)$
    \vspace{-3mm}\\
\begin{enumerate}
    \item Zerlegung bestimmen:
    \begin{itemize}
        \item Gauss-Elimination durchführen
        \item Eliminationsfaktoren $-\frac{a_{ji}}{a_{ii}}$ in $L$ speichern
        \item Resultierende obere Dreiecksmatrix ist $R$
    \end{itemize}
    
    \item System lösen:
    \begin{itemize}
        \item Vorwärtseinsetzen: $Ly = b$
        \item Rückwärtseinsetzen: $Rx = y$
    \end{itemize}
    
    \item Bei Pivotisierung:
    \begin{itemize}
        \item Permutationsmatrix $P$ erstellen
        \item $PA = LR$ speichern
        \item $Ly = Pb$ lösen
    \end{itemize}
\end{enumerate}
\end{KR}

\begin{remark}
    $E$ = Einheitsmatrix, $P$ = Permutationsmatrix
\end{remark}


\begin{example2}{LR-Zerlegung}
$\underbrace{\begin{psmallmatrix}
-1 & 1 & 1\\
1 & -3 & -2\\
5 & 1 & 4
\end{psmallmatrix}}_{A}, \underbrace{\begin{psmallmatrix}
0\\
5\\
3
\end{psmallmatrix}}_{b}
\rightarrow \begin{vsmallmatrix}
0 & 0 & 1\\
0 & 1 & 0\\
1 & 0 & 0
\end{vsmallmatrix} \begin{vsmallmatrix}
-1 & 1 & 1\\
1 & -3 & -2\\
5 & 1 & 4
\end{vsmallmatrix}
\begin{vsmallmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{vsmallmatrix}$

\paragraph{Schritt 1: Erste Spalte}
Max. Element in 1. Spalte: $|a_{31}| = 5$, also Z1 und Z3 tauschen:

\begin{minipage}{0.5\textwidth}
$$\begin{vsmallmatrix}
0 & 0 & 1\\
0 & 1 & 0\\
1 & 0 & 0
\end{vsmallmatrix} \begin{vsmallmatrix}
5 & 1 & 4\\
1 & -3 & -2\\
-1 & 1 & 1
\end{vsmallmatrix}
\begin{vsmallmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{vsmallmatrix}$$
\end{minipage}
\begin{minipage}{0.45\textwidth}
    \vspace{2mm}
    Eliminationsfaktoren:\\
    $l_{21} = \frac{1}{5}, \quad l_{31} = -\frac{1}{5}$
\end{minipage}


Nach Elimination:
$\begin{vsmallmatrix}
0 & 0 & 1\\
0 & 1 & 0\\
1 & 0 & 0
\end{vsmallmatrix} \begin{vsmallmatrix}
5 & 1 & 4\\
0 & -3.2 & -2.8\\
0 & 1.2 & 1.8
\end{vsmallmatrix}
\begin{vsmallmatrix}
1 & 0 & 0\\
\frac{1}{5} & 1 & 0\\
-\frac{1}{5} & 0 & 1
\end{vsmallmatrix}$

\paragraph{Schritt 2: Zweite Spalte}
Max. Element in 2. Spalte unter Diagonale: $|-3.2| > |1.2|$, \\ keine Vertauschung nötig.
Eliminationsfaktor:
$l_{32} = \frac{1.2}{-3.2} = -\frac{3}{8}$
\vspace{2mm}\\
Nach Elimination:
$\underbrace{\begin{vsmallmatrix}
0 & 0 & 1\\
0 & 1 & 0\\
1 & 0 & 0
\end{vsmallmatrix}}_{P} \underbrace{\begin{vsmallmatrix}
5 & 1 & 4\\
0 & -3.2 & -2.8\\
0 & 0 & 0.75
\end{vsmallmatrix}}_{R}
\underbrace{\begin{vsmallmatrix}
1 & 0 & 0\\
\frac{1}{5} & 1 & 0\\
-\frac{1}{5} & -\frac{3}{8} & 1
\end{vsmallmatrix}}_{L}$

\paragraph{Lösung des Systems}
\begin{enumerate}
    \item $Pb = \begin{psmallmatrix} 3\\ 5\\ 0 \end{psmallmatrix}$
    \item Löse $Ly = Pb$ durch Vorwärtseinsetzen:
    $y = \begin{psmallmatrix} 3\\ 4.4\\ 2.25 \end{psmallmatrix}$
    \item Löse $Rx = y$ durch Rückwärtseinsetzen:
    $x = \begin{psmallmatrix} -1\\ -4\\ 3 \end{psmallmatrix}$
\end{enumerate}

Probe:
$Ax = \begin{psmallmatrix}
-1 & 1 & 1\\
1 & -3 & -2\\
5 & 1 & 4
\end{psmallmatrix} \begin{psmallmatrix} -1\\ -4\\ 3 \end{psmallmatrix} = \begin{psmallmatrix} 0\\ 5\\ 3 \end{psmallmatrix} = b$
\end{example2}

\columnbreak

\subsubsection{QR-Zerlegung}

\begin{concept}{QR-Zerlegung}\\
Eine orthogonale Matrix $Q \in \mathbb{R}^{n\times n}$ erfüllt: $Q^T Q = QQ^T = I_n$
\vspace{1mm}\\
Die QR-Zerlegung einer Matrix $A$ ist: $A = QR$
\vspace{1mm}\\
wobei $Q$ orthogonal und $R$ eine obere Dreiecksmatrix ist.
\end{concept}

\begin{definition}{Householder-Transformation}\\
Eine Householder-Matrix hat die Form:
$H = I_n - 2uu^T$

mit $u \in \mathbb{R}^n$, $\|u\| = 1$. Es gilt:
\begin{itemize}
    \item $H$ ist orthogonal ($H^T = H^{-1}$) und symmetrisch ($H^T = H$)
    \item $H^2 = I_n$
\end{itemize}
\end{definition}

\begin{KR}{QR-Zerlegung mit Householder}
\begin{enumerate}
    \item Initialisierung: $R := A$, $Q := I_n$
    \item Für $i = 1,\ldots,n-1$:
        \begin{itemize}
            \item Bilde Vektor $v_i$ aus i-ter Spalte von $R$ ab Position $i$
            \item $w_i := v_i + \text{sign}(v_{i1})\|v_i\|e_1$
            \item $u_i := w_i/\|w_i\|$
            \item $H_i := I_{n-i+1} - 2u_iu_i^T$
            \item Erweitere $H_i$ zu $Q_i$ durch $I_{i-1}$ links oben
            \item $R := Q_iR$ und $Q := QQ_i^T$
        \end{itemize}
\end{enumerate}
\end{KR}

\begin{example2}[breakable]{QR-Zerlegung mit Householder}
    %TODO: check if this is correct and/or relevant - either correct or replace with better example
$A = \begin{psmallmatrix}
2 & 5 & -1\\
-1 & -4 & 2\\
0 & 2 & 1
\end{psmallmatrix}$

\paragraph{Schritt 1: Erste Spalte}
Erste Spalte $a_1$ und Einheitsvektor $e_1$:
$a_1 = \begin{psmallmatrix} 2\\ -1\\ 0 \end{psmallmatrix}, \quad e_1 = \begin{psmallmatrix} 1\\ 0\\ 0 \end{psmallmatrix}$

Householder-Vektor für erste Spalte:
\vspace{1mm}
\begin{enumerate}
    \item Berechne Norm: $|a_1| = \sqrt{2^2 + (-1)^2 + 0^2} = \sqrt{5}$
    \vspace{1mm}
    \item Bestimme Vorzeichen: $\text{sign}(a_{11}) = \text{sign}(2) = 1$
         \begin{itemize}
              \item Wähle positives Vorzeichen, da erstes Element positiv
              \item Dies maximiert die erste Komponente von $v_1$
              \item Verhindert Auslöschung bei der Subtraktion
         \end{itemize}
         \vspace{1mm}
    \item $v_1 = a_1 + \text{sign}(a_{11})|a_1|e_1 = \begin{psmallmatrix} 2\\ -1\\ 0 \end{psmallmatrix} + \sqrt{5}\begin{psmallmatrix} 1\\ 0\\ 0 \end{psmallmatrix} = \begin{psmallmatrix} 2 + \sqrt{5}\\ -1\\ 0 \end{psmallmatrix}$
    \vspace{1mm}
    \item Normiere $v_1$: $|v_1| = \sqrt{(2 + \sqrt{5})^2 + 1} \Rightarrow
            u_1 = \frac{v_1}{|v_1|} = \begin{psmallmatrix} 0.91\\ -0.41\\ 0 \end{psmallmatrix}$
\end{enumerate}
\vspace{1mm}
Householder-Matrix berechnen:
$H_1 = I - 2u_1u_1^T = \begin{psmallmatrix} 
-0.67 & -0.75 & 0\\
-0.75 & 0.67 & 0\\
0 & 0 & 1
\end{psmallmatrix}$
A nach 1. Transformation:
$A^{(1)} = H_1A = \begin{psmallmatrix}
-\sqrt{5} & -6.71 & 0.45\\
0 & -0.89 & 1.79\\
0 & 2.00 & 1.00
\end{psmallmatrix}$
\paragraph{Schritt 2: Zweite Spalte}
Untermatrix für zweite Transformation:
$A_2 = \begin{psmallmatrix} -0.89 & 1.79\\ 2.00 & 1.00 \end{psmallmatrix}$

Householder-Vektor für zweite Spalte:
\vspace{1mm}
\begin{enumerate}
    \item $|a_2| = \sqrt{(-0.89)^2 + 2^2} = 2.19$
    \vspace{1mm}
    \item $\text{sign}(a_{22}) = \text{sign}(-0.89) = -1$ (da erstes Element negativ)
    \vspace{1mm}
    \item $v_2 = \begin{psmallmatrix} -0.89\\ 2.00 \end{psmallmatrix} - 2.19\begin{psmallmatrix} 1\\ 0 \end{psmallmatrix} = \begin{psmallmatrix} -3.09\\ 2.00 \end{psmallmatrix}$
    \vspace{1mm}
    \item $u_2 = \frac{v_2}{|v_2|} = \begin{psmallmatrix} -0.84\\ 0.54 \end{psmallmatrix}$
\end{enumerate}
\vspace{1mm}
Erweiterte Householder-Matrix: %TODO: nicer formatting!
$Q_2 = \begin{psmallmatrix}
1 & 0 & 0\\
0 & -0.41 & -0.91\\
0 & -0.91 & 0.41
\end{psmallmatrix}$

nach 2. Transformation:
$R = Q_2A^{(1)} = \begin{psmallmatrix}
-\sqrt{5} & -6.71 & 0.45\\
0 & -2.19 & 1.34\\
0 & 0 & -1.79
\end{psmallmatrix}$

\paragraph{Endergebnis}
Die QR-Zerlegung $A = QR$ ist:

$Q = H_1^TQ_2^T = \begin{psmallmatrix}
-0.89 & -0.45 & 0\\
0.45 & -0.89 & 0\\
0 & 0 & 1
\end{psmallmatrix},
R = \begin{psmallmatrix}
-\sqrt{5} & -6.71 & 0.45\\
0 & -2.19 & 1.34\\
0 & 0 & -1.79
\end{psmallmatrix}$

\paragraph{Probe}
\small
\begin{enumerate}
    \item $QR = A$ (bis auf Rundungsfehler)
    \item $Q^TQ = QQ^T = I$ (Orthogonalität)
    \item $R$ ist obere Dreiecksmatrix
\end{enumerate}

\paragraph{Wichtige Beobachtungen}
\small
\begin{itemize}
    \item Vorzeichenwahl bei $v_k$ ist entscheidend für numerische Stabilität
    \item Ein falsches Vorzeichen kann zu Auslöschung führen
    \item Betrag der Diagonalelemente in $R$ = Norm transformierter Spalten
    \item $Q$ ist orthogonal: Spaltenvektoren sind orthonormal
\end{itemize}
\end{example2}




\subsubsection{Iterative Verfahren}

\begin{definition}{Zerlegung der Systemmatrix} $A$ zerlegt in: $A = L + D + R$
    \small
\begin{itemize}
    \item $L$: streng untere Dreiecksmatrix
    \item $D$: Diagonalmatrix
    \item $R$: streng obere Dreiecksmatrix
\end{itemize}
\end{definition}

\begin{concept}{Jacobi-Verfahren}
Gesamtschrittverfahren 
\vspace{1mm}\\
Iteration: $x^{(k+1)} = -D^{-1}(L + R)x^{(k)} + D^{-1}b$
\vspace{1mm}\\
Komponentenweise:
$x_i^{(k+1)} = \frac{1}{a_{ii}}(b_i - \sum_{j=1,j\neq i}^n a_{ij}x_j^{(k)})$
\end{concept}

\begin{concept}{Gauss-Seidel-Verfahren}
Einzelschrittverfahren 
\vspace{1mm}\\
Iteration: $x^{(k+1)} = -(D+L)^{-1}Rx^{(k)} + (D+L)^{-1}b$
\vspace{1mm}\\
Komponentenweise:\\
$x_i^{(k+1)} = \frac{1}{a_{ii}}(b_i - \sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \sum_{j=i+1}^n a_{ij}x_j^{(k)})$
\end{concept}

\begin{theorem}{Konvergenzkriterien}
Ein iteratives Verfahren konvergiert, wenn:
\begin{enumerate}
    \item Die Matrix $A$ diagonaldominant ist:\\
    $|a_{ii}| > \sum_{j\neq i} |a_{ij}|$ für alle $i$
    \item Der Spektralradius der Iterationsmatrix kleiner 1 ist:\\
    $\rho(B) < 1$ mit $B$ als jeweilige Iterationsmatrix
\end{enumerate}
\end{theorem}

\begin{KR}{Implementierung von Jacobi- und Gauss-Seidel-Verfahren}
    \paragraph{Vorbereitungsphase}
    \begin{itemize}
        \item Matrix zerlegen in $A = L + D + R$
        \item Diagonaldominanz prüfen: $|a_{ii}| > \sum_{j\neq i} |a_{ij}|$ für alle $i$
        \item Sinnvolle Startwerte wählen (z.B. $x^{(0)}=0$ oder $x^{(0)}=b$)
        \item Toleranz $\epsilon$ und max. Iterationszahl $n_{max}$ festlegen
    \end{itemize}

    \paragraph{Verfahren durchführen}
    \begin{itemize}
        \item \textbf{Jacobi}: Komponentenweise parallel berechnen 
        \item \textbf{Gauss-Seidel}: Komponentenweise sequentiell berechnen 
    \end{itemize}

    \paragraph{Konvergenzprüfung/Abbruchkriterien}
    \begin{itemize}
        \item Absolute Änderung: $\|x^{(k+1)} - x^{(k)}\| < \epsilon$
        \item Relatives Residuum: $\frac{\|Ax^{(k)} - b\|}{\|b\|} < \epsilon$
        \item Maximale Iterationszahl: $k < n_{max}$
    \end{itemize}

    \paragraph{A-priori Fehlerabschätzung}
    \begin{itemize}
        \item Spektralradius $\rho$ der Iterationsmatrix bestimmen
        \item Benötigte Iterationen $n$ für Genauigkeit $\epsilon$:\\
        $n \geq \frac{\ln(\epsilon(1-\rho)/\|x^{(1)}-x^{(0)}\|)}{\ln(\rho)}$
    \end{itemize}
\end{KR}







\subsubsection{Fehleranalyse}

\begin{definition}{Matrix- und Vektornormen}\\
Eine Vektornorm $\|\cdot\|$ erfüllt für alle $x,y \in \mathbb{R}^n, \lambda \in \mathbb{R}$:
\begin{itemize}
    \item $\|x\| \geq 0$ und $\|x\| = 0 \Leftrightarrow x = 0$
    \item $\|\lambda x\| = |\lambda| \cdot \|x\|$
    \item $\|x + y\| \leq \|x\| + \|y\|$ (Dreiecksungleichung)
\end{itemize}
\end{definition}

\begin{concept}{Wichtige Normen}

\textbf{1-Norm:}
        $\|x\|_1 = \sum_{i=1}^n |x_i|,
        \|A\|_1 = \max_j \sum_{i=1}^n |a_{ij}|$

\textbf{2-Norm:}
        $\|x\|_2 = \sqrt{\sum_{i=1}^n x_i^2}, 
        \|A\|_2 = \sqrt{\rho(A^TA)}$

$\infty$\textbf{-Norm:}
        $\|x\|_\infty = \max_i |x_i|, 
        \|A\|_\infty = \max_i \sum_{j=1}^n |a_{ij}|$
\end{concept}

\begin{theorem}{Fehlerabschätzung für LGS}\\
Sei $\|\cdot\|$ eine Norm, $A \in \mathbb{R}^{n\times n}$ regulär und $Ax = b$, $A\tilde{x} = \tilde{b}$
\vspace{1mm}\\
\begin{minipage}[t]{0.47\textwidth}
    \textbf{Absoluter Fehler:}
    \vspace{-5mm}\\
    \begin{center}
        $\|x - \tilde{x}\| \leq \|A^{-1}\| \cdot \|b - \tilde{b}\|$
    \end{center}
\end{minipage}
\hspace{2mm}
\begin{minipage}[t]{0.47\textwidth}
    \textbf{Relativer Fehler:}
    \vspace{-5mm}\\
    \begin{center}
        $\frac{\|x - \tilde{x}\|}{\|x\|} \leq \text{cond}(A) \cdot \frac{\|b - \tilde{b}\|}{\|b\|}$
    \end{center}
\end{minipage}
\vspace{1mm}\\
Mit der Konditionszahl $\text{cond}(A) = \|A\| \cdot \|A^{-1}\|$
\end{theorem}

\begin{concept}{Konditionierung}\\
Die Konditionszahl beschreibt die numerische Stabilität eines LGS:
\begin{itemize}
    \item $\text{cond}(A) \approx 1$: gut konditioniert
    \item $\text{cond}(A) \gg 1$: schlecht konditioniert
    \item $\text{cond}(A) \to \infty$: singulär
\end{itemize}
\end{concept}

\begin{example2}{Konditionierung}
$A = \begin{psmallmatrix}
1 & 1\\
1 & 1.01
\end{psmallmatrix}, b = \begin{psmallmatrix}
2\\
2.01
\end{psmallmatrix}$\\
Konditionszahl:
$\text{cond}(A) = \|A\| \cdot \|A^{-1}\| \approx 400$
\paragraph{Fehlerabschätzung}
$\text{Absoluter Fehler: }\|x - \tilde{x}\| \leq 400 \cdot 0.01 = 4$ \\
$\text{Relativer Fehler: }\frac{\|x - \tilde{x}\|}{\|x\|} \leq 400 \cdot \frac{0.01}{2} = 2$
\end{example2}

\begin{KR}{Vergleich Lösungsverfahren}
$A = \begin{psmallmatrix}
1 & 2 & 0\\
2 & 1 & 2\\
0 & 2 & 1
\end{psmallmatrix}, \quad b = \begin{psmallmatrix}
1\\
2\\
3
\end{psmallmatrix}$

\small
\begin{itemize}
    \item Matrix ist symmetrisch und nicht streng diagonaldominant
    \item $\text{cond}_\infty(A) \approx 12.5$
\end{itemize}
\begin{center}
\begin{tabular}{l|ccc}
Verfahren & Iterationen & Residuum & Zeit\\
\hline
LR mit Pivot & 1 & $2.2\cdot10^{-16}$ & 1.0\\
QR & 1 & $2.2\cdot10^{-16}$ & 2.3\\
Jacobi & 12 & $1.0\cdot10^{-6}$ & 1.8\\
Gauss-Seidel & 7 & $1.0\cdot10^{-6}$ & 1.4\\
\end{tabular}
\end{center}
\vspace{-2mm}
\begin{itemize}
    \item Direkte Verfahren erreichen höhere Genauigkeit
    \item Iterative Verfahren brauchen mehrere Schritte
\end{itemize}
\end{KR}

\begin{example2}{Konvergenzverhalten}
$\begin{psmallmatrix}
4 & 1 & 0\\
1 & 4 & 1\\
0 & 1 & 4
\end{psmallmatrix}
\begin{psmallmatrix}
x_1\\
x_2\\
x_3
\end{psmallmatrix} =
\begin{psmallmatrix}
1\\
2\\
3
\end{psmallmatrix}$
\vspace{1mm}\\
\small
Die Matrix ist diagonaldominant:
$|a_{ii}| = 4 > 1 = \sum_{j\neq i} |a_{ij}|$
\vspace{1mm}\\
\begin{tabular}{c|cc|cc}
k & \multicolumn{2}{c|}{Residuum} & \multicolumn{2}{c}{Rel. Fehler}\\
& Jacobi & G-S & Jacobi & G-S\\
\hline
0 & 3.74 & 3.74 & - & -\\
1 & 0.94 & 0.47 & 0.935 & 0.468\\
2 & 0.23 & 0.06 & 0.246 & 0.125\\
3 & 0.06 & 0.01 & 0.065 & 0.017\\
4 & 0.01 & 0.001 & 0.016 & 0.002
\end{tabular}

\paragraph{Beobachtungen:}
\begin{itemize}
    \item Gauss-Seidel konvergiert etwa doppelt so schnell wie Jacobi
    \item Das Residuum fällt linear (geometrische Folge)
    \item Die Konvergenz ist gleichmäßig (keine Oszillationen)
\end{itemize}
\end{example2}

\subsubsection{Vorgehen und Implementation}

\begin{KR}{Systematisches Vorgehen bei LGS}
\begin{enumerate}
    \item Eigenschaften der Matrix analysieren:
    \begin{itemize}
        \item Diagonaldominanz prüfen
        \item Konditionszahl berechnen oder abschätzen
        \item Symmetrie erkennen
    \end{itemize}
    
    \item Verfahren auswählen:
    \begin{itemize}
        \item Direkte Verfahren: für kleinere Systeme
        \item Iterative Verfahren: für große, dünnbesetzte Systeme
        \item Spezialverfahren: für symmetrische/bandförmige Matrizen
    \end{itemize}
    
    \item Implementation planen:
    \begin{itemize}
        \item Pivotisierung bei Gauss berücksichtigen
        \item Speicherbedarf beachten
        \item Abbruchkriterien festlegen
    \end{itemize}
\end{enumerate}
\end{KR}

\begin{KR}{Zeilenvertauschungen verfolgen}
\begin{enumerate}
    \item Initialisiere $P = I_n$
    \item Für jede Vertauschung von Zeile $i$ und $j$:
    \begin{itemize}
        \item Erstelle $P_k$ durch Vertauschen von Zeilen $i,j$ in $I_n$
        \item Aktualisiere $P = P_k \cdot P$
        \item Wende Vertauschung auf Matrix an: $A := P_kA$
    \end{itemize}
    \item Bei der LR-Zerlegung mit Pivotisierung:
    \begin{itemize}
        \item $PA = LR$ 
        \item Löse $Ly = Pb$ und $Rx = y$
    \end{itemize}
\end{enumerate}
\end{KR}



\subsection{Numerische Berechnung von Eigenwerten}

\subsubsection{QR-Verfahren}

\begin{concept}{QR-Verfahren}\\
Das QR-Verfahren transformiert die Matrix $A$ iterativ in eine obere Dreiecksmatrix, deren Diagonalelemente die Eigenwerte sind:
\begin{enumerate}
    \item Initialisierung: $A_0 := A$, $P_0 := I_n$
    \item Für $i = 0,1,2,\ldots$:
    \begin{itemize}
        \item QR-Zerlegung: $A_i = Q_iR_i$
        \item Neue Matrix: $A_{i+1} = R_iQ_i$
        \item Update: $P_{i+1} = P_iQ_i$
    \end{itemize}
\end{enumerate}
\end{concept}

\begin{KR}{QR-Verfahren}
\paragraph{Voraussetzungen}
\begin{itemize}
    \item Matrix $A \in \mathbb{R}^{n \times n}$
    \item Eigenwerte sollten verschiedene Beträge haben für gute Konvergenz
\end{itemize}

\paragraph{Algorithmus}
\begin{enumerate}
    \item Initialisierung:
    \begin{itemize}
        \item $A_0 := A$ und $Q_0 := I_n$
        \item Maximale Iterationszahl und Toleranz festlegen
    \end{itemize}

    \item Für $k = 0,1,2,\ldots$ bis zur Konvergenz:
    \begin{itemize}
        \item QR-Zerlegung von $A_k$ berechnen: $A_k = Q_kR_k$
        \item Neue Matrix berechnen: $A_{k+1} = R_kQ_k$
        \item Transformationsmatrix aktualisieren: $P_{k+1} = P_kQ_k$
    \end{itemize}

    \item Abbruchkriterien prüfen:
    \begin{itemize}
        \item Subdiagonalelemente nahe Null: $|a_{i+1,i}| < \varepsilon$
        \item Änderung der Diagonalelemente klein
        \item Maximale Iterationszahl erreicht
    \end{itemize}
\end{enumerate}

\paragraph{Auswertung}
\begin{itemize}
    \item Eigenwerte: Diagonalelemente von $A_k$
    \item Eigenvektoren: Spalten der Matrix $P_k$
    \item Bei $2\times2$-Blöcken: Komplexe Eigenwertpaare
\end{itemize}
\end{KR}

\begin{example2}{QR-Verfahren}
Gegeben sei die Matrix
$A = \begin{psmallmatrix} 
1 & 2 & 0 \\
2 & 1 & 1 \\
0 & 1 & 1
\end{psmallmatrix}$

\begin{enumerate}
    \item Erste Iteration:
    \begin{itemize}
        \item QR-Zerlegung:
        $Q_1 = \begin{psmallmatrix} 
        0.45 & 0.89 & 0 \\
        0.89 & -0.45 & 0 \\
        0 & 0 & 1
        \end{psmallmatrix}$,
        $R_1 = \begin{psmallmatrix}
        2.24 & 2.24 & 0.45 \\
        0 & -1 & 0.89 \\
        0 & 0 & 1
        \end{psmallmatrix}$
        \item $A_1 = R_1Q_1 = \begin{psmallmatrix}
        2.24 & 0.45 & 0.45 \\
        0.45 & 0.38 & 0.89 \\
        0.45 & 0.89 & 1
        \end{psmallmatrix}$
    \end{itemize}

    \item Nach Konvergenz:
    $A_k \approx \begin{psmallmatrix}
    3 & * & * \\
    0 & 0 & * \\
    0 & 0 & 0
    \end{psmallmatrix}$

    Eigenwerte sind also $\lambda_1 = 3, \lambda_2 = 0, \lambda_3 = 0$
\end{enumerate}
\end{example2}

\begin{example2}{QR-Iteration} Gegeben:
$A = \begin{psmallmatrix}
1 & 1 \\
1 & 0
\end{psmallmatrix}$

\paragraph{Lösung:}
\begin{enumerate}
    \item QR-Zerlegung von $A$:
    $Q_1 = \frac{1}{\sqrt{2}}\begin{psmallmatrix}
    1 & -1 \\
    1 & 1
    \end{psmallmatrix}, 
    R_1 = \frac{1}{\sqrt{2}}\begin{psmallmatrix}
    \sqrt{2} & 1 \\
    0 & -1
    \end{psmallmatrix}$
    \vspace{1mm}
    \item Neue Matrix:
    $A_1 = R_1Q_1 = \begin{psmallmatrix}
    1.5 & 0.5 \\
    0.5 & -0.5
    \end{psmallmatrix}$
    \vspace{1mm}
    \item Konvergenz nach mehreren Iterationen gegen:\\
    $A_\infty \approx \begin{psmallmatrix}
    \phi & 0 \\
    0 & -\phi^{-1}
    \end{psmallmatrix}$
    mit $\phi = \frac{1+\sqrt{5}}{2}$
    \vspace{1mm}
    \item Eigenwerte: $\lambda_1 = \phi, \lambda_2 = -\phi^{-1}$
\end{enumerate}
\end{example2}

\subsubsection{Von-Mises-Iteration}

\begin{concept}{Von-Mises-Iteration (Vektoriteration)}\\
Für eine diagonalisierbare Matrix $A$ mit Eigenwerten $|\lambda_1| > |\lambda_2| \geq \cdots \geq |\lambda_n|$ konvergiert die Folge:
$$v^{(k+1)} = \frac{Av^{(k)}}{\|Av^{(k)}\|_2}, \quad
\lambda^{(k+1)} = \frac{(v^{(k)})^TAv^{(k)}}{(v^{(k)})^Tv^{(k)}}$$
gegen einen Eigenvektor $v$ zum betragsmäßig größten Eigenwert $\lambda_1$.\\
$\Rightarrow$ sehe \textcolor{blue}{Spektralradius} auf nächster Seite
\end{concept}

\begin{KR}{Von-Mises-Iteration / Vektoriteration}
\paragraph{Voraussetzungen}
\begin{itemize}
    \item Matrix diagonalisierbar und $|\lambda_1| > |\lambda_2|$
\end{itemize}

\paragraph{Iteration durchführen}
\begin{itemize}
    \item Startvektor $v^{(0)}$ wählen:
    \begin{itemize}
        \item Zufälligen Vektor oder $(1,\ldots,1)^T$ wählen
        \item Auf Länge 1 normieren: $\|v^{(0)}\|_2 = 1$
    \end{itemize}

    \item Für $k = 0,1,2,\ldots$ bis zur Konvergenz:
    \vspace{1mm}
    \begin{enumerate}
        \item Iterationsvektor berechnen: $w^{(k)} = Av^{(k)}$
        \vspace{1mm}
        \item Normieren: $v^{(k+1)} = \frac{w^{(k)}}{\|w^{(k)}\|_2}$
        \vspace{1mm}
        \item Eigenwertapproximation:
              $\lambda^{(k+1)} = \frac{(v^{(k)})^TAv^{(k)}}{(v^{(k)})^Tv^{(k)}}$\\
              (Rayleigh-Quotient)
        \end{enumerate}
        \vspace{1mm}
    

    \item Abbruchkriterien prüfen:
    \begin{itemize}
        \item Änderung des Eigenvektors: $\|v^{(k+1)} - v^{(k)}\| < \varepsilon$
        \item Änderung des Eigenwertes: $|\lambda^{(k+1)} - \lambda^{(k)}| < \varepsilon$
        \item Maximale Iterationszahl erreicht
    \end{itemize}

    \item Konvergenz:
\begin{itemize}
    \item $v^{(k)} \to$ Eigenvektor zu $|\lambda_1|$
    \item $\lambda^{(k)} \to |\lambda_1|$
\end{itemize}
\end{itemize}

\paragraph{Verifikation}
\begin{itemize}
    \item Prüfen ob $Av^{(k)} \approx \lambda^{(k)}v^{(k)}$
    \item Residuum berechnen: $\|Av^{(k)} - \lambda^{(k)}v^{(k)}\|$
    \item Orthogonalität zu anderen Eigenvektoren prüfen
\end{itemize}
\end{KR}

\begin{example2}{Von-Mises-Iteration}
Gegeben sei die Matrix
$A = \begin{psmallmatrix} 
4 & -1 & 1 \\
-1 & 3 & -2 \\
1 & -2 & 3
\end{psmallmatrix}$

Mit Startvektor $v^{(0)} = \frac{1}{\sqrt{3}}(1,1,1)^T$:
\vspace{1mm}\\
\begin{enumerate}
    \item Erste Iteration:
    \vspace{1mm}
    \begin{itemize}
        \item $w^{(0)} = Av^{(0)} = \frac{1}{\sqrt{3}}(4,0,2)^T$
        \vspace{1mm}
        \item $v^{(1)} = \frac{w^{(0)}}{\|w^{(0)}\|} = \frac{1}{\sqrt{20}}(4,0,2)^T$
        \vspace{1mm}
        \item $\lambda^{(1)} = (v^{(0)})^TAv^{(0)} = 3.33$
        \vspace{1mm}
    \end{itemize}

    \item Zweite Iteration:
    \vspace{1mm}
    \begin{itemize}
        \item $w^{(1)} = Av^{(1)} = \frac{1}{\sqrt{20}}(18,-2,8)^T$
        \vspace{1mm}
        \item $v^{(2)} = \frac{w^{(1)}}{\|w^{(1)}\|} = \frac{1}{\sqrt{388}}(18,-2,8)^T$
        \vspace{1mm}
        \item $\lambda^{(2)} = 5.12$
        \vspace{1mm}
    \end{itemize}

Konvergenz gegen $\lambda_1 \approx 5.17$ und $v = (0.89, -0.10, 0.39)^T$
\end{enumerate}
\end{example2}





\subsubsection{Inverse Iteration}

\begin{concept}{Inverse Iteration}
Die inverse Iteration berechnet einen Eigenvektor zu einem bekannten oder geschätzten Eigenwert $\mu$ durch:
$$v^{(k+1)} = \frac{(A-\mu I)^{-1}v^{(k)}}{\|(A-\mu I)^{-1}v^{(k)}\|_2}$$
Konvergiert typischerweise gegen den Eigenvektor zum betragsmäßig kleinsten Eigenwert $\lambda_i - \mu$.
\end{concept}

\begin{KR}{Inverse Iteration anwenden}
\begin{enumerate}
    \item Vorbereitung:
    \begin{itemize}
        \item Näherungswert $\mu$ für Eigenwert wählen
        \item Zufälligen Startvektor $v^{(0)}$ normieren 
        \item LR-Zerlegung von $(A-\mu I)$ berechnen
    \end{itemize}
    
    \item Iteration durchführen:
    \begin{itemize}
        \item LR-System $(A-\mu I)w^{(k)} = v^{(k)}$ lösen
        \item Neuen Vektor normieren: $v^{(k+1)} = \frac{w^{(k)}}{\|w^{(k)}\|_2}$
        \item Rayleigh-Quotient berechnen für Eigenwert
    \end{itemize}
    
    \item Abbruch wenn:
    \begin{itemize}
        \item Residuum $\|(A-\lambda^{(k)}I)v^{(k)}\| < \epsilon$
        \item Maximale Iterationszahl erreicht
    \end{itemize}
\end{enumerate}
\end{KR}

\begin{example2}{Inverse Iteration}
Bestimmen Sie einen Eigenvektor zum Eigenwert $\lambda \approx 2$ der Matrix:
$$A = \begin{psmallmatrix}
2.1 & -0.1 & 0.1 \\
-0.1 & 2.0 & 0.2 \\
0.1 & 0.2 & 1.9
\end{psmallmatrix}$$

\paragraph{Lösung:}
\begin{enumerate}
    \item $\mu = 2.0$ als Näherung wählen
    \item Startvektor $v^{(0)} = \frac{1}{\sqrt{3}}(1,1,1)^T$
    \item Erste Iteration:
    \begin{itemize}
        \item $(A-2I)w^{(0)} = v^{(0)}$ lösen
        \item $v^{(1)} = \frac{w^{(0)}}{\|w^{(0)}\|} \approx (0.61, 0.63, 0.48)^T$
        \item $\lambda^{(1)} \approx 2.01$
    \end{itemize}
\end{enumerate}
\end{example2}


\subsubsection{Vergleich der Eigenwertverfahren}





\begin{concept}{Vergleich der Eigenwertverfahren}
\begin{enumerate}
    \item Von-Mises Iteration: Findet betragsmäßig größten Eigenwert
    \begin{itemize}
        \item Einfach zu implementieren, Langsame lineare Konvergenz
    \end{itemize}
    
    \item Inverse Iteration: Braucht Näherung für Eigenwert
    \begin{itemize}
        \item Schnelle Konvergenz, LR-Zerlegung pro Schritt nötig
    \end{itemize}
    
    \item QR-Verfahren: Berechnet alle Eigenwerte
    \begin{itemize}
        \item Kubischer Aufwand pro Iteration, Globale und stabile Konvergenz
    \end{itemize}
\end{enumerate}
\end{concept}

\begin{corollary}{Numerische Aspekte der Verfahren}
\begin{itemize}
    \item Wahl des Startpunkts:
    \begin{itemize}
        \item Von-Mises: zufälliger normierter Vektor
        \item Inverse Iteration: Näherung für $\mu$ wichtig
        \item QR: Matrix vorher auf Hessenberg-Form
    \end{itemize}
    
    \item Konvergenzprüfung:
    \begin{itemize}
        \item Residuum $\|Ax^{(k)} - \lambda^{(k)}x^{(k)}\|$
        \item Änderung in aufeinanderfolgenden Iterationen
        \item Subdiagonalelemente bei QR
    \end{itemize}
\end{itemize}
\end{corollary}

\subsubsection{Spektralradius und Konvergenz}

\begin{definition}{Spektralradius}
Der Spektralradius einer Matrix $A$ ist definiert als:
$$\rho(A) = \max\{|\lambda| \mid \lambda \text{ ist Eigenwert von } A\}$$
Er gibt den Betrag des betragsmäßig größten Eigenwerts an.
\end{definition}

\begin{concept}{Bedeutung des Spektralradius}
Der Spektralradius ist wichtig für:
\begin{itemize}
    \item Konvergenz von Iterationsverfahren
    \item Stabilität dynamischer Systeme 
    \item Abschätzung von Matrixnormen
    \item Konvergenz von Potenzreihen mit Matrizen
\end{itemize}
\end{concept}
\begin{theorem}{Konvergenzsatz}
Für eine Matrix $A \in \mathbb{R}^{n\times n}$ sind äquivalent:
\begin{itemize}
    \item $\rho(A) < 1$
    \item $\lim_{k\to\infty} A^k = 0$
    \item Die Neumannsche Reihe $\sum_{k=0}^\infty A^k$ konvergiert
    \item $(I-A)$ ist invertierbar mit $(I-A)^{-1} = \sum_{k=0}^\infty A^k$
\end{itemize}
\end{theorem}

\begin{KR}{Spektralradius bestimmen und anwenden}
\begin{enumerate}
    \item Berechnung:
    \begin{itemize}
        \item Eigenwerte $\lambda_i$ bestimmen
        \item Maximum der Absolutbeträge bilden
        \item Bei großen Matrizen: numerische Verfahren
    \end{itemize}
    
    \item Konvergenzanalyse:
    \begin{itemize}
        \item Bei Iterationsverfahren: $\rho(M) < 1$ prüfen
        \item Bei Matrixpotenzen: $\rho(A) < 1$ prüfen
        \item Konvergenzgeschwindigkeit $\approx |\rho(A)|^k$
    \end{itemize}
    
    \item Abschätzungen:
    \begin{itemize}
        \item $\rho(A) \leq \|A\|$ für jede Matrixnorm
        \item $\rho(AB) = \rho(BA)$ für beliebige Matrizen
        \item $\rho(A^k) = [\rho(A)]^k$ für $k \in \mathbb{N}$
    \end{itemize}
\end{enumerate}
\end{KR}

\begin{KR}{Anwendungen des Spektralradius}
\begin{enumerate}
    \item Iterative Verfahren:
    \begin{itemize}
        \item Jacobi: $\rho(-D^{-1}(L+R)) < 1$
        \item Gauss-Seidel: $\rho(-(D+L)^{-1}R) < 1$
        \item SOR: Optimaler Parameter $\omega$ bestimmen
    \end{itemize}
    
    \item Matrixreihen:
    \begin{itemize}
        \item Konvergenz von $\sum_{k=0}^\infty A^k$
        und Existenz von $(I-A)^{-1}$
        \item Abschätzung der Reihensumme
    \end{itemize}
\end{enumerate}
\end{KR}

\begin{example2}{Spektralradius und Konvergenz}
Untersuchen Sie die Konvergenz des Jacobi-Verfahrens für:
$A = \begin{psmallmatrix}
4 & -1 & 0 \\
-1 & 4 & -1 \\
0 & -1 & 4
\end{psmallmatrix}$

\paragraph{Lösung:}
\begin{enumerate}
    \item Zerlegung $A = D + L + R$:
    $D = \begin{psmallmatrix}
    4 & 0 & 0 \\
    0 & 4 & 0 \\
    0 & 0 & 4
    \end{psmallmatrix}, L+R = \begin{psmallmatrix}
    0 & -1 & 0 \\
    -1 & 0 & -1 \\
    0 & -1 & 0
    \end{psmallmatrix}$
    
    \item Jacobi-Matrix $M = -D^{-1}(L+R)$:
    $M = \begin{psmallmatrix}
    0 & 1/4 & 0 \\
    1/4 & 0 & 1/4 \\
    0 & 1/4 & 0
    \end{psmallmatrix}$
    
    \item Eigenwerte von $M$: $\lambda_1 = 0.5$, $\lambda_2 = 0$, $\lambda_3 = -0.5$
    
    \item Spektralradius: $\rho(M) = 0.5 < 1$
    
    \item Schlussfolgerung:
    \begin{itemize}
        \item Jacobi-Verfahren konvergiert (Konvergenzrate ist linear)
        \item Fehler reduziert sich pro Iteration etwa um Faktor 0.5
    \end{itemize}
\end{enumerate}
\end{example2}














