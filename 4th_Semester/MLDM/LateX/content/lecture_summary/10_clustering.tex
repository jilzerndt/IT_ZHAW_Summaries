\section{Clustering}

\subsection{Unsupervised Learning}

\begin{definition}{Unsupervised Learning}\\
In unsupervised learning, the training data does not contain any output (target) values. The goal is to model the underlying distribution of the data to describe its structure, discover hidden patterns, and gain insights.
\end{definition}

\subsection{Introduction to Clustering}

\begin{definition}{Clustering}\\
Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to objects in other groups. Given a set of $M$ data points $X = \{x^{(1)}, x^{(2)}, \ldots, x^{(M)}\}$, where each data point consists of $N$ features $x^{(i)} = (x^{(i)}_1, \ldots, x^{(i)}_N) \in \mathbb{R}^N$, and a distance measure $d$, a clustering algorithm separates the data into $K$ clusters.
\end{definition}

\begin{concept}{Types of Clustering}\\
There are two main types of clustering:
\begin{itemize}
    \item \textbf{Hard Clustering}: Each data point is assigned to exactly one cluster
    \item \textbf{Soft Clustering}: Each data point is assigned a probability or membership degree for each cluster
\end{itemize}
\end{concept}

\begin{example}{Clustering Application}
Customer segmentation for targeted marketing:
\begin{itemize}
    \item Data: Customer purchase history, demographics, website behavior
    \item Clustering reveals natural customer segments:
    \begin{itemize}
        \item Cluster 1: Young, high-income professionals who buy luxury items
        \item Cluster 2: Middle-aged parents who focus on household essentials
        \item Cluster 3: Budget-conscious shoppers who primarily buy discounted items
    \end{itemize}
    \item Marketing strategies can be tailored to each segment
\end{itemize}
\end{example}

\subsection{K-Means Algorithm}

\begin{definition}{K-Means Algorithm}\\
K-means is a clustering algorithm that aims to partition $M$ data points into $K$ clusters, where each data point belongs to the cluster with the nearest mean (centroid). The algorithm minimizes the within-cluster sum of squares (inertia):
\[\sum_{k=1}^{K} \sum_{x^{(i)} \in C_k} ||x^{(i)} - \mu_k||^2\]
where $C_k$ is the set of points in cluster $k$ and $\mu_k$ is the centroid of cluster $k$.
\end{definition}

\begin{KR}{Implementing K-Means}\\
\paragraph{Initialize centroids}
Choose $K$ initial centroids:
\begin{itemize}
    \item Random selection: Randomly select $K$ data points
    \item K-means++: Select centroids that are farther apart
\end{itemize}

\paragraph{Assign points to closest centroid}
For each data point $x^{(i)}$:
\begin{itemize}
    \item Calculate distance to each centroid
    \item Assign the point to the closest centroid's cluster
\end{itemize}

\paragraph{Update centroids}
For each cluster $k$:
\begin{itemize}
    \item Calculate the mean of all points in the cluster
    \item Set the new centroid to this mean: $\mu_k = \frac{1}{|C_k|} \sum_{x^{(i)} \in C_k} x^{(i)}$
\end{itemize}

\paragraph{Repeat until convergence}
Repeat steps 2 and 3 until:
\begin{itemize}
    \item Centroids no longer change significantly
    \item Maximum number of iterations is reached
    \item Assignment of points to clusters stabilizes
\end{itemize}
\end{KR}

\begin{example2}{K-Means Step-by-Step}\\
Consider a dataset with six 2D points: (1,1), (2,1), (4,3), (5,4), (1,2), (2,2)
\begin{itemize}
    \item Initialize $K=2$ centroids: $\mu_1 = (1,1)$ and $\mu_2 = (5,4)$
\end{itemize}
\tcblower
Iteration 1:
\begin{itemize}
    \item Assign points to clusters:
    \begin{itemize}
        \item Cluster 1: (1,1), (2,1), (1,2), (2,2) [closer to $\mu_1$]
        \item Cluster 2: (4,3), (5,4) [closer to $\mu_2$]
    \end{itemize}
    \item Update centroids:
    \begin{itemize}
        \item $\mu_1 = \frac{(1,1) + (2,1) + (1,2) + (2,2)}{4} = (1.5, 1.5)$
        \item $\mu_2 = \frac{(4,3) + (5,4)}{2} = (4.5, 3.5)$
    \end{itemize}
\end{itemize}

Iteration 2:
\begin{itemize}
    \item Assign points to clusters:
    \begin{itemize}
        \item Cluster 1: (1,1), (2,1), (1,2), (2,2) [closer to $\mu_1$]
        \item Cluster 2: (4,3), (5,4) [closer to $\mu_2$]
    \end{itemize}
    \item The assignments haven't changed, so the algorithm has converged
\end{itemize}

Final clusters:
\begin{itemize}
    \item Cluster 1: (1,1), (2,1), (1,2), (2,2) with centroid (1.5, 1.5)
    \item Cluster 2: (4,3), (5,4) with centroid (4.5, 3.5)
\end{itemize}
\end{example2}

\begin{definition}{K-means++}\\
K-means++ is an initialization method for K-means that selects initial centroids that are far away from each other:
\begin{enumerate}
    \item Choose the first centroid randomly from the data points
    \item For each subsequent centroid, select a data point with probability proportional to the squared distance to the nearest existing centroid
    \item Repeat until $K$ centroids are selected
\end{enumerate}
This approach typically leads to better and more consistent results than random initialization.
\end{definition}

\subsection{Evaluating Clustering Quality}

\begin{definition}{Inertia}\\
Inertia (within-cluster sum of squares) measures how internally coherent clusters are:
\[\text{Inertia} = \sum_{k=1}^{K} \sum_{x^{(i)} \in C_k} ||x^{(i)} - \mu_k||^2\]
Lower inertia indicates better clustering, but it always decreases with more clusters.
\end{definition}

\begin{definition}{Silhouette Score}\\
The silhouette score measures how similar objects are to their own cluster compared to other clusters:
\[s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}\]
where:
\begin{itemize}
    \item $a(i)$ is the average distance of point $i$ to other points in the same cluster
    \item $b(i)$ is the minimum average distance of point $i$ to points in a different cluster
\end{itemize}
The silhouette score ranges from -1 to 1, with higher values indicating better clustering.
\end{definition}

\begin{concept}{Elbow Method}\\
The elbow method helps find the optimal number of clusters:
\begin{itemize}
    \item Run K-means with different values of $K$
    \item Plot inertia vs. number of clusters
    \item Look for the "elbow" point where adding more clusters gives diminishing returns
\end{itemize}
\end{concept}

\begin{KR}{Choosing the Optimal Number of Clusters}\\
\paragraph{Elbow method}
\begin{itemize}
    \item Run K-means for a range of $K$ values (e.g., 1-10)
    \item Plot inertia (within-cluster sum of squares) vs. $K$
    \item Identify the "elbow" point where the rate of decrease sharply changes
\end{itemize}

\paragraph{Silhouette method}
\begin{itemize}
    \item Run K-means for a range of $K$ values
    \item Calculate the average silhouette score for each $K$
    \item Choose $K$ that maximizes the average silhouette score
\end{itemize}

\paragraph{Gap statistic}
\begin{itemize}
    \item Compare the within-cluster dispersion to that expected under a null reference distribution
    \item Choose $K$ that maximizes the gap statistic
\end{itemize}

\paragraph{Domain knowledge}
\begin{itemize}
    \item Consider business requirements or prior knowledge
    \item Sometimes the number of clusters has a natural interpretation in the domain
\end{itemize}
\end{KR}

\subsection{DBSCAN Algorithm}

\begin{definition}{DBSCAN}\\
Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a clustering algorithm that groups together points that are closely packed together (points with many nearby neighbors):
\begin{itemize}
    \item Does not require specifying the number of clusters in advance
    \item Can find arbitrarily shaped clusters
    \item Has a notion of noise (points that don't belong to any cluster)
    \item Based on two parameters: $\epsilon$ (maximum distance between points) and minPts (minimum number of points in a neighborhood)
\end{itemize}
\end{definition}

\begin{definition}{Types of Points in DBSCAN}\\
DBSCAN classifies points into three types:
\begin{itemize}
    \item \textbf{Core Point}: Has at least minPts points within distance $\epsilon$
    \item \textbf{Border Point}: Has at least one core point within distance $\epsilon$ but fewer than minPts points within distance $\epsilon$
    \item \textbf{Noise Point (Outlier)}: Neither a core point nor a border point
\end{itemize}
\end{definition}

\begin{KR}{Implementing DBSCAN}\\
\paragraph{Set parameters}
\begin{itemize}
    \item $\epsilon$: Maximum distance between points in the same neighborhood
    \item minPts: Minimum number of points required to form a dense region
\end{itemize}

\paragraph{Classify points}
For each unvisited point $P$:
\begin{itemize}
    \item Mark $P$ as visited
    \item Find all points within distance $\epsilon$ of $P$ (its neighborhood)
    \item If neighborhood has fewer than minPts, mark $P$ as noise (can be changed later)
    \item If neighborhood has at least minPts, start a new cluster with $P$ as a core point
\end{itemize}

\paragraph{Expand clusters}
For each core point $P$ in a cluster:
\begin{itemize}
    \item Find all points in the $\epsilon$-neighborhood of $P$
    \item For each point $Q$ in the neighborhood:
    \begin{itemize}
        \item If $Q$ is unvisited or marked as noise, add it to the cluster
        \item If $Q$ is a core point, recursively expand the cluster from $Q$
    \end{itemize}
\end{itemize}
\end{KR}

\begin{concept}{Advantages of DBSCAN}\\
DBSCAN has several advantages over other clustering algorithms:
\begin{itemize}
    \item Doesn't require specifying the number of clusters in advance
    \item Can find arbitrarily shaped clusters, not just spherical ones
    \item Robust to outliers (identifies them as noise points)
    \item Only needs two parameters ($\epsilon$ and minPts)
    \item Can handle clusters of different sizes and densities (to some extent)
\end{itemize}
\end{concept}

\begin{concept}{Limitations of DBSCAN}\\
DBSCAN also has some limitations:
\begin{itemize}
    \item Struggles with varying density clusters
    \item Sensitive to parameter choices
    \item Doesn't work well with high-dimensional data due to the "curse of dimensionality"
    \item Not deterministic when points can be reached through multiple paths
    \item Computationally expensive for large datasets (though optimized implementations exist)
\end{itemize}
\end{concept}

\begin{example}{DBSCAN Application}
Consider identifying geographical regions in a city based on crime density:
\begin{itemize}
    \item Data: Latitude and longitude coordinates of crime incidents
    \item Parameters: $\epsilon = 0.5$ km, minPts = 10
    \item Results:
    \begin{itemize}
        \item Cluster 1: Downtown area with high crime density
        \item Cluster 2: Entertainment district with moderate crime
        \item Cluster 3: Shopping mall area with focused retail theft
        \item Noise points: Isolated incidents throughout the city
    \end{itemize}
    \item Advantages: Naturally identifies crime hotspots without predefined number of clusters
\end{itemize}
\end{example}

\subsection{Distance Metrics}

\begin{definition}{Common Distance Metrics}\\
The choice of distance metric affects clustering results:
\begin{itemize}
    \item \textbf{Euclidean Distance}: $d(p, q) = \sqrt{\sum_{i=1}^{N} (q_i - p_i)^2}$ (standard straight-line distance)
    \item \textbf{Manhattan Distance}: $d(p, q) = \sum_{i=1}^{N} |q_i - p_i|$ (sum of absolute differences)
    \item \textbf{Maximum Distance}: $d(p, q) = \max_i |q_i - p_i|$ (largest difference along any dimension)
    \item \textbf{Cosine Similarity}: $d_{cos}(p, q) = \frac{\sum_{i=1}^{N} p_i q_i}{\sqrt{\sum_{i=1}^{N} p_i^2} \sqrt{\sum_{i=1}^{N} q_i^2}}$ (angle between vectors)
\end{itemize}
\end{definition}

\begin{KR}{Choosing the Right Distance Metric}\\
\paragraph{Understand the data}
\begin{itemize}
    \item Consider the nature of features (categorical vs. numerical)
    \item Consider the relative importance of features
    \item Consider the scale of different features
\end{itemize}

\paragraph{Consider the application domain}
\begin{itemize}
    \item Euclidean: Good for continuous data in low dimensions
    \item Manhattan: Good when movement is restricted along axes (e.g., city blocks)
    \item Cosine: Good for text documents or high-dimensional data where magnitude is less important
    \item Correlation-based: Good when patterns matter more than absolute values
\end{itemize}

\paragraph{Preprocessing matters}
\begin{itemize}
    \item Standardize or normalize features before clustering
    \item Consider dimensionality reduction for high-dimensional data
    \item Handle categorical variables appropriately (one-hot encoding, etc.)
\end{itemize}
\end{KR}

\subsection{Other Clustering Algorithms}

\begin{concept}{Hierarchical Clustering}\\
Hierarchical clustering builds a tree of clusters (dendrogram):
\begin{itemize}
    \item \textbf{Agglomerative (bottom-up)}: Start with each point as its own cluster, then merge closest clusters
    \item \textbf{Divisive (top-down)}: Start with all points in one cluster, then recursively divide
    \item Doesn't require specifying the number of clusters in advance
    \item Can visualize cluster structure at different levels
    \item Linkage methods (single, complete, average, Ward) determine how cluster distances are measured
\end{itemize}
\end{concept}

\begin{concept}{Gaussian Mixture Models (GMM)}\\
GMMs are a probabilistic clustering approach:
\begin{itemize}
    \item Model the data as a mixture of several Gaussian distributions
    \item Each Gaussian component represents a cluster
    \item Provides soft cluster assignments (probabilities)
    \item Parameters estimated using Expectation-Maximization (EM) algorithm
    \item More flexible than K-means but more computationally intensive
\end{itemize}
\end{concept}

\begin{example2}{Comparison of Clustering Algorithms}\\
Consider clustering customer purchase behavior:
\begin{itemize}
    \item Features: Average purchase amount, purchase frequency, time since last purchase
    \item Dataset: 1000 customers
\end{itemize}
\tcblower
K-means:
\begin{itemize}
    \item Fast computation (convergence in 15 iterations)
    \item Creates spherical clusters of similar sizes
    \item Some customers don't fit well in any cluster
    \item Needs careful initialization
\end{itemize}

DBSCAN:
\begin{itemize}
    \item Identifies high-spending and frequent shopper clusters effectively
    \item Marks outliers (abnormal purchase patterns) as noise
    \item Doesn't force customers into clusters artificially
    \item More computationally expensive than K-means
\end{itemize}

Hierarchical Clustering:
\begin{itemize}
    \item Provides insight into relationships between clusters
    \item Shows that the high-spending cluster has two distinct sub-groups
    \item Most computationally expensive of the three
    \item Results sensitive to the linkage method chosen
\end{itemize}
\end{example2}

\begin{KR}{Implementing a Clustering Project}\\
\paragraph{Define the problem}
\begin{itemize}
    \item Clarify business objectives
    \item Understand what insights are needed
    \item Define what makes a "good" cluster in your context
\end{itemize}

\paragraph{Prepare the data}
\begin{itemize}
    \item Clean and preprocess data (handle missing values, outliers)
    \item Select relevant features
    \item Normalize or standardize features
    \item Consider dimensionality reduction if necessary
\end{itemize}

\paragraph{Choose and apply clustering algorithms}
\begin{itemize}
    \item Select appropriate algorithms based on data characteristics
    \item Try multiple algorithms and parameter settings
    \item Evaluate using internal validation metrics
\end{itemize}

\paragraph{Interpret and validate results}
\begin{itemize}
    \item Profile clusters to understand their characteristics
    \item Visualize clusters using dimensionality reduction (PCA, t-SNE)
    \item Validate with domain experts
    \item Test stability by rerunning with different samples
\end{itemize}

\paragraph{Implement findings}
\begin{itemize}
    \item Create actionable insights from clusters
    \item Develop a strategy to use clusters in business processes
    \item Set up a system to assign new data points to clusters
\end{itemize}
\end{KR}

\section{Partitioning Clustering}

\begin{definition}{Clustering Task}\\
Clustering is the task of grouping a set of objects in such a way that objects in the same group (cluster) are more similar to each other than to those in other groups.
\end{definition}

\subsection{Clustering Models}

\begin{concept}{Types of Clustering Models}
\begin{itemize}
    \item \textbf{Connectivity models}: For example, hierarchical clustering builds models based on distance connectivity
    \item \textbf{Centroid models}: For example, the k-means algorithm represents each cluster by a single mean vector
    \item \textbf{Distribution models}: Clusters are modelled using statistical distributions, such as multivariate normal distributions
    \item \textbf{Density models}: For example, DBSCAN and OPTICS defines clusters as connected dense regions in the data space
\end{itemize}
\end{concept}

\subsection{Cluster Properties}

\begin{concept}{Cluster Characteristics}
\begin{itemize}
    \item Clusters may have different sizes, shapes, and densities
    \item Clusters may form a hierarchy
    \item Clusters may be overlapping or disjoint
\end{itemize}
\end{concept}

\subsection{K-Means Clustering}

\begin{definition}{K-Means Goal}\\
Aim to partition $n$ observations into $k$ clusters in which each observation belongs to the cluster with the nearest mean.
\end{definition}

\begin{concept}{K-Means Input/Output}
\textbf{Input}:
\begin{itemize}
    \item $D = p_1, \ldots, p_n$ Points
    \item $k$ Number of clusters
\end{itemize}

\textbf{Output}:
\begin{itemize}
    \item $C = c_1, \ldots, c_k$ Cluster centroids
    \item $D \rightarrow 1, \ldots, k$ Cluster membership
\end{itemize}
\end{concept}

\begin{formula}{Distance Functions}\\
$$euclidean = \sqrt{\sum_{i=1}^{d}(x_i - y_i)^2}$$
$$manhattan = \sqrt{\sum_{i=1}^{d}|x_i - y_i|}$$
\end{formula}

\begin{KR}{K-Means Algorithm}\\
\paragraph{Step 1}
Choose $k$ objects as initial cluster centers

\paragraph{Step 2}
Assign each data point to the cluster which has the closest mean point (centroid) under chosen distance metric

\paragraph{Step 3}
When all data points have been assigned, recalculate the positions of $k$ centroids (mean points)

\paragraph{Step 4}
Repeat steps 2 and 3 until the centroids do not change anymore
\end{KR}

% NOTE: Add K-means clustering visualization showing iterations 1-6 of the algorithm

\subsection{Clustering Evaluation}

\subsubsection{Evaluation without Labels}

\begin{concept}{Evaluation Methods without Labels}
\begin{itemize}
    \item \textbf{Elbow method}: A heuristic used in determining the number of clusters in a data set
    \item \textbf{Silhouette coefficient}: Measure for cluster validity/consistency
    \item \textbf{Dendrogram}: Visual inspection of distances and balancing in hierarchical clustering
\end{itemize}
\end{concept}

\subsubsection{Evaluation with Labels}

\begin{concept}{Evaluation Methods with Labels}
\begin{itemize}
    \item \textbf{Purity}: A simple and transparent measure of correctness of clustering
    \item \textbf{Rand index}: Compares two clusterings
    \item \textbf{Precision and Recall}: Can also be applied
    \item \textbf{Misclassification rate} $MR$:
    \begin{itemize}
        \item $N =$ number of samples
        \item $C =$ number of true clusters
        \item $e_j =$ number of wrongly assigned samples of true cluster $j$
    \end{itemize}
\end{itemize}

$$MR = \frac{1}{N}\sum_{j=1}^{c}e_j$$
\end{concept}

\subsection{Silhouette Coefficient}

\begin{formula}{Silhouette Score Interpretation}\\
The silhouette score falls within the range $[-1,1]$:
\begin{enumerate}
    \item $s \geq 1$: clusters are very dense and nicely separated
    \item $s \approx 0$: clusters are overlapping
    \item $s < 0$: cluster data may be wrong/incorrect
\end{enumerate}

The silhouette plots can be used to select the most optimal value of $K$ in k-means clustering.
\end{formula}

\subsection{ISODATA (K-Means Refinement)}

\begin{concept}{ISODATA Algorithm}\\
It doesn't need to know the number of clusters. Clusters are merged if either the number of members in a cluster is less than a certain threshold or if the centers of two clusters are closer than a certain threshold. The clusters are split into two different clusters if the clusters standard deviation exceeds a predefined value and the number of members is twice the threshold for the minimum number of members. The user must provide several additional parameters.
\end{concept}

\begin{KR}{ISODATA Process}\\
\paragraph{Step 1}
Choose $k$ objects as initial cluster centers

\paragraph{Step 2}
(Re)assign each object to the cluster to which the object is the most similar based on the mean value of the objects in the cluster

\paragraph{Step 3}
Recalculate the centroid by taking the average of all points in the cluster

\paragraph{Step 4}
Reassignment of data points do not change clusters. Clusters are splitted, merged or eliminated, if requirements are not met

\paragraph{Step 5}
Repeat steps 2 and 3 until the centroids do not change anymore
\end{KR}

\section{Hierarchical Density Clustering}

\begin{definition}{Hierarchical Clustering}\\
Hierarchical clustering builds models based on distance connectivity and merging of clusters with minimum distance.

Bottom-up strategy: initially each data object is in its own atomic cluster. Then merge these atomic clusters into larger and larger clusters.
\end{definition}

\subsection{Hierarchical Clustering Method}

\begin{KR}{Hierarchical Clustering Algorithm}\\
\paragraph{Step 1}
Form initial clusters consisting of a single object and compute the distance between each pair of clusters

\paragraph{Step 2}
Merge the two clusters having minimum distance

\paragraph{Step 3}
Calculate the distance between the new cluster and all other clusters

\paragraph{Step 4}
Repeat step 2 and 3 until there is only one cluster remaining
\end{KR}

\subsection{Dendrogram}

\begin{definition}{Dendrogram Properties}\\
A Dendrogram is a tree of nodes representing clusters, satisfying the following properties:
\begin{itemize}
    \item Root represents the whole data set
    \item Leaf nodes represent clusters containing a single object
    \item Inner nodes represent the union of all objects contained in its corresponding subtrees
\end{itemize}
\end{definition}

% NOTE: Add dendrogram visualization showing hierarchical cluster tree structure

\subsection{Linkage Methods}

\begin{definition}{Single Linkage}\\
Shortest distance single link (min) between an element in one cluster and an element in the other, i.e., $d(C_i, C_j) = \min\{d(x_{ip}, x_{jq})\}$.
\end{definition}

\begin{definition}{Complete Linkage}\\
Largest distance between an element in one cluster complete link (max) and an element in the other, i.e., $d(C_i, C_j) = \max\{d(x_{ip}, x_{jp})\}$.
\end{definition}

\begin{definition}{Average Linkage}\\
Average distance between average elements in one cluster and elements in the other, i.e., $d(C_i, C_j) = avg\{d(x_{ip}, x_{jp})\}$.
\end{definition}

\subsection{Clustering Example}

\begin{example2}{1D Clustering Example}\\
Clustering of the 1-dimensional data set $\{2, \mathbf{12}, \mathbf{16}, \mathbf{25}, \mathbf{29}, 45\}$

\textbf{Step 1}: Find lowest distance $\rightarrow$ (10, \textbf{4}, 9, \textbf{4}, 45) and build clusters
$\{2, (12,16), (25,29), 45\}$

\textbf{Step 2}: Find lowest distance based on metrics and build clusters:
\begin{itemize}
    \item \textbf{Centroid} $\{2, (12,16), (25,29), 45\} \rightarrow$ (\textbf{12}, 13,18)
    $\{(\mathbf{2}, \mathbf{12}, \mathbf{16}), (25,29), 45\}$
    
    \item \textbf{Single} $\{2, (12,16), (25,29), 45\} \rightarrow$ (10, \textbf{9}, 16)
    $\{2, (\mathbf{12}, \mathbf{16}, \mathbf{25}, \mathbf{29}), 45\}$
    
    \item \textbf{Complete} $\{2, (12,16), (25,29), 45\} \rightarrow$ (\textbf{14}, 17,16)
    $\{(\mathbf{2}, \mathbf{12}, \mathbf{16}), (25,29), 45\}$
\end{itemize}
\end{example2}

% NOTE: Add three dendrograms showing Centroid, Single linkage, and Complete linkage results

\subsection{DBSCAN}

\begin{definition}{Density-Based Clustering}\\
Density based clustering defines clusters as connected dense regions in the data space. There is no need to specify the number of clusters.
\end{definition}

\begin{definition}{DBSCAN Parameters}\\
DBSCAN (Density-Based Spatial Clustering of Applications with Noise):
\begin{itemize}
    \item $minpoints$: minimum number of points clustered together for a region to be considered dense
    \item $\varepsilon$: a distance measure that will be used to locate the points in the neighbourhood of any point
    \item $core$: a point that has at least $m$ points within distance $n$ from itself
    \item $border$: a point that has at least one core point at a distance $n$
    \item $noise$: a point that is neither core nor a border with less than $m$ points within distance $n$ from itself
\end{itemize}
\end{definition}

\begin{KR}{DBSCAN Algorithm}\\
\paragraph{Step 1}
It starts with a random unvisited point. All points within a distance $\varepsilon$ classify as neighborhood points

\paragraph{Step 2}
It needs a minimum number of points $minpoints$ within the neighbourhood to start the clustering process. Otherwise, the point gets labeled as Noise

\paragraph{Step 3}
All points within the distance $\varepsilon$ become part of the same cluster. Repeat the procedure for all the new points added to the cluster group. Continue till it visits and labels each point within the $\varepsilon$ neighborhood of the cluster

\paragraph{Step 4}
On Completion of the process, it starts again with a new unvisited point thereby leading to the discovery of more cluster or noise. At the end of the process each point is marked
\end{KR}

\subsection{DBSCAN Advantages and Disadvantages}

\begin{concept}{DBSCAN Advantages}
\begin{itemize}
    \item No need to specify the number of clusters
    \item Able to find arbitrarily shaped clusters
    \item Able to detect noise
\end{itemize}
\end{concept}

\begin{concept}{DBSCAN Disadvantages}
\begin{itemize}
    \item Cannot cluster data sets well with large differences in densities
\end{itemize}
\end{concept}

% NOTE: Add DBSCAN visualization showing core points, border points, and noise points