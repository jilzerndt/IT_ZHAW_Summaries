\section{Evaluation}

\subsection{Standard Error Measure}

\begin{formula}{Standard Error Measure}\\
$$E = \frac{1}{N}\sum_{i=1}^{N}(1 - id(\hat{y}_i, y_i))$$

where $id(a,b) = \begin{cases} 1 & \text{if } a = b \\ 0 & \text{else} \end{cases}$

Example: $datasize = 9$, $correct = 6$, $wrong = 3$
$$E = \frac{1}{9} \cdot 3 = 0.33$$
\end{formula}

\subsection{Error Types}

\begin{definition}{Error Classification}
\begin{itemize}
    \item \textbf{Type-I}: False-Positive
    \item \textbf{Type-II}: False-Negative
\end{itemize}
\end{definition}

\begin{definition}{Confusion Matrix}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
$\downarrow y, \hat{y} \rightarrow$ & 1 & 0 \\
\hline
1 & TP=True Positive & FN=False Negative \\
\hline
0 & FP=False Positive & TN=True Negative \\
\hline
\end{tabular}
\end{center}
\end{definition}

\subsection{Evaluation Measures}

\begin{formula}{Performance Metrics}
\begin{itemize}
    \item $A$ = Accuracy: Standard measure (doesn't regard "costs" of errors)
    \item $R$ = Recall: How many relevant documents have been returned
    \item $P$ = Precision: How many of the returned documents are relevant
    \item $F$ = F-Measure: Harmonic mean of recall and precision
\end{itemize}

$$A = \frac{TP + TN}{TP + TN + FP + FN}$$

$$R = \frac{TP}{TP + FN}$$

$$P = \frac{TP}{TP + FP}$$

$$F_1 = 2 \cdot \frac{R \cdot P}{R + P}$$
\end{formula}

\begin{definition}{Recall and Precision Interpretation}\\
$$Recall = \frac{relevant_{found}}{relevant_{total}}$$
$$Precision = \frac{relevant_{found}}{total_{found}}$$
\end{definition}

% NOTE: Add precision/recall visualization diagrams showing four quadrants

\subsection{Kappa Calculation}

\begin{formula}{Kappa Coefficient}
\begin{itemize}
    \item $P(A)$ = proportion of agreements of the raters
    \item $P(E)$ = agreement, which we could get randomly
\end{itemize}

$$K = \frac{P(A) - P(E)}{1 - P(E)}$$
\end{formula}

\subsection{Model Evaluation}

\begin{concept}{Model Evaluation Patterns}\\
Three common patterns in model evaluation:
\begin{itemize}
    \item \textbf{Underfitted}: High bias, low variance
    \item \textbf{Good Fit/Robust}: Balanced bias and variance
    \item \textbf{Overfitted}: Low bias, high variance
\end{itemize}
\end{concept}

% NOTE: Add three time series plots showing underfitting, good fit, and overfitting patterns

\subsection{Clustering Evaluation}

\begin{formula}{Silhouette Coefficient}\\
The Silhouette $s$ coefficient indicates the following:
\begin{enumerate}
    \item $s \geq 1$: Sample is far away from the neighboring clusters
    \item $s < 1$: Sample is close to the decision boundary
    \item $s \approx 0$: Sample is at the decision boundary
\end{enumerate}
\end{formula}